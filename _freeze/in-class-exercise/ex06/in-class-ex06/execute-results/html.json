{
  "hash": "4c730f8520a796ce8df7ee922a651834",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"06 In-class Exercise (Review)\"\nauthor: \"Heng Kuan Xin\"\ndate: 2024-09-23\ndate-modified: \"last-modified\"\n\ntoc: true\ntoc-expand: true\nnumber-sections: true\n\nexecute:\n  eval: true\n  echo: true\n  freeze: true\n  output: true\n  warning: false\n  error: false\n---\n\n\n\n# Recap\n\nLocal Statistics -\\> Test for outliers between states and its neighbours?\n\nTobler's First Law of Geography -\\> i.e basically nearer things are more related than further things\n\nIn this lesson, we will not touch on Spatial Dependency, we will focus on Spatial Autocorrelation.\n\nStatisticians often use it in time series data -- since time series data have a lot of cyclical or seasonal patterns.\n\nInstead of testing for serial stability, in geospatial, we will use autocorrelation for spatial stability. Whether locations are randomly distributed or is it autocorrelated to its neighbours.\n\nTypes of Spatial Autocorrelation Interpretation:\n\n-- Reject null hypo, can infer that there is spatial autocorrelation when interpreting, know that spatial autocorrelation is continuous, just like your correlation coefficients. e.g. in Positive Spatial Autocorrelation, we likely see a range of values from 0 to 1 (clustering is strong when nearer to 1.)\n\n-- When you have more negative spatial autocorrelation, we are likely to see more outliers, that is the checkboard patterns.\n\nBrief history: 1950s to 1960s --\\> '[quantitative geography revolution](https://en.wikipedia.org/wiki/Quantitative_revolution)' Using quantitative methods to measure geographical phenomena.\n\nConfidence Interval:\n\n[![Click on Image for Reference](images/image7-8.jpg){fig-align=\"center\" width=\"468\"}](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2689604/)\n\nWhen we reject the null hypothesis, we can only infer that spatial points are not randomly distributed, we cannot say for sure, or determine that it is really not spatially randomly distributed.\n\nGetis-Ord Global G:\n\nd has to be distance matrix, cannot use proximity matrix.\n\nThe function only tells us whether there are signs of positive or negative clusters –\\> high-high (high value with high value neighbours) clusters and low-low (low value with low value neighbours) clusters.\n\nMeanwhile, there are signs of outliers –\\> high-low, and low-high, which means high value surrounded by low value neighbours and vice versa.\n\n![](images/image7-15.jpg){fig-align=\"center\" width=\"528\"}\n\nThe function only accepts only positive variables, since we are multiplying variables together.\n\nLocal Indicator of Spatial Association (LISA)\n\n-   Break Moran's I into local to come up with LISA\n-   the LISA for each observation gives an indication of the extent of significant spatial clustering of similar values around that observation\n\n# In-class Exercise\n\n::: callout-note\nWe are going to use a different package, and therefore different method, of computing the Spatial Autocorrelation in this exercise. (Compared to Hands-on Exercise 6)\n:::\n\nIn your Takehome Exercise 2, you have to use [sfdep](https://sfdep.josiahparry.com/), a wrapper of spdep, which is based on **sf** and uses tibble DataFrames, which makes working with the data a lot easier.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(sf, sfdep, tmap, tidyverse)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhunan_2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\nhunan_sf <- st_read(\"data/geospatial/\", layer = \"Hunan\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nReading layer `Hunan' from data source \n  `C:\\hengkuanxin\\SMU_Geospatial_Analytics\\in-class-exercise\\ex06\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n```\n\n\n:::\n\n```{.r .cell-code}\nhunan_GDPPC <- left_join(hunan_sf, hunan_2012) %>% select(1:4, 7, 15)\n```\n:::\n\n\n\nTips and Tricks:\n\nWhen using mutate, include argument .before = i, to insert the new attributes before column i. This makes the tibble DataFrame easier to read when we open the table.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwm_q <- hunan_GDPPC %>%\n  mutate( nb = st_contiguity(geometry), # calculate contiguity neighbours as new column called nb\n          wt = st_weights(nb,           # calculate weights as new column called wt\n                          style = \"W\"), # row standardised weights\n          .before = 1)                  # .before makes mutate insert new columns at the front!\n```\n:::\n\n\n\n## Computing Global Moran's I \\[Optional\\]\n\nglobal_moran() function is used to compute the Moran's I value.\n\nDifferent from spdep package, the output is a tibble data.frame.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmoranI <- global_moran(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\nglimpse(moranI)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 2\n $ I: num 0.301\n $ K: num 7.64\n```\n\n\n:::\n:::\n\n\n\nI refers to Moran's I value.\n\nK refers to the average number of neighbours found.\n\n## Performing Global Moran's I Test \\[Base Moran I, no Simulation\\]\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglobal_moran_test(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMoran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n```\n\n\n:::\n:::\n\n\n\n**Check the p-value first!**\n\nThe p-value is smaller than the alpha value if confidence level = 95%. We are therefore able to reject the null hypothesis and we can say that we have enough statistical evidence such that we are 95% confident that clustering is present.\n\n**Then interpret the Moran I statistics**\n\nI is positive, there are signs of clustering.\n\n## Performing Global Moran's I Permutation Test (Repeatedly Test) \\[Use Simulations\\]\n\nIn the real world, in fact you do not need to perform the previous tests. You can just start with the permutation test.\n\nWhat if the Global Moran's I test, under randomisation,\n\nUsually, just a hundred iterations is enough. Note that iterations start from 0. You can also use 999 if need be.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\nglobal_moran_perm(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 99)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMonte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value < 2.2e-16\nalternative hypothesis: two.sided\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlisa <- wm_q %>%\n  mutate(local_moran = local_moran(GDPPC, \n                                   nb, \n                                   wt, \n                                   nsim = 99),\n         .before = 1) %>%\n  unnest(local_moran)\n```\n:::\n\n\n\n![](images/clipboard-4120759321.png){fig-align=\"center\"}\n\nDescribing the columns:\n\n-   **ii**: refers to your local Moran I\n-   **p_ii**: p value, using base method\n-   **p_ii_sim**: p value, using the simulation method –\\> hundred simulations without replacement\n-   **p_folded_sim**: p value, is called the pysal method, which uses K fold validation.\n-   **mean**: label the clusters\n    -   (if normal distribution, mean is useful)\n-   **median:** label the clusters\n    -   (if distribution is skewed, median is more useful) REFER TO **skewness**!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"plot\")\nm1 <- tm_shape(lisa) +\n        tm_fill( \"ii\")+\n        tm_borders(alpha = 0.5)+ \n        tm_view(set.zoom.limits = c(6,8))+\n        tm_layout(\n          main.title = \"Local Moran's I of GDPPC\",\n          main.title.size = 0.8\n        )\n\n m2 <- tm_shape(lisa) +\n          tm_fill( \"p_ii\",\n                   breaks = c(0,0.001,0.01,0.05,1),\n                   labels = c(\"0.001\",\"0.01\",\"0.05\",\"Not significant\"))+\n          tm_borders(alpha = 0.5)+ \n          tm_view(set.zoom.limits = c(6,8))+\n          tm_layout(\n            main.title = \"p-value of Local Moran I's of GDPPC\",\n            main.title.size = 0.8\n          )\n \n tmap_arrange(m1,m2)\n```\n\n::: {.cell-output-display}\n![](in-class-ex06_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlisa_sig <- lisa %>%\n  filter(p_ii < 0.05)\n\ntmap_mode(\"plot\")\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") +\n  tm_borders(alpha = 0.)\n```\n\n::: {.cell-output-display}\n![](in-class-ex06_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\nLISA map is a categorical map showing outliers and clusters. There are two types of outliers, namely: High-Low and Low-High outliers. Likewise, there are two types of clusters namely: High_High and Low-Low clusters. In fact, LISA map is an interpreted map by combining local Moran's I of geographical areas and their respective p-values.\n\n**Context is very important. Note that in your Take-home Exercise, you are looking at Clusters of Drug Usage.**\n\n## Computing Local Gi\\* statistics\n\nAs usual, we will need to derive a spatial weight matrix before we can compute local Gi\\* statistics, Code chunk below will be used to derive a spatial weight matrix by using sfdep functions and tidyverse approach.\n\nYou will also need to include\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwm_idw <- hunan_GDPPC %>%\n  mutate( nb = st_contiguity(geometry),\n          wt = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n          .before = 1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nHCSA <- wm_idw %>%\n  mutate( local_Gi = local_gstar_perm(GDPPC, nb, wt, nsim = 99),\n          .before = 1) %>%\n  unnest(local_Gi)\n\nHCSA\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimple feature collection with 88 features and 18 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 19\n   gi_star cluster   e_gi     var_gi std_dev p_value p_sim p_folded_sim skewness\n     <dbl> <fct>    <dbl>      <dbl>   <dbl>   <dbl> <dbl>        <dbl>    <dbl>\n 1  0.0416 Low     0.0114 0.00000641  0.0493 9.61e-1  0.7          0.35    0.875\n 2 -0.333  Low     0.0106 0.00000384 -0.0941 9.25e-1  1            0.5     0.661\n 3  0.281  High    0.0126 0.00000751 -0.151  8.80e-1  0.9          0.45    0.640\n 4  0.411  High    0.0118 0.00000922  0.264  7.92e-1  0.6          0.3     0.853\n 5  0.387  High    0.0115 0.00000956  0.339  7.34e-1  0.62         0.31    1.07 \n 6 -0.368  High    0.0118 0.00000591 -0.583  5.60e-1  0.72         0.36    0.594\n 7  3.56   High    0.0151 0.00000731  2.61   9.01e-3  0.06         0.03    1.09 \n 8  2.52   High    0.0136 0.00000614  1.49   1.35e-1  0.2          0.1     1.12 \n 9  4.56   High    0.0144 0.00000584  3.53   4.17e-4  0.04         0.02    1.23 \n10  1.16   Low     0.0104 0.00000370  1.82   6.86e-2  0.12         0.06    0.416\n# ℹ 78 more rows\n# ℹ 10 more variables: kurtosis <dbl>, nb <nb>, wt <list>, NAME_2 <chr>,\n#   ID_3 <int>, NAME_3 <chr>, ENGTYPE_3 <chr>, County <chr>, GDPPC <dbl>,\n#   geometry <POLYGON [°]>\n```\n\n\n:::\n:::\n",
    "supporting": [
      "in-class-ex06_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}