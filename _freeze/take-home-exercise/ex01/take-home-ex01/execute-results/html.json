{
  "hash": "230408645af3f5ec39ab47ec93016720",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"01 Take Home Exercise 1\"\nsubtitle: \"Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar\"\nauthor: \"Heng Kuan Xin\"\ndate: 2024-09-04\ndate-modified: \"last-modified\"\n\ntoc-expand: true\nnumber-sections: true\n\nexecute:\n  eval: true\n  echo: true\n  freeze: true\n  output: true\n  warning: false\n  error: false\n---\n\n\n\nData sets:\n\n-   [Data Set Columns Information](https://acleddata.com/knowledge-base/codebook/)\n-   [Data Set Myanmar Overview](https://acleddata.com/knowledge-base/acled-methodology-and-coding-decisions-around-political-violence-and-demonstrations-in-myanmar/)\n-   Categories by Event Type: Battles, Explosion/Remote violence, Strategic developments, and Violence against civilians.\n-   In terms of study period, students should focus on quarterly armed conflict events from January 2021 until June 2024.\n\nImporting Data sets:\n\nBullet points:\n\n-   point 1 on lesson 1 (dataset: administrative boundary data -\\> sf tibble dataframe)\n-   point 2 and 3 last week lesson 3 (dataset: ACLED point data -\\> derive KDE & 2nd-Order Point Pattern Analysis)\n    -   **Caution!** dataset is very large! For point 2, it may take 8 mins. For point 3, it may take half a day.\n    -   Make sure to estimate the time, effort you require -\\> plan ahead\n\nCase Study:\n\nHistory of Myanmar Armed Conflict:\n\nCurrent Situation:\n\nStudy Area:\n\nFocus and Objective:\n\nAnalysis Methods Used:\n\nOur main purpose here this time is to study the **quarterly spatio-temporal distribution of armed conflict events in Myanmar**. Therefore, the focus is mainly on the **types of events**, **date of events**, and the **location of events**. We will not be focusing on other matters in this exercise.\n\n# Importing the Necessary\n\n## Importing Libraries / Packages\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(tidyverse, sf, spatstat, ggplot2, tmap)\n```\n:::\n\n\n\n## Importing Raw Data\n\n### ACLED Data Set\n\nBefore we start manipulating the data, it is important that we understand what each data column and data type mean and how valuable it is for our analysis.\n\nTo begin, let us refer to the [ACLED data set codebook](https://acleddata.com/acleddatanew/wp-content/uploads/dlm_uploads/2023/06/ACLED_Codebook_2023.pdf)\n\nNow, let us first import the data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n##| eval: false\n\n# filter_columns <- c(\"event_date\",\"year\",\"disorder_type\",\"sub_event_type\",\"admin1\",\"admin2\",\"admin3\")\n\nviolence_against_civ <- read_csv(\"data/raw/aspatial/ACLED/violence-against-civilians.csv\") %>%\n  st_as_sf(coords =c(\n    \"longitude\", \"latitude\"),\n    crs=4326) %>%\n    st_transform(crs = 32647) %>%\n    mutate(event_date = dmy(event_date))\n\nstrategic_dev <- read_csv(\"data/raw/aspatial/ACLED/strategic-developments.csv\") %>%\n  st_as_sf(coords =c(\n    \"longitude\", \"latitude\"),\n    crs=4326) %>%\n    st_transform(crs = 32647) %>%\n    mutate(event_date = dmy(event_date))\n\nbattles <- read_csv(\"data/raw/aspatial/ACLED/battles.csv\") %>%\n  st_as_sf(coords =c(\n    \"longitude\", \"latitude\"),\n    crs=4326) %>%\n    st_transform(crs = 32647) %>%\n    mutate(event_date = dmy(event_date))\n\nexplosion_or_remote_vio <- read_csv(\"data/raw/aspatial/ACLED/explosion-or-remoteviolence.csv\") %>%\n  st_as_sf(coords =c(\n    \"longitude\", \"latitude\"),\n    crs=4326) %>%\n    st_transform(crs = 32647) %>%\n    mutate(event_date = dmy(event_date))\n```\n:::\n\n\n\n### Administrative Boundary Dataset\n\n[![Myamar's Administrative Division Hierachy (source: Wikipedia)](images/clipboard-2034245623.png){fig-alt=\"Myamar's Administrative Division Hierachy\"}](https://en.wikipedia.org/wiki/Administrative_divisions_of_Myanmar)\n\nIn this exercise, we will only analyse geographical distribution of point data up to the 3rd degree administrative division, i.e. up to Township level.\n\nThe following data are the administrative boundaries of Myanmar, obtained from [MIMU Vector Boundaries](https://geonode.themimu.info/layers/?limit=100&offset=0&type__in=vector&category__identifier__in=boundaries), a common data and information repository by a NGO (related to UN).\n\nWhile many data formats exist, we will choose `.csv` files when downloading as they are easier to read and work with. A suitable alternative is the `.shp` file format available on the website.\n\nDatasets:\n\n-   `mmr_polbnda_adm0_250k_mimu_1.csv` National boundary of Myanmar\n\n-   `mmr_polbnda_adm1_250k_mimu_1.csv` **Region/State/Union Territory** level boundary of Myanmar\n\n-   `mmr_polbnda2_adm1_250k_mimu_1.csv` **Sub-Region/State/Union Territory** level boundary of Myanmar; Sub-region divides a region into smaller divisions, such as \"Bago (East)\" and \"Bago (West)\" instead of \"Bago\"\n\n-   `mmr_polbnda_adm2_250k_mimu.csv` **District/Self-Administered Zone** level boundary of Myanmar\n\n-   `mmr_polbnda_adm3_250k_mimu_1.csv` **Township** level boundary of Myanmar\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# kml file not easy to read due to nested columns for attributes data\nadmin_0 <- read_csv(\"data/raw/geospatial/mimu_admin_boundary/mmr_polbnda_adm0_250k_mimu_1.csv\") %>%\n  st_as_sf(wkt = \"the_geom\",\n    crs=4326) %>%\n    st_transform(crs = 32647)\n    \nadmin_1 <- read_csv(\"data/raw/geospatial/mimu_admin_boundary/mmr_polbnda_adm1_250k_mimu_1.csv\") %>%\n  st_as_sf(wkt = \"the_geom\",\n    crs=4326) %>%\n    st_transform(crs = 32647)\n\nadmin_1_sub <- read_csv(\"data/raw/geospatial/mimu_admin_boundary/mmr_polbnda2_adm1_250k_mimu_1.csv\") %>%\n  st_as_sf(wkt = \"the_geom\",\n    crs=4326) %>%\n    st_transform(crs = 32647)\n\nadmin_2 <- read_csv(\"data/raw/geospatial/mimu_admin_boundary/mmr_polbnda_adm2_250k_mimu.csv\") %>%\n  st_as_sf(wkt = \"the_geom\",\n    crs=4326) %>%\n    st_transform(crs = 32647)\n\nadmin_3 <- read_csv(\"data/raw/geospatial/mimu_admin_boundary/mmr_polbnda_adm3_250k_mimu_1.csv\") %>%\n  st_as_sf(wkt = \"the_geom\",\n    crs=4326) %>%\n    st_transform(crs = 32647)\n```\n:::\n\n\n\n### Quick Plot To Visualise Data Sets\n\nBefore we continue, let's get a visual sense of the data by plotting it on a map to ensure we are working with the right dataset. In this step, we will also try to see if there are any erranous data that we have to clean later.\n\n**Check the Administrative Boundaries Data Set**\n\nFrom the left to right, we are able to see the increase in degree of administrative boundary division; respectively, they represent\n\n1.  \"**National**\",\n2.  \"**Region/State/Union Territory**\",\n3.  \"**Sub-Region/State/Union Territory**\",\n4.  \"**District/Self-Administered Zone**\", and\n5.  \"**Township**\"\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"plot\")\ntmap_style(\"classic\")\n\ntmap_arrange(\n  qtm(admin_0, title=\"National\"), \n  qtm(admin_1, title=\"Region/State/Union\\nTerritory\"), \n  qtm(admin_1_sub, title=\"Sub-Region/State/Union\\nTerritory\"), \n  qtm(admin_2, title=\"District/Self-\\nAdministered Zone\"), \n  qtm(admin_3, title=\"Township\"), \n  ncol = 5\n)\n```\n\n::: {.cell-output-display}\n![](take-home-ex01_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n**Conclusion: Administrative Boundaries Looks Okay**\n\nLooking at both the attribute table and the plot, the administrative boundaries data seem quite alright, so we might not need to do any cleaning. Anyway, the precision of the polygons it not as critical as the precision of the ACLED data (which will be used for our spatial point patterns analysis).\n\n**Check ACLED Data Set**\n\nSince the ACLED data is our main focus, let us now plot the data points onto the map of Myanmar.\n\nIn the code chunk below, we plot out the various types of data points from ACLED, namely (from left to right):\n\n1.  \"**Violence against Civilians**\",\n2.  \"**Strategic Developments**\",\n3.  \"**Explosion/Remote Violence**\", and\n4.  \"**Battles**\"\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"plot\")\ntmap_style(\"cobalt\")\n\ntmap_arrange(\n  qtm(admin_0) + qtm(violence_against_civ, title=\"Violence against\\nCivilians\"), \n  qtm(admin_0) + qtm(strategic_dev, title=\"Strategic\\nDevelopments\"), \n  qtm(admin_0) + qtm(explosion_or_remote_vio, title=\"Explosion/\\nRemote Violence\"), \n  qtm(admin_0) + qtm(battles, title=\"Battles\"), \n  ncol = 4\n)\n```\n\n::: {.cell-output-display}\n![](take-home-ex01_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n**Observation: ACLED Data Set needs further study, may need to clean**\n\nNothing looks very off at a glance, but we should look into the attribute columns to see if the data set is truly clean. Let us refer to the [codebook](https://acleddata.com/acleddatanew/wp-content/uploads/dlm_uploads/2023/06/ACLED_Codebook_2023.pdf) again, and see if we can spot any potentially critical problems.\n\nReferring to the attribute columns and the codebook, we can see potentially critical concerns:\n\n1.  **geo_precision** – The precision of the geocoded coordinates ranges from code 1 to code 3; where lower level implies higher precision.\n    -   In particular, \"\\[if\\] a larger region is mentioned, the closest natural location noted in reporting (like “border area,” “forest,” or “sea,” among others) – or a provincial capital is used if no other information at all is available – is chosen to represent the region, and ‘Geo-precision’ code 3 is recorded.\" (page 36)\n2.  **time_precision** – The precision of the recorded datetime ranges from code 1 to code 3; where lower level implies higher precision.\n    -   In particular, \"if the source material only indicates that an event took place sometime during a month (i.e. in the past two or three weeks, or in January), without reference to the particular date, the month mid-point is chosen. If the beginning or end of the month is noted, the first and last date is used, respectively. In both of these cases, a ‘Time precision’ code of 3 is recorded.\" (page 36-37)\n\nIn both cases, ACLED do not include events with less spatial or temporal precision.\n\n**Identifying code 3 precisions**\n\nGiven that precision of our point data is crucial to our spatial-temporal point patterns analysis, we should see how much of our data is imprecise, and whether we should keep the imprecise data points.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Violence against Civilians\nviolence_against_civ %>% count(geo_precision)\nviolence_against_civ %>% count(time_precision)\n\n# Strategic Developments\nstrategic_dev %>% count(geo_precision)\nstrategic_dev %>% count(time_precision)\n\n# Explosion or Remote Violence\nexplosion_or_remote_vio %>% count(geo_precision)\nexplosion_or_remote_vio %>% count(time_precision)\n\n# Battles\nbattles %>% count(geo_precision)\nbattles %>% count(time_precision)\n```\n:::\n\n\n\n**Conclusion: Drop time and spatial precision code 3 data values from ACLED Data set**\n\nFrom this analysis, we can see that the count of precision code 3 in both time and spatial precision is actually very low, it might be worth dropping the values with low time and spatial precision.\n\nLet us start cleaning the data\n\n## Data Cleaning\n\nLet us filter out the data we want (time_precision and geo_precision codes \\< 3), and see how many rows we have removed.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract all rows where either geo_precision or time_precision is not = 3 \nviolence_against_civ_filtered <- violence_against_civ %>%\n  filter(!(geo_precision == 3 | time_precision == 3))\n\nstrategic_dev_filtered <- strategic_dev %>%\n  filter(!(geo_precision == 3 | time_precision == 3))\n\nexplosion_or_remote_vio_filtereed <- explosion_or_remote_vio %>%\n  filter(!(geo_precision == 3 | time_precision == 3))\n\nbattles_filtered <- battles %>%\n  filter(!(geo_precision == 3 | time_precision == 3))\n\ncat(\"Number of rows dropped:\", nrow(violence_against_civ) - nrow(violence_against_civ_filtered) , \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of rows dropped: 34 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Number of rows dropped:\", nrow(strategic_dev) - nrow(strategic_dev_filtered) , \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of rows dropped: 227 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Number of rows dropped:\", nrow(explosion_or_remote_vio) - nrow(explosion_or_remote_vio_filtereed) , \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of rows dropped: 22 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Number of rows dropped:\", nrow(battles) - nrow(battles_filtered) , \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of rows dropped: 42 \n```\n\n\n:::\n:::\n\n\n\n**Overall, the result seems satisfactory**. Let us continue with our data cleaning. by keeping only columns that are important for our analysis.\n\nWe will only **keep the following columns** as other columns are not relevant to our study:\n\n-   event_date,\n-   year,\n-   disorder_type,\n-   sub_event_type,\n-   admin1,\n-   admin2,\n-   admin3\n-   (also including the geometry data)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfilter_columns <- c(\"event_date\",\"year\",\"disorder_type\",\"sub_event_type\",\"admin1\",\"admin2\",\"admin3\")\n\nviolence_against_civ_filtered <- violence_against_civ_filtered %>%\n  select(all_of(filter_columns))\n\nstrategic_dev_filtered <- strategic_dev_filtered %>%\n  select(all_of(filter_columns))\n\nexplosion_or_remote_vio_filtereed <- explosion_or_remote_vio_filtereed %>%\n  select(all_of(filter_columns))\n\nbattles_filtered <- battles_filtered %>%\n  select(all_of(filter_columns))\n```\n:::\n\n\n\n## Data Extraction\n\n### Extract Quarterly Data from ACLED Dataset\n\nNow, since we want to analyse the quarterly events, let us add a new column within the tibble DataFrame called **quarter** to represent the quarter of each date in numerical format, e.g. (1, 2, 3, or 4). After that, let us create another column called **year_quarter** to represent the quarter of every year in string format, e.g. (\"2021-Q1\", \"2023-Q4\").\n\nThe package we will be using here is called lubridate, a package within the tidyverse library.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nviolence_against_civ_filtered <- violence_against_civ_filtered %>% \n  mutate(quarter = quarter(event_date)) %>%\n  mutate(year_quarter = paste0(year, \"-Q\", quarter))\n\nstrategic_dev_filtered <- strategic_dev_filtered %>% \n  mutate(quarter = quarter(event_date)) %>%\n  mutate(year_quarter = paste0(year, \"-Q\", quarter))\n\nexplosion_or_remote_vio_filtereed <- explosion_or_remote_vio_filtereed %>% \n  mutate(quarter = quarter(event_date)) %>%\n  mutate(year_quarter = paste0(year, \"-Q\", quarter))\n\nbattles_filtered <- battles_filtered %>% \n  mutate(quarter = quarter(event_date)) %>%\n  mutate(year_quarter = paste0(year, \"-Q\", quarter))\n```\n:::\n\n\n\n## Export Data Sets (RDS file format)\n\nBefore we continue, let's export our cleaned data sets so that our changes are saved. **We will export the data sets in the RDS format.**\n\nRDS stands for R Data Serialization. It’s a binary serialization format in R used to save R objects to a file. This format preserves the class, attributes, and structure of the R object, making it useful for saving and loading data while maintaining its integrity.\n\n::: {.callout-tip collapse=\"true\"}\nBy exporting the data, and importing it again, it also serves as a *checkpoint* for our analysis. We will be able to stop loading old variables in our environment, and only load in the new variables. It also allows readers who are trying to reproduce the analysis verify their own results with our analysis results.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Save the sf object to an RDS file\nsaveRDS(violence_against_civ, \"data/rds/violence_against_civilians.rds\")\nsaveRDS(strategic_dev, \"data/rds/strategic_developments.rds\")\nsaveRDS(explosion_or_remote_vio, \"data/rds/explosion_or_remoteviolence.rds\")\nsaveRDS(battles, \"data/rds/battles.rds\")\n```\n:::\n\n\n\nNow, let's continue to the next section.\n\n# Kernel-Density Estimation\n\n## Import Data Sets (RDS file format)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nviolence_against_civ <- readRDS(\"data/rds/violence_against_civilians.rds\")\nstrategic_dev <- readRDS(\"data/rds/strategic_developments.rds\")\nexplosion_or_remote_vio <- readRDS(\"data/rds/explosion_or_remoteviolence.rds\")\nbattles <- readRDS(\"data/rds/battles.rds\")\n```\n:::\n\n\n\nTO BE COMPLETED\n",
    "supporting": [
      "take-home-ex01_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}