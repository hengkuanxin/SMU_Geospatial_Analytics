[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for Geospatial Analytics",
    "section": "",
    "text": "Contents overview:\n\nHome (this page)\nAbout\nHands-on exercises\nIn-class exercises\nTake-home Assignment\n\nThe table below shows the latest edits I have added.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nModified\n\n\nTitle\n\n\n\n\n\n\nSep 23, 2024\n\n\n9/23/24, 10:39:13 AM\n\n\n06 In-class Exercise (Review)\n\n\n\n\nSep 4, 2024\n\n\n9/22/24, 10:48:11 PM\n\n\n01 Take Home Exercise 1\n\n\n\n\nSep 21, 2024\n\n\n9/22/24, 12:05:02 AM\n\n\n06 (Part 2) ‘Local Measures of Spatial Autocorrelation’\n\n\n\n\nSep 21, 2024\n\n\n9/21/24, 11:41:57 PM\n\n\n06 (Part 1) ‘Global Measures of Spatial Autocorrelation’\n\n\n\n\nSep 16, 2024\n\n\n9/16/24, 11:21:43 AM\n\n\n05 In-class Exercise (Review)\n\n\n\n\nSep 9, 2024\n\n\n9/9/24, 4:43:44 PM\n\n\n04 In-class Exercise 4 (Review)\n\n\n\n\nSep 3, 2024\n\n\n9/9/24, 9:45:13 AM\n\n\n04 ‘Network Constrained Spatial Point Patterns Analysis’\n\n\n\n\nAug 29, 2024\n\n\n9/4/24, 1:06:04 AM\n\n\n03 (Part 1) ‘1st Order Spatial Point Patterns Analysis Methods’\n\n\n\n\nAug 29, 2024\n\n\n9/4/24, 1:02:08 AM\n\n\n03 (Part 2) ‘2nd Order Spatial Point Patterns Analysis Methods’\n\n\n\n\nSep 2, 2024\n\n\n9/4/24, 12:47:23 AM\n\n\n03 In-class Exercise 3 (Review)\n\n\n\n\nAug 21, 2024\n\n\n9/2/24, 12:50:39 PM\n\n\n01 In-class Exercise 1 (Review)\n\n\n\n\nAug 26, 2024\n\n\n9/2/24, 12:50:06 PM\n\n\n02 In-class Exercise 2 (Review)\n\n\n\n\nAug 21, 2024\n\n\n8/28/24, 12:59:02 AM\n\n\n02 ‘Thematic Mapping and GeoVisualisation with R’\n\n\n\n\nAug 20, 2024\n\n\n8/28/24, 12:58:55 AM\n\n\n01 ‘Geospatial Data Science with R’\n\n\n\n\n\nNo matching items\n\n\n  \n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\n\n\n Back to top"
  },
  {
    "objectID": "hands-on-exercise/ex01/hands-on-ex01.html",
    "href": "hands-on-exercise/ex01/hands-on-ex01.html",
    "title": "Exercise 01 ‘Geospatial Data Science with R’",
    "section": "",
    "text": "(Copied from Exercise) Learning Outcome:\n\ninstalling and loading sf and tidyverse packages into R environment,\nimporting geospatial data by using appropriate functions of sf package,\nimporting aspatial data by using appropriate function of readr package,\nexploring the content of simple feature data frame by using appropriate Base R and sf functions,\nassigning or transforming coordinate systems by using using appropriate sf functions,\nconverting an aspatial data into a sf data frame by using appropriate function of sf package,\nperforming geoprocessing tasks by using appropriate functions of sf package,\nperforming data wrangling tasks by using appropriate functions of dplyr package and\nperforming Exploratory Data Analysis (EDA) by using appropriate functions from ggplot2 package.\n\n\n1 Import packages\n\n\npacman::p_load(sf, tidyverse)\n\n\n\n2 Import data\n\nThe data used in this assignment is acquired from publicly available sources:\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nPre-Schools Location from data.gov.sg\nCycling Path from LTADataMall\nLatest version of Singapore Airbnb listing data from Inside Airbnb\n\n\n# Singapore Master Plan 2014 Subzone Boundary (polygon feature)\nmpsz &lt;- st_read(dsn = \"data/geospatial/MasterPlan2014SubzoneBoundaryWebSHP\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n# Singapore Cycling Paths (line feature)\ncyclingpath &lt;- st_read(dsn=\"data/geospatial/CyclingPath_Jul2024\",\n                       layer = \"CyclingPathGazette\")\n\n# Singapore Preschool Locations (point feature)\npreschool &lt;- st_read(\"data/geospatial/PreSchoolsLocation.kml\")\n\n# The above files are read as different geometry-type data frames (as a 'simple feature')\n\n# Airbnb Listing Data (aspatial, but contains coordinate attributes)\nairbnb_listings &lt;- read_csv(\"data/aspatial/listings.csv\",\n                            show_col_types = FALSE)\n\n# The above file is read as a data frame.\n\n\n\n3 Read geometry data\n\n\nRetrieve Geometry List\n\n# Retrieve geometry list to see detailed information\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((31495.56 30140.01, 31980.96 296...\n\n\nMULTIPOLYGON (((29092.28 30021.89, 29119.64 300...\n\n\nMULTIPOLYGON (((29932.33 29879.12, 29947.32 298...\n\n\nMULTIPOLYGON (((27131.28 30059.73, 27088.33 297...\n\n\nMULTIPOLYGON (((26451.03 30396.46, 26440.47 303...\n\n#alternatively, use mpsz$geom or mpsz[[1]]\n\n# Output: \n# Geometry set for 323 features \n# Geometry type: MULTIPOLYGON\n# Dimension:     XY\n# Bounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\n# Projected CRS: SVY21\n# First 5 geometries:...\n\n\n\nShow data type of each attribute/field\n\n# Retrieve data type of every attribute in the data frame\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n# Rows: 323\n# Columns: 16\n# $ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13…\n# $ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, …\n# $ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUA…\n# $ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"…\n# $ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"…\n# $ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVE…\n# $ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\",…\n# $ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n# $ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n# $ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"…\n# $ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014…\n# $ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 2…\n# $ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 3…\n# $ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2…\n# $ SHAPE_Area &lt;dbl&gt; 1630379.3, 559816.2, 160807.5, 595428.9, …\n# $ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30.…\n\n\n\nShow first n rows of data frame\n\nhead(mpsz, n=5)\n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\n\n\n\n4 Plot Geospatial Data\n\nThe default plot of an sf object is a multi-plot of all attributes, up to a reasonable maximum as shown above.\n\nplot(mpsz)\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\n\n\nPlot geometry only\n\nplot(st_geometry(mpsz))\n\n\n\n\n\n\n\n\n\n\nPlot by attribute\n\nplot(mpsz[\"PLN_AREA_N\"]) # specify a particular attribute used to plot sf object\n\n\n\n\n\n\n\n\n\n\n\n5 View CRS details and assign a new EPSG code\n\n\n# Retrieve CRS details\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n# Although mpsz data frame is projected in svy21 but when we read until the end of the print, it indicates that the EPSG is 9001. This is a wrong EPSG code because the correct EPSG code for svy21 should be 3414.\n\n#In order to assign the correct EPSG code to mpsz data frame, st_set_crs() of sf package is used as shown in the code chunk below.\n\n\nSet new CRS\n\n# set crs to EPSG:3414, also called SVY21\nmpsz3414 &lt;- st_set_crs(mpsz, 3414)\n\nWarning: st_crs&lt;- : replacing crs does not reproject data; use st_transform for\nthat\n\n\n\n\n\n6 CRS Transformation\n\n\n# Observe that preschool uses WGS84 for its CRS\nst_geometry(preschool)\n\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nPOINT Z (103.8072 1.299333 0)\n\n\nPOINT Z (103.826 1.312839 0)\n\n\nPOINT Z (103.8409 1.348843 0)\n\n\nPOINT Z (103.8048 1.435024 0)\n\n\nPOINT Z (103.839 1.33315 0)\n\n# To standardise the coordinate reference system, we will convert it into Singapore's standard CRS SVY21\n\n# WGS84(Geodetic CRS) to SVY21(Projected CRS)\n\npreschool3414 &lt;- st_transform(preschool,\n                              crs = 3414)\n\n\n\n7 Converting Aspatial Data\n\n\n# airbnb_listings is an aspatial data, but it contains coordinates in the form of longitude and latitude.\n\n# we will create a sf data frame using the coordinates\nairbnb_listings_sf &lt;- st_as_sf(airbnb_listings,\n                               coords = c(\"longitude\",\"latitude\"),\n                               crs = 4326) %&gt;%\n  st_transform(crs=3414)\n\n# EPSG:4326 is inferred from the data source, and it is also the standard CRS based on WGS84. The coordinates are in degrees (long, lat)\n\n# glimpse(airbnb_listings_sf)\n\n\n\n8 Geoprocessing with sf package\n\n\nBuffer around geometry\n\n# we will create a buffer around the whole stretch of the cycling paths (line feature), this will also convert the line feature into a polygon feature.\n\nbuffer_cycling &lt;- st_buffer(cyclingpath,\n                            dist = 5,\n                            nQuadSegs = 30)\n\n# the cycling path is now a polygon feature\nst_geometry(buffer_cycling)\n\nGeometry set for 3138 features \nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 11849.32 ymin: 28342.98 xmax: 42649.17 ymax: 48953.15\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nPOLYGON ((40130.06 37081.7, 40132.39 37082.12, ...\n\n\nPOLYGON ((35125.99 42959.75, 35122.14 42938.35,...\n\n\nPOLYGON ((35094.01 42820.45, 35094.63 42819.06,...\n\n\nPOLYGON ((35093.81 42821.63, 35093.83 42821.49,...\n\n\nPOLYGON ((16031.24 36929.24, 16035.69 36944.95,...\n\nbuffer_cycling$area &lt;- st_area(buffer_cycling)\n\nsum(buffer_cycling$area)\n\n2218855 [m^2]\n\n\n\n\nPoint-in-polygon count\n\n# we will find out how many Preschools (point feature) exist within each planning subzone (polygon feature)\n\nmpsz3414$`PreSch Count` &lt;- lengths(st_intersects(mpsz3414,preschool3414))\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\n\n# list the planning subzone with the most number of pre-school\n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\n\n\nCalculate the density of ‘pre-School by planning subzone’\nSteps:\n\nCalculate area of each planning subzone\nCount number of pre-school per area (in each planning subzone)\n\n\n# Calculate area of each planning subzone\nmpsz3414$Area &lt;- mpsz3414 %&gt;% \n  st_area()\n\nglimpse(mpsz3414)\n\nRows: 323\nColumns: 18\n$ OBJECTID       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n$ SUBZONE_NO     &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2,…\n$ SUBZONE_N      &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON…\n$ SUBZONE_C      &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ…\n$ CA_IND         &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", …\n$ PLN_AREA_N     &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MER…\n$ PLN_AREA_C     &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"…\n$ REGION_N       &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"…\n$ REGION_C       &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"…\n$ INC_CRC        &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0…\n$ FMEL_UPD_D     &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-1…\n$ X_ADDR         &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358…\n$ Y_ADDR         &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991…\n$ SHAPE_Leng     &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.…\n$ SHAPE_Area     &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44,…\n$ geometry       &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYG…\n$ `PreSch Count` &lt;int&gt; 0, 6, 0, 5, 3, 13, 5, 1, 11, 1, 4, 2, 0, 1, 6, 0, 0, 0,…\n$ Area           [m^2] 1630379.27 [m^2], 559816.25 [m^2], 160807.50 [m^2], 595…\n\n\n\n# Create new column called 'PreSch Density' by dividing areschool count by area (multiplied by 1000000 as area is in millions)\n\nmpsz3414 &lt;- mpsz3414 %&gt;% mutate(`PreSch Density` = `PreSch Count`/ Area * 1000000)\n\nglimpse(mpsz3414)\n\nRows: 323\nColumns: 19\n$ OBJECTID         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…\n$ SUBZONE_NO       &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, …\n$ SUBZONE_N        &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERS…\n$ SUBZONE_C        &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BM…\n$ CA_IND           &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\"…\n$ PLN_AREA_N       &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT M…\n$ PLN_AREA_C       &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\",…\n$ REGION_N         &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\",…\n$ REGION_C         &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC          &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13…\n$ FMEL_UPD_D       &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014…\n$ X_ADDR           &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 253…\n$ Y_ADDR           &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 299…\n$ SHAPE_Leng       &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 442…\n$ SHAPE_Area       &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.4…\n$ geometry         &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOL…\n$ `PreSch Count`   &lt;int&gt; 0, 6, 0, 5, 3, 13, 5, 1, 11, 1, 4, 2, 0, 1, 6, 0, 0, …\n$ Area             [m^2] 1630379.27 [m^2], 559816.25 [m^2], 160807.50 [m^2], 5…\n$ `PreSch Density` [1/m^2] 0.0000000 [1/m^2], 10.7178026 [1/m^2], 0.0000000 [1…\n\n\n\n\n\n9 Exploratory Data Analysis\n\n\n# \"In this section, you will learn how to use appropriate ggplot2 functions to create functional and yet truthful statistical graphs for EDA purposes.\"\n\n# plot the histogram to observe distribution of preschool density (count per area in subzone)\n\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\n\n\n\n\n\n# to improve on visualisation, we will specify the mapping aesthetics + histogram(bin size, color, fill) + labels\n\nggplot(\n  data = mpsz3414, \n  mapping = aes(\n    x = as.numeric(`PreSch Density`))\n  ) +\n  geom_histogram(\n    bins=20, \n    color= 'black', \n    fill=\"light blue\"\n  ) +\n  labs(\n    title = \"Are pre-school even distributed in Singapore?\",\n    subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n    x = \"Pre-school density (per km sq)\",\n    y = \"Frequency\"\n  )\n\n\n\n\n\n\n\n\n\n# a scatter plot: using PreSch Density as x, PreSch Count as y, adding color by their region names + point plot\n\nggplot(\n  data = mpsz3414, \n  mapping = aes(\n    x = as.numeric(`PreSch Density`),\n    y = `PreSch Count`,\n    color = REGION_N)\n  ) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "01 ‘Geospatial Data Science with R’"
    ]
  },
  {
    "objectID": "in-class-exercise/ex01/in-class-ex01.html",
    "href": "in-class-exercise/ex01/in-class-ex01.html",
    "title": "01 In-class Exercise 1 (Review)",
    "section": "",
    "text": "1 Import packages & data\n\n\npacman::p_load(tidyverse, sf)\n\n\nmpsz &lt;- st_read(\n  dsn = \"data/geospatial/MPSZ-2019\",\n  layer = \"MPSZ-2019\")\n\nReading layer `MPSZ-2019' from data source \n  `C:\\hengkuanxin\\SMU_Geospatial_Analytics\\in-class-exercise\\ex01\\data\\geospatial\\MPSZ-2019' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\n2 Reading Data\n\n\nst_geometry(mpsz)\n\nGeometry set for 332 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\nplot(mpsz)\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "01 In-class Exercise 1 (Review)"
    ]
  },
  {
    "objectID": "in-class-exercise/ex01/data/geospatial/MPSZ-2019/MPSZ-2019.html",
    "href": "in-class-exercise/ex01/data/geospatial/MPSZ-2019/MPSZ-2019.html",
    "title": "IS415 Practice Site",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "hands-on-exercise/ex02/hands-on-ex02.html",
    "href": "hands-on-exercise/ex02/hands-on-ex02.html",
    "title": "Exercise 02 ‘Thematic Mapping and GeoVisualisation with R’",
    "section": "",
    "text": "(Copied from Exercise) Overview:\nIn general, thematic mapping involves the use of map symbols to visualize selected properties of geographic features that are not naturally visible, such as population, temperature, crime rate, and property prices, just to mention a few of them.\nGeovisualisation, on the other hand, works by providing graphical ideation to render a place, a phenomenon or a process visible, enabling human’s most powerful information-processing abilities – those of spatial cognition associated with our eye–brain vision system – to be directly brought to bear.\nIn this chapter, you will learn how to plot functional and truthful choropleth maps by using an R package called tmap package.\n\n1 Import packages\n\n\npacman::p_load(sf, tidyverse, tmap)\n\n\n\n2 Import data\n\n(Copied from Exercise) Two data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\nmpsz &lt;- st_read(\n  dsn=\"data/geospatial/MasterPlan2014SubzoneBoundaryWebSHP\",\n  layer=\"MP14_SUBZONE_WEB_PL\")\n\n# mpsz\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020/respopagesextod2011to2020.csv\", show_col_types = FALSE)\n\n# popdata\n# colnames(popdata)\n\n\n\n3 Data Preparation\n\n\nCreate Data Table for Year 2020\nWe want to plot a choropleth map using year 2020 values, by joining the population table with the masterplan subzone object.\nAim: Create a data table for year 2020 values, containing columns as shown:\n\n\n\nPA\nSZ\nYOUNG\nECONOMY ACTIVE\nAGED\nTOTAL\nDEPENDENCY\n\n\n\nTherefore, we will derive values for:\n\nYOUNG: age group 0 to 4 until age group 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time==2020) %&gt;%    # select rows where Time = 2020 \n  group_by(PA, SZ, AG) %&gt;%  # group by PA, SZ, AG\n  summarise(`POP` = sum(`Pop`)) %&gt;% # sum up the population within each group\n  ungroup() %&gt;%              # ungroup table (which allows for further calculations\n                             # on individual rows)\n  pivot_wider(               # create new columns using values from AG as names\n    names_from=AG,           # and values from POP as values\n    values_from=POP)\n\ncolnames(popdata2020)\n\n [1] \"PA\"          \"SZ\"          \"0_to_4\"      \"10_to_14\"    \"15_to_19\"   \n [6] \"20_to_24\"    \"25_to_29\"    \"30_to_34\"    \"35_to_39\"    \"40_to_44\"   \n[11] \"45_to_49\"    \"50_to_54\"    \"55_to_59\"    \"5_to_9\"      \"60_to_64\"   \n[16] \"65_to_69\"    \"70_to_74\"    \"75_to_79\"    \"80_to_84\"    \"85_to_89\"   \n[21] \"90_and_over\"\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate(YOUNG = rowSums(.[3:6]) + rowSums(.[14])) %&gt;% # add YOUNG = sum across the row where index = [3:6] and [15] \n  mutate(`ECONOMY ACTIVE` = rowSums(.[7:13]) + rowSums(.[15])) %&gt;% # add ECONOMY ACTIVE\n  mutate(`AGED`= rowSums(.[16:21])) %&gt;% # add AGED\n  mutate(`TOTAL`= rowSums(.[3:21])) %&gt;% # add TOTAL\n  mutate(`DEPENDENCY` = (`YOUNG` + `AGED`)/`ECONOMY ACTIVE`) %&gt;% # add DEPENDENCY\n  select(`PA`, `SZ`, `YOUNG`, `ECONOMY ACTIVE`, `AGED`, `TOTAL`, `DEPENDENCY`)\n\ncolnames(popdata2020)\n\n[1] \"PA\"             \"SZ\"             \"YOUNG\"          \"ECONOMY ACTIVE\"\n[5] \"AGED\"           \"TOTAL\"          \"DEPENDENCY\"    \n\n\n\n\nJoin attribute and geospatial data (geolocational join)\n\n# convert PA and SZ variable data to upper cases to match SUBZONE_N and PLN_AREA_N of mspz\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ),      # mutate at variables PA , SZ\n          .funs = list(toupper)) %&gt;%   # apply function list(toupper)\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\n# left join by SUBZONE_N = SZ\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\n\nExport/Write Data frame as RDS object\n\n# Write to a R Data Serialization(RDS) file, which is useful for storing state of objects between R sessions\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\n\n\n4 Choropleth Mapping Geospatial Data Using tmap\n\n\nOverview Examples\n\nQuick Method of Visualising using qtm()\n\n# set tmap mode to \"plot\" , alternatively use view for interactive plot\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020,\n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n# mpsz_pop2020\n\n\n\nAdjusting layouts, borders, and adding furnitures (e.g. compass, scale)\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\nBasic Steps to create a Choropleth Map. Using tm_polygons() vs tm_fill() + tm_borders()\n\n# st_make_valid(mpsz_pop2020)\n# tmap_options(check.and.fix = TRUE)\n# tmap_mode(\"plot\")\n\n# Step 1a: Plain Polygon\ntm1 &lt;- tm_shape(mpsz_pop2020) + tm_polygons() + tm_layout(\n  main.title = \"Step 1a: Plain Polygon\",\n  main.title.position = \"center\",\n  main.title.size = 0.7)\n\n# Step 1b: Coloured Polygon using DEPENDENCY\ntm2 &lt;- tm_shape(mpsz_pop2020)+ tm_polygons(\"DEPENDENCY\") + tm_layout(\n  main.title = \"Step 1b: Coloured Polygon using DEPENDENCY\",\n  main.title.position = \"center\",\n  main.title.size = 0.7,\n  legend.width = 5.0)\n\n# Alternatively, Step 2a: Coloured Fill using DEPENDENCY instead of Polygon\ntm3 &lt;- tm_shape(mpsz_pop2020)+ tm_fill(\"DEPENDENCY\") + tm_layout(\n  main.title = \"Alternatively, Step 2a: Coloured Fill using DEPENDENCY instead of Polygon\",\n  main.title.position = \"center\",\n  main.title.size = 0.7)\n\n# Step 2b: Add in borders on top of Fill\ntm4 &lt;- tm_shape(mpsz_pop2020) + tm_fill(\"DEPENDENCY\") + tm_borders(lwd = 0.1, alpha = 1) + tm_layout(\n  main.title = \"Step 2b: Add in borders on top of Fill\",\n  main.title.position = \"center\",\n  main.title.size = 0.7)\n\n# Specify Data Classification (Quantile, n=5)\ntm5 &lt;- tm_shape(mpsz_pop2020) + tm_fill(\"DEPENDENCY\", n = 5, style = \"quantile\") + tm_borders(alpha = 0.5) + tm_layout(\n  main.title = \"Specify Data Classification (Quantile, n=5)\",\n  main.title.position = \"center\",\n  main.title.size = 0.7)\n\n# Specify Data Classification (Equal range), n=5)\ntm6 &lt;- tm_shape(mpsz_pop2020) + tm_fill(\"DEPENDENCY\", n = 5, style = \"equal\") + tm_borders(alpha = 0.5) + tm_layout(\n  main.title = \"Specify Data Classification (Equal range), n=5)\",\n  main.title.position = \"center\",\n  main.title.size = 0.7)\n\n# Plot all maps together\ntmap_arrange(tm1,tm2,tm3,tm4,tm5,tm6)\n\n\n\n\n\n\n\n\n\n\n\nDetermining Data Classification Methods and Number of Classes\n\n\n\nData Classification Guide\n\n\nGiven that Dependency ranges from 0 to 19, let us try n = 1 + 3.32 * log(20) ~= 5.\n\nggplot(data = mpsz_pop2020, \n       mapping = aes(x = `DEPENDENCY`)\n       ) +\n  geom_histogram()\n\n\n\n\n\n\n\n  # geom_dotplot()\n  # geom_freqpoly()\n\nBy plotting the distribution of DEPENDENCY, we see that there is an outlier that is significantly larger. To avoid misrepresenting the data, we should avoid equal intervals, quantile, and standard deviation. A good choice may be Jenks (Natural Breaks), or other manual classifications.\n\n# Visualise the attribute value range\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.6519  0.7025  0.7742  0.7645 19.0000      92 \n\n\n\n\nCustom Intervals/Ranges for Data Classification\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nAdjusting palette, Inverting Palette\n\n# Using colour palette Blues instead of default\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n## Changing palette to Greens. Use -Greens for inverse colouring\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nAdjusting Legend properties\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nAdjusting tmap_style()\n\ntmap_style(\"classic\")\n\n# other available styles are: \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nMore Examples\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\ntmap_style(\"white\")\n\n\n\n\n5 Showing Multiple Maps\n\n\nMethod 1: specify within tm_fill()\n\n# note two maps created, one using \"YOUNG\", the other \"AGED\"\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\nMethod 2: Using tm_facets(by = ATTRIBUTE)\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nMethod 3: Using tmap_arrange(map1, map2, …)\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n6 Specifying Specific Region in Map to Show\n\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "02 ‘Thematic Mapping and GeoVisualisation with R’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex02/hands-on-ex02.html#create-data-table-for-year-2020",
    "href": "hands-on-exercise/ex02/hands-on-ex02.html#create-data-table-for-year-2020",
    "title": "Exercise 02 ‘Thematic Mapping and GeoVisualisation with R’",
    "section": "Create Data Table for Year 2020",
    "text": "Create Data Table for Year 2020\nWe want to plot a choropleth map using year 2020 values, by joining the population table with the masterplan subzone object.\nAim: Create a data table for year 2020 values, containing columns as shown:\n\n\n\nPA\nSZ\nYOUNG\nECONOMY ACTIVE\nAGED\nTOTAL\nDEPENDENCY\n\n\n\nTherefore, we will derive values for:\n\nYOUNG: age group 0 to 4 until age group 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time==2020) %&gt;%    # select rows where Time = 2020 \n  group_by(PA, SZ, AG) %&gt;%  # group by PA, SZ, AG\n  summarise(`POP` = sum(`Pop`)) %&gt;% # sum up the population within each group\n  ungroup() %&gt;%              # ungroup table (which allows for further calculations\n                             # on individual rows)\n  pivot_wider(               # create new columns using values from AG as names\n    names_from=AG,           # and values from POP as values\n    values_from=POP)\n\ncolnames(popdata2020)\n\n [1] \"PA\"          \"SZ\"          \"0_to_4\"      \"10_to_14\"    \"15_to_19\"   \n [6] \"20_to_24\"    \"25_to_29\"    \"30_to_34\"    \"35_to_39\"    \"40_to_44\"   \n[11] \"45_to_49\"    \"50_to_54\"    \"55_to_59\"    \"5_to_9\"      \"60_to_64\"   \n[16] \"65_to_69\"    \"70_to_74\"    \"75_to_79\"    \"80_to_84\"    \"85_to_89\"   \n[21] \"90_and_over\"\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate(YOUNG = rowSums(.[3:6]) + rowSums(.[14])) %&gt;% # add YOUNG = sum across the row where index = [3:6] and [15] \n  mutate(`ECONOMY ACTIVE` = rowSums(.[7:13]) + rowSums(.[15])) %&gt;% # add ECONOMY ACTIVE\n  mutate(`AGED`= rowSums(.[16:21])) %&gt;% # add AGED\n  mutate(`TOTAL`= rowSums(.[3:21])) %&gt;% # add TOTAL\n  mutate(`DEPENDENCY` = (`YOUNG` + `AGED`)/`ECONOMY ACTIVE`) %&gt;% # add DEPENDENCY\n  select(`PA`, `SZ`, `YOUNG`, `ECONOMY ACTIVE`, `AGED`, `TOTAL`, `DEPENDENCY`)\n\ncolnames(popdata2020)\n\n[1] \"PA\"             \"SZ\"             \"YOUNG\"          \"ECONOMY ACTIVE\"\n[5] \"AGED\"           \"TOTAL\"          \"DEPENDENCY\""
  },
  {
    "objectID": "hands-on-exercise/ex02/hands-on-ex02.html#join-attribute-and-geospatial-data-geolocational-join",
    "href": "hands-on-exercise/ex02/hands-on-ex02.html#join-attribute-and-geospatial-data-geolocational-join",
    "title": "Exercise 02 ‘Thematic Mapping and GeoVisualisation with R’",
    "section": "Join attribute and geospatial data (geolocational join)",
    "text": "Join attribute and geospatial data (geolocational join)\n\n# convert PA and SZ variable data to upper cases to match SUBZONE_N and PLN_AREA_N of mspz\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ),      # mutate at variables PA , SZ\n          .funs = list(toupper)) %&gt;%   # apply function list(toupper)\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\n# left join by SUBZONE_N = SZ\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))"
  },
  {
    "objectID": "hands-on-exercise/ex02/hands-on-ex02.html#exportwrite-data-frame-as-rds-object",
    "href": "hands-on-exercise/ex02/hands-on-ex02.html#exportwrite-data-frame-as-rds-object",
    "title": "Exercise 02 ‘Thematic Mapping and GeoVisualisation with R’",
    "section": "Export/Write Data frame as RDS object",
    "text": "Export/Write Data frame as RDS object\n\n# Write to a R Data Serialization(RDS) file, which is useful for storing state of objects between R sessions\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "hands-on-exercise/ex02/hands-on-ex02.html#overview-examples",
    "href": "hands-on-exercise/ex02/hands-on-ex02.html#overview-examples",
    "title": "Exercise 02 ‘Thematic Mapping and GeoVisualisation with R’",
    "section": "Overview Examples",
    "text": "Overview Examples\n\nQuick Method of Visualising using qtm()\n\n# set tmap mode to \"plot\" , alternatively use view for interactive plot\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020,\n    fill = \"DEPENDENCY\")\n\n\n\n# mpsz_pop2020\n\n\n\nAdjusting layouts, borders, and adding furnitures (e.g. compass, scale)\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\nBasic Steps to create a Choropleth Map. Using tm_polygons() vs tm_fill() + tm_borders()\n\n# st_make_valid(mpsz_pop2020)\n# tmap_options(check.and.fix = TRUE)\n# tmap_mode(\"plot\")\n\n# Step 1a: Plain Polygon\ntm1 &lt;- tm_shape(mpsz_pop2020) + tm_polygons() + tm_layout(\n  main.title = \"Step 1a: Plain Polygon\",\n  main.title.position = \"center\",\n  main.title.size = 0.7)\n\n# Step 1b: Coloured Polygon using DEPENDENCY\ntm2 &lt;- tm_shape(mpsz_pop2020)+ tm_polygons(\"DEPENDENCY\") + tm_layout(\n  main.title = \"Step 1b: Coloured Polygon using DEPENDENCY\",\n  main.title.position = \"center\",\n  main.title.size = 0.7,\n  legend.width = 5.0)\n\n# Alternatively, Step 2a: Coloured Fill using DEPENDENCY instead of Polygon\ntm3 &lt;- tm_shape(mpsz_pop2020)+ tm_fill(\"DEPENDENCY\") + tm_layout(\n  main.title = \"Alternatively, Step 2a: Coloured Fill using DEPENDENCY instead of Polygon\",\n  main.title.position = \"center\",\n  main.title.size = 0.7)\n\n# Step 2b: Add in borders on top of Fill\ntm4 &lt;- tm_shape(mpsz_pop2020) + tm_fill(\"DEPENDENCY\") + tm_borders(lwd = 0.1, alpha = 1) + tm_layout(\n  main.title = \"Step 2b: Add in borders on top of Fill\",\n  main.title.position = \"center\",\n  main.title.size = 0.7)\n\n# Specify Data Classification (Quantile, n=5)\ntm5 &lt;- tm_shape(mpsz_pop2020) + tm_fill(\"DEPENDENCY\", n = 5, style = \"quantile\") + tm_borders(alpha = 0.5) + tm_layout(\n  main.title = \"Specify Data Classification (Quantile, n=5)\",\n  main.title.position = \"center\",\n  main.title.size = 0.7)\n\n# Specify Data Classification (Equal range), n=5)\ntm6 &lt;- tm_shape(mpsz_pop2020) + tm_fill(\"DEPENDENCY\", n = 5, style = \"equal\") + tm_borders(alpha = 0.5) + tm_layout(\n  main.title = \"Specify Data Classification (Equal range), n=5)\",\n  main.title.position = \"center\",\n  main.title.size = 0.7)\n\n# Plot all maps together\ntmap_arrange(tm1,tm2,tm3,tm4,tm5,tm6)"
  },
  {
    "objectID": "hands-on-exercise/ex02/hands-on-ex02.html#determining-data-classification-methods-and-number-of-classes",
    "href": "hands-on-exercise/ex02/hands-on-ex02.html#determining-data-classification-methods-and-number-of-classes",
    "title": "Exercise 02 ‘Thematic Mapping and GeoVisualisation with R’",
    "section": "Determining Data Classification Methods and Number of Classes",
    "text": "Determining Data Classification Methods and Number of Classes\n\n\n\nData Classification Guide\n\n\nGiven that Dependency ranges from 0 to 19, let us try n = 1 + 3.32 * log(20) ~= 5.\n\nggplot(data = mpsz_pop2020, \n       mapping = aes(x = `DEPENDENCY`)\n       ) +\n  geom_histogram()\n\n\n\n  # geom_dotplot()\n  # geom_freqpoly()\n\nBy plotting the distribution of DEPENDENCY, we see that there is an outlier that is significantly larger. To avoid misrepresenting the data, we should avoid equal intervals, quantile, and standard deviation. A good choice may be Jenks (Natural Breaks), or other manual classifications.\n\n# Visualise the attribute value range\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.6519  0.7025  0.7742  0.7645 19.0000      92"
  },
  {
    "objectID": "hands-on-exercise/ex02/hands-on-ex02.html#custom-intervalsranges-for-data-classification",
    "href": "hands-on-exercise/ex02/hands-on-ex02.html#custom-intervalsranges-for-data-classification",
    "title": "Exercise 02 ‘Thematic Mapping and GeoVisualisation with R’",
    "section": "Custom Intervals/Ranges for Data Classification",
    "text": "Custom Intervals/Ranges for Data Classification\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "hands-on-exercise/ex02/hands-on-ex02.html#adjusting-palette-inverting-palette",
    "href": "hands-on-exercise/ex02/hands-on-ex02.html#adjusting-palette-inverting-palette",
    "title": "Exercise 02 ‘Thematic Mapping and GeoVisualisation with R’",
    "section": "Adjusting palette, Inverting Palette",
    "text": "Adjusting palette, Inverting Palette\n\n# Using colour palette Blues instead of default\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n## Changing palette to Greens. Use -Greens for inverse colouring\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "hands-on-exercise/ex02/hands-on-ex02.html#adjusting-legend-properties",
    "href": "hands-on-exercise/ex02/hands-on-ex02.html#adjusting-legend-properties",
    "title": "Exercise 02 ‘Thematic Mapping and GeoVisualisation with R’",
    "section": "Adjusting Legend properties",
    "text": "Adjusting Legend properties\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "hands-on-exercise/ex02/hands-on-ex02.html#adjusting-tmap_style",
    "href": "hands-on-exercise/ex02/hands-on-ex02.html#adjusting-tmap_style",
    "title": "Exercise 02 ‘Thematic Mapping and GeoVisualisation with R’",
    "section": "Adjusting tmap_style()",
    "text": "Adjusting tmap_style()\n\ntmap_style(\"classic\")\n\n# other available styles are: \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "hands-on-exercise/ex02/hands-on-ex02.html#more-examples",
    "href": "hands-on-exercise/ex02/hands-on-ex02.html#more-examples",
    "title": "Exercise 02 ‘Thematic Mapping and GeoVisualisation with R’",
    "section": "More Examples",
    "text": "More Examples\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\ntmap_style(\"white\")"
  },
  {
    "objectID": "hands-on-exercise/ex02/hands-on-ex02.html#method-1-specify-within-tm_fill",
    "href": "hands-on-exercise/ex02/hands-on-ex02.html#method-1-specify-within-tm_fill",
    "title": "Exercise 02 ‘Thematic Mapping and GeoVisualisation with R’",
    "section": "Method 1: specify within tm_fill()",
    "text": "Method 1: specify within tm_fill()\n\n# note two maps created, one using \"YOUNG\", the other \"AGED\"\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))"
  },
  {
    "objectID": "hands-on-exercise/ex02/hands-on-ex02.html#method-2-using-tm_facetsby-attribute",
    "href": "hands-on-exercise/ex02/hands-on-ex02.html#method-2-using-tm_facetsby-attribute",
    "title": "Exercise 02 ‘Thematic Mapping and GeoVisualisation with R’",
    "section": "Method 2: Using tm_facets(by = ATTRIBUTE)",
    "text": "Method 2: Using tm_facets(by = ATTRIBUTE)\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "hands-on-exercise/ex02/hands-on-ex02.html#method-3-using-tmap_arrangemap1-map2",
    "href": "hands-on-exercise/ex02/hands-on-ex02.html#method-3-using-tmap_arrangemap1-map2",
    "title": "Exercise 02 ‘Thematic Mapping and GeoVisualisation with R’",
    "section": "Method 3: Using tmap_arrange(map1, map2, …)",
    "text": "Method 3: Using tmap_arrange(map1, map2, …)\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)"
  },
  {
    "objectID": "in-class-exercise/ex02/in-class-ex02.html",
    "href": "in-class-exercise/ex02/in-class-ex02.html",
    "title": "02 In-class Exercise 2 (Review)",
    "section": "",
    "text": "# load packages\n\npacman::p_load(sf, tidyverse, tmap)\n\n#pacman::p_load(sf, tidyverse, ggstatsplot, tmap)\n\n\n\n\nWe are going to import the Master Plan 2014 Subzone Boundary Web (file format .shp and .kml) retrieved from data.gov.sg.\nThis code chunk imports the shapefile data\n\nmpsz_shp &lt;- st_read(dsn=\"data/geospatial/MasterPlan2014SubzoneBoundaryWebSHP\", \n                    layer=\"MP14_SUBZONE_WEB_PL\")\n\nIn RStudio, check the Help tab to search for library functions, it will show you the input arguments and the output.\n\n\n\nWhenever you import data, make sure to view the data and understand what data types, data formats, crs (coordinate reference systems), geometry type, and so on.\n\nclass(mpsz_shp)\n\n[1] \"sf\"         \"data.frame\"\n\n\nNote: when reading data types:\n\nint : an integer\nnum : numerical; can be any real number\nchr : characters - geometry : sf’s geometry data.\n\nE.g. “sfc_MULTIPOLYGONS” if the package detects multipolygons.\n\n\nrefer to basic data types in R\n\n\n\nThis code chunk tries to import the kml data\n\n# mpsz_kml &lt;- st_read(\"data/geospatial/MasterPlan2014SubzoneBoundaryWebKML.kml\")\n# This data file cannot be used; The data source is corrupted.\n\nThere might be a reasons why the kml cannot be used: (1) it is a zipped file, or (2) the file got tampered/corrupted during the uploading process.\nIf you want to convert to another file format, you can generate it using sf::st_write().\n\n\n\n\n#| output: false\n\nst_write(mpsz_shp,\n         \"data/geospatial/MP14_SUBZONE_WEB_PL.kml\",\n         delete_dsn = TRUE) # delete old file if exists.\n\nDeleting source `data/geospatial/MP14_SUBZONE_WEB_PL.kml' using driver `KML'\nWriting layer `MP14_SUBZONE_WEB_PL' to data source \n  `data/geospatial/MP14_SUBZONE_WEB_PL.kml' using driver `KML'\nWriting 323 features with 15 fields and geometry type Multi Polygon.",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "02 In-class Exercise 2 (Review)"
    ]
  },
  {
    "objectID": "in-class-exercise/ex02/in-class-ex02.html#working-with-master-plan-planning-sub-zone-data",
    "href": "in-class-exercise/ex02/in-class-ex02.html#working-with-master-plan-planning-sub-zone-data",
    "title": "02 In-class Exercise 2 (Review)",
    "section": "",
    "text": "# load packages\n\npacman::p_load(sf, tidyverse, tmap)\n\n#pacman::p_load(sf, tidyverse, ggstatsplot, tmap)\n\n\n\n\nWe are going to import the Master Plan 2014 Subzone Boundary Web (file format .shp and .kml) retrieved from data.gov.sg.\nThis code chunk imports the shapefile data\n\nmpsz_shp &lt;- st_read(dsn=\"data/geospatial/MasterPlan2014SubzoneBoundaryWebSHP\", \n                    layer=\"MP14_SUBZONE_WEB_PL\")\n\nIn RStudio, check the Help tab to search for library functions, it will show you the input arguments and the output.\n\n\n\nWhenever you import data, make sure to view the data and understand what data types, data formats, crs (coordinate reference systems), geometry type, and so on.\n\nclass(mpsz_shp)\n\n[1] \"sf\"         \"data.frame\"\n\n\nNote: when reading data types:\n\nint : an integer\nnum : numerical; can be any real number\nchr : characters - geometry : sf’s geometry data.\n\nE.g. “sfc_MULTIPOLYGONS” if the package detects multipolygons.\n\n\nrefer to basic data types in R\n\n\n\nThis code chunk tries to import the kml data\n\n# mpsz_kml &lt;- st_read(\"data/geospatial/MasterPlan2014SubzoneBoundaryWebKML.kml\")\n# This data file cannot be used; The data source is corrupted.\n\nThere might be a reasons why the kml cannot be used: (1) it is a zipped file, or (2) the file got tampered/corrupted during the uploading process.\nIf you want to convert to another file format, you can generate it using sf::st_write().\n\n\n\n\n#| output: false\n\nst_write(mpsz_shp,\n         \"data/geospatial/MP14_SUBZONE_WEB_PL.kml\",\n         delete_dsn = TRUE) # delete old file if exists.\n\nDeleting source `data/geospatial/MP14_SUBZONE_WEB_PL.kml' using driver `KML'\nWriting layer `MP14_SUBZONE_WEB_PL' to data source \n  `data/geospatial/MP14_SUBZONE_WEB_PL.kml' using driver `KML'\nWriting 323 features with 15 fields and geometry type Multi Polygon.",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "02 In-class Exercise 2 (Review)"
    ]
  },
  {
    "objectID": "in-class-exercise/ex02/in-class-ex02.html#choosing-good-data-sets",
    "href": "in-class-exercise/ex02/in-class-ex02.html#choosing-good-data-sets",
    "title": "02 In-class Exercise 2 (Review)",
    "section": "3. Choosing Good Data sets",
    "text": "3. Choosing Good Data sets\n\n3.1 Pre-school locations\ndownload and load the preschool locations from data.gov.sg\nTO BE COMPLETED\n\n#import data set\npreschool_kml &lt;- st_read(dsn=\"data/geospatial/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\hengkuanxin\\SMU_Geospatial_Analytics\\in-class-exercise\\ex02\\data\\geospatial\\PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n# preschool_geojson &lt;- st_read(\"data/geospatial/PreschoolsLocation.geojson\")\n\n\n\n3.2 Masterplan 2019 Sub-zone\n\n#import data set\nmpsz19_kml &lt;- st_read(\"data/geospatial/MasterPlan2019SubzoneBoundaryNoSeaKML.kml\")\n\nmpsz19_shp &lt;- st_read(dsn=\"data/geospatial/MPSZ-2019\",\n                      layer=\"MPSZ-2019\")\n\nNote the differences: both have 332 features, but one has 2+1 fields, the other has 6+1 fields. KML has nested multiple attributes within 1 column. The KML file format in this context is harder to use than the SHP file format.\n\n\n3.3 Transforming the CRS\n\nmpsz19_shp_3414 &lt;- mpsz19_shp %&gt;%\n  st_transform(crs=3414)\n\nEPSG:3414 refers to Singapore’s projected coordinate system (SVY21), as opposed to WGS84. A projected coordinate system uses distances measure, rather than degrees (latitude, longitude)",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "02 In-class Exercise 2 (Review)"
    ]
  },
  {
    "objectID": "in-class-exercise/ex02/in-class-ex02.html#working-with-population-data",
    "href": "in-class-exercise/ex02/in-class-ex02.html#working-with-population-data",
    "title": "02 In-class Exercise 2 (Review)",
    "section": "4.0 Working with Population Data",
    "text": "4.0 Working with Population Data\n\npop2023 &lt;- read_csv(\"data/aspatial/respopagesextod2023.csv\", )\n\n# show column data types\nspec(pop2023)\n\nNote: Ease of use of data: see how the columns and data are categorised between different the different data sets respopagesextod2023.csv and respopagesextod2023.xlsx. Check the other data sets: respopagesexfa2023.csv\n\npop2023_sum &lt;- pop2023 %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()\n\ncolnames(pop2023_sum)\n\n[1] \"PA\"  \"SZ\"  \"AG\"  \"POP\"\n\npop2023_sum &lt;- pop2023_sum %&gt;%\n  pivot_wider(names_from = `AG`,\n              values_from = `POP`)\n\ncolnames(pop2023_sum)\n\n [1] \"PA\"          \"SZ\"          \"0_to_4\"      \"10_to_14\"    \"15_to_19\"   \n [6] \"20_to_24\"    \"25_to_29\"    \"30_to_34\"    \"35_to_39\"    \"40_to_44\"   \n[11] \"45_to_49\"    \"50_to_54\"    \"55_to_59\"    \"5_to_9\"      \"60_to_64\"   \n[16] \"65_to_69\"    \"70_to_74\"    \"75_to_79\"    \"80_to_84\"    \"85_to_89\"   \n[21] \"90_and_Over\"\n\n\nNote: tidyverse is a collection of packages for data science, also called a disjointed integrated packages. The separate packages maintained separately and do not contain same function names. Collectively, they might conflict with Base R functions, however; e.g. dpylr::filter().\n\npop2023_sum &lt;- pop2023_sum %&gt;%\n  mutate(YOUNG = rowSums(.[3:6]) + rowSums(.[14])) %&gt;% # add YOUNG = sum across the row where index = [3:6] and [15] \n  mutate(`ECONOMY ACTIVE` = rowSums(.[7:13]) + rowSums(.[15])) %&gt;% # add ECONOMY ACTIVE\n  mutate(`AGED`= rowSums(.[16:21])) %&gt;% # add AGED\n  mutate(`TOTAL`= rowSums(.[3:21])) %&gt;% # add TOTAL\n  mutate(`DEPENDENCY` = (`YOUNG` + `AGED`)/`ECONOMY ACTIVE`) %&gt;% # add DEPENDENCY\n  select(`PA`, `SZ`, `YOUNG`, `ECONOMY ACTIVE`, `AGED`, `TOTAL`, `DEPENDENCY`)\n\ncolnames(pop2023_sum)\n\n[1] \"PA\"             \"SZ\"             \"YOUNG\"          \"ECONOMY ACTIVE\"\n[5] \"AGED\"           \"TOTAL\"          \"DEPENDENCY\"    \n\n\n\nJoin together population data with geospatial data (masterplan sub-zone)\n\n# convert PA and SZ variable data to upper cases to match SUBZONE_N and PLN_AREA_N of mspz\npop2023_sum &lt;- pop2023_sum %&gt;%\n  mutate_at(.vars = vars(PA, SZ),      # mutate at variables PA , SZ\n          .funs = list(toupper)) %&gt;%   # apply function list(toupper)\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\n# left join by SUBZONE_N = SZ\nmpsz19_shp_3414 &lt;- left_join(mpsz19_shp_3414, pop2023_sum,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "02 In-class Exercise 2 (Review)"
    ]
  },
  {
    "objectID": "in-class-exercise/ex02/data/geospatial/MPSZ-2019/MPSZ-2019.html",
    "href": "in-class-exercise/ex02/data/geospatial/MPSZ-2019/MPSZ-2019.html",
    "title": "R for Geospatial Analytics",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "hands-on-exercise/hands-on-ex-intro.html",
    "href": "hands-on-exercise/hands-on-ex-intro.html",
    "title": "00 Hands-On Exercises Introduction",
    "section": "",
    "text": "This section showcases my weekly attempts at geospatial analytics for my course.\nEach exercise is focused on a central topic; e.g. Hands-on exercise 02 ‘Thematic Mapping and GeoVisualisation with R’ corresponding to the topic of the assignment of the week.\nAll analyses were done using R programming language and are for learning purposes only.\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "00 Hands-On Exercises Introduction"
    ]
  },
  {
    "objectID": "in-class-exercise/in-class-ex-intro.html",
    "href": "in-class-exercise/in-class-ex-intro.html",
    "title": "00 In-Class Exercises Introduction",
    "section": "",
    "text": "This section showcases my practices and additional notes during my weekly classes, which are mainly reviews of the hands on exercises.\nAll analyses were done using R programming language and are for learning purposes only.\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "00 In-Class Exercises Introduction"
    ]
  },
  {
    "objectID": "index.html#new-posts",
    "href": "index.html#new-posts",
    "title": "R for Geospatial Analytics",
    "section": "New Posts",
    "text": "New Posts"
  },
  {
    "objectID": "hands-on-exercise/ex03/hands-on-ex03_01.html",
    "href": "hands-on-exercise/ex03/hands-on-ex03_01.html",
    "title": "03 (Part 1) ‘1st Order Spatial Point Patterns Analysis Methods’",
    "section": "",
    "text": "(Copied from Exercise)",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "03 (Part 1) ‘1st Order Spatial Point Pattern Analysis’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex03/hands-on-ex03_01.html#overview",
    "href": "hands-on-exercise/ex03/hands-on-ex03_01.html#overview",
    "title": "03 (Part 1) ‘1st Order Spatial Point Patterns Analysis Methods’",
    "section": "Overview:",
    "text": "Overview:\nSpatial Point Pattern Analysis is the evaluation of the pattern or distribution, of a set of points on a surface. The point can be location of:\n\nevents such as crime, traffic accident and disease onset, or\nbusiness services (coffee and fastfood outlets) or facilities such as childcare and eldercare.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childcare centres in Singapore.\nThe specific questions we would like to answer are as follows:\n\nAre the childcare centres in Singapore randomly distributed throughout the country?\nIf the answer is not, then the next logical question is where are the locations with higher concentration of childcare centres?\n\n\nData Used\nTo provide answers to the questions above, three data sets will be used. They are:\n\nCHILDCARE, a point feature data providing both location and attribute information of childcare centres. It was downloaded from Data.gov.sg and is in geojson format.\nMP14_SUBZONE_WEB_PL, a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg.\nCostalOutline, a polygon feature data showing the national boundary of Singapore. It is provided by SLA and is in ESRI shapefile format.\n\n\n\nPackages Used\nIn this hands-on exercise, five R packages will be used, they are:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat, which has a wide range of useful functions for point pattern analysis. In this hands-on exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nraster which reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools which provides a set of tools for manipulating geographic data. In this hands-on exercise, we mainly use it to convert Spatial objects into ppp format of spatstat.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\nThe above is a copy from the exercise overview from workbook",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "03 (Part 1) ‘1st Order Spatial Point Pattern Analysis’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex03/hands-on-ex03_01.html#import-packages",
    "href": "hands-on-exercise/ex03/hands-on-ex03_01.html#import-packages",
    "title": "03 (Part 1) ‘1st Order Spatial Point Patterns Analysis Methods’",
    "section": "1 Import Packages",
    "text": "1 Import Packages\n\npacman::p_load(sf, spatstat, raster, tmap, rvest, tidyverse)",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "03 (Part 1) ‘1st Order Spatial Point Pattern Analysis’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex03/hands-on-ex03_01.html#import-data",
    "href": "hands-on-exercise/ex03/hands-on-ex03_01.html#import-data",
    "title": "03 (Part 1) ‘1st Order Spatial Point Patterns Analysis Methods’",
    "section": "2 Import Data",
    "text": "2 Import Data\n\n# childcare services data\nchildcare_sf &lt;- st_read(dsn=\"data/geospatial/child-care-services-geojson.geojson\")\n\n# coastal outline data\nsg_sf &lt;- st_read(dsn=\"data/geospatial\", layer=\"CostalOutline\")\n\n# masterplan subzone data\nmpsz_sf &lt;- st_read(dsn=\"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\n# notice childcare_sf is in WGS84, transform it into SV21:\nchildcare_sf &lt;- childcare_sf %&gt;% st_transform(crs=3414)\n\n\nChecking Attribute Information\n* Notice how the attributes of childcare_sf is nested within html syntax. Instead of using BeautifulSoup4 in Python, in R we can use the rvest package to parse the html data. (Converted my python function to R with help of ChatGPT)\n\nIn order to read the attributes, we need to first isolate the ‘Description’ attribute, and read line by line using each &lt;tr&gt;.\nWithin each line, identify &lt;th&gt; and &lt;td&gt;. The attribute name is located within a pair of &lt;th&gt;, and value is located within a pair of &lt;td&gt;.\n\nNOTE: list() in R can behave as both an array and a hashtable, the equivalent of which are list() and dictionary() in Python respectively.\n\nparse_html_table &lt;- function(html) {\n  # read the HTML content\n  rows &lt;- read_html(html) %&gt;% html_nodes(\"tr\")\n  \n  # initialise an empty list to store data\n  data &lt;- list()\n  \n  # read row by row\n  for (row in rows) {\n    # find all columns in the row\n    cols &lt;- html_nodes(row, xpath = \".//th|.//td\")\n    \n    # if exactly 2 columns, add them to the data list\n    if (length(cols) == 2) {\n      \n      ## get KEY-VALUE pair\n      key &lt;- html_text(cols[1], trim = TRUE)\n      value &lt;- html_text(cols[2], trim = TRUE)\n      data[[key]] &lt;- value\n    }\n  }\n  \n  return(data)\n}\n\nchildcare_sf$Parsed &lt;- lapply(X=childcare_sf$Description, FUN=parse_html_table)\n\nExtracting attribute NAME from Parsed column\n\n# head(childcare_sf$Parsed,n=5)\n# childcare_sf$Parsed[[1]]$NAME\n\n# Apply function where we retrieve\nchildcare_sf$NAME &lt;- lapply(X=childcare_sf$Parsed, FUN=function(x) x$NAME) %&gt;% \n  # unlist to turn list into vector\n  unlist()\n\n# childcare_sf$NAME\nchildcare_sf &lt;- childcare_sf %&gt;% select(`NAME`) # Keep Name only, since other attributes are not used\n\n\n# show result\nhead(childcare_sf)\n\nSimple feature collection with 6 features and 1 field\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 20644.07 ymin: 29900.09 xmax: 41217.74 ymax: 45716.7\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\n                                      NAME                      geometry\n1 AVERBEL CHILD DEVELOPMENT CENTRE PTE LTD  POINT Z (27976.73 45716.7 0)\n2                                AWWA LTD.    POINT Z (25824 29900.09 0)\n3             BABIES BY-THE-PARK PTE. LTD. POINT Z (31399.04 37416.36 0)\n4             Baby Elk Infant Care Pte Ltd  POINT Z (29268.43 40942.1 0)\n5          BABYPLANET MONTESSORI PTE. LTD. POINT Z (41217.74 33554.94 0)\n6                    BAMBINI CHILDCARE LLP POINT Z (20644.07 36118.78 0)\n\n\n\n\nCheck CRS are all consistent– All EPSG:3414\n\n#| output: false\n\n# st_geometry(sg_sf)\n# st_geometry(mpsz_sf)\n# st_geometry(childcare_sf)\nst_crs(sg_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\nst_crs(mpsz_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\nst_crs(childcare_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNote: even though sg_sf and mpsz_sf is using SVY21, and have its coordinates in meters, they do not actually record itself as EPSG:3414, instead the CRS information shows EPSG:9001. In order to ensure consistency and a clean data, let us set the CRS to 3414.\n\n# change CRS to EPSG:3414\nst_crs(sg_sf) &lt;- 3414\nst_crs(mpsz_sf) &lt;- 3414\n\n\n\nQuick View of Data\n\ntmap_mode(\"plot\")\nqtm(sg_sf) +\nqtm(mpsz_sf) +\nqtm(childcare_sf)\n\n\n\n\n\n\n\n\n\n\nQuick Interactive Plot of Data\nWe can also use tmap ‘view’ mode to plot our data layer on an interactive map, which uses Leaflet for R, where Leaflet is an open-source JavaScript library for interactive maps. In this mode, we can drag and zoom around and click on the features of our data layer to query its attribute information.\n\ntmap_mode('view')\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\n\n\n# reset back  to 'plot' mode\ntmap_mode('plot')\n\nNote from Instructor:\nReminder: Always remember to switch back to plot mode after the interactive map. This is because, each interactive mode will consume a connection. You should also avoid displaying ecessive numbers of interactive maps (i.e. not more than 10) in one RMarkdown document when publish on Netlify.",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "03 (Part 1) ‘1st Order Spatial Point Pattern Analysis’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex03/hands-on-ex03_01.html#geospatial-data-wrangling",
    "href": "hands-on-exercise/ex03/hands-on-ex03_01.html#geospatial-data-wrangling",
    "title": "03 (Part 1) ‘1st Order Spatial Point Patterns Analysis Methods’",
    "section": "3 Geospatial Data Wrangling",
    "text": "3 Geospatial Data Wrangling\n\n[Handling sp objects (Optional)] Convert sf DataFrame to sp’s Spatial* class (and Spatial*DataFrame).\n“Many geospatial analysis packages require the input geospatial data in sp’s Spatial*classes. But it is not always required, you can use sf instead.”\n\n# The code chunk below uses as_Spatial() of sf package to convert the three geospatial data from simple feature data frame to sp’s Spatial* class.\n\n# childcare &lt;- as_Spatial(from=childcare_sf)\n# mpsz &lt;- as_Spatial(from=mpsz_sf)\n# sg &lt;- as_Spatial(from=sg_sf)\n\n# also see different method\nchildcare &lt;- as(childcare_sf,\"Spatial\")\nmpsz &lt;- as(mpsz_sf,\"Spatial\")\nsg &lt;- as(sg_sf,\"Spatial\")\n\nShowing the output information\n\nchildcare\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 1\nnames       :                    NAME \nmin values  :    3-IN-1 FAMILY CENTRE \nmax values  : ZEE SCHOOLHOUSE PTE LTD \n\nmpsz\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 323 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 15\nnames       : OBJECTID, SUBZONE_NO, SUBZONE_N, SUBZONE_C, CA_IND, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C,          INC_CRC, FMEL_UPD_D,     X_ADDR,     Y_ADDR,    SHAPE_Leng,    SHAPE_Area \nmin values  :        1,          1, ADMIRALTY,    AMSZ01,      N, ANG MO KIO,         AM, CENTRAL REGION,       CR, 00F5E30B5C9B7AD8,      16409,  5092.8949,  19579.069, 871.554887798, 39437.9352703 \nmax values  :      323,         17,    YUNNAN,    YSSZ09,      Y,     YISHUN,         YS,    WEST REGION,       WR, FFCCF172717C2EAF,      16409, 50424.7923, 49552.7904, 68083.9364708,  69748298.792 \n\nsg\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 4\nnames       : GDO_GID, MSLINK, MAPID,              COSTAL_NAM \nmin values  :       1,      1,     0,             ISLAND LINK \nmax values  :      60,     67,     0, SINGAPORE - MAIN ISLAND \n\n\n\n\n[Handling sp objects (Optional)] Converting the Spatial* class into generic sp format\n“spatstat requires the analytical data in ppp object form. There is no direct way to convert a Spatial* classes into ppp object. We need to convert the Spatial classes* into a Spatial(sp) object first.”\n\nchildcare_sp &lt;- as(childcare, \"SpatialPoints\")\nsg_sp &lt;- as(sg, \"SpatialPolygons\")\n\nShowing the output information\n\nchildcare_sp\n\nclass       : SpatialPoints \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\nAt a glance, we can see that by converting Spatial*DataFrame to SpatialPoints or SpatialPolygons, we have lost all attribute information except the geometry features.\n\n\n[Handling sp objects (Optional)] Converting the generic sp format into spatstat’s ppp format\nNow, we will use as.ppp() function of spatstat to convert the spatial data into spatstat’s ppp object format\n(ChatGPT) Note a few things about the as.ppp() function:\n\nObservation Window: The as.ppp function needs an observation window to define the spatial context. Without explicitly setting this, the function might use a default or incorrect window, leading to inaccurate spatial analysis.\nCoordinate Extraction: The as.ppp function expects coordinates in a specific format. If childcare_sf is not properly converted, the function might misinterpret the data, resulting in errors or incorrect point patterns.\nData Integrity: Direct conversion without proper handling might ignore or misinterpret additional attributes or marks associated with the points, leading to loss of important information.\n\n\n# Check for duplicated points within original sf\nduplicated_coords &lt;- childcare_sf %&gt;% st_coordinates() %&gt;% duplicated()\nsum(duplicated_coords)\n\n[1] 74\n\n\n\n# childcare_ppp &lt;- as.ppp(childcare_sf)\n# childcare_ppp &lt;- as.ppp(x=childcare_sf$geometry, W=NULL)\nchildcare_ppp &lt;- as.ppp(st_coordinates(childcare_sf), st_bbox(childcare_sf)) %&gt;% unmark()\n\n# # Alternatively, to include attributes, use:\n# # Extract coordinates and attributes\n# coords &lt;- st_coordinates(childcare_sf)\n# # attributes &lt;- st_drop_geometry(childcare_sf)\n# # Define the observation window\n# bbox &lt;- st_bbox(childcare_sf)\n# window &lt;- owin(xrange = c(bbox[\"xmin\"], bbox[\"xmax\"]), yrange = c(bbox[\"ymin\"], bbox[\"ymax\"]))\n# # Create the ppp object with marks\n# childcare_ppp &lt;- ppp(x = coords[,1], y = coords[,2], window = window)\n\nchildcare_ppp\n\nPlanar point pattern: 1545 points\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\n\nCheck Whether Data is accurately converted into PPP object\n\noriginal_coords &lt;- st_coordinates(childcare_sf)\nppp_coords &lt;- cbind(childcare_ppp$x, childcare_ppp$y)\nall.equal(original_coords, ppp_coords)\n\n[1] \"Attributes: &lt; Length mismatch: comparison on first 1 components &gt;\"     \n[2] \"Attributes: &lt; Component \\\"dim\\\": Mean relative difference: 0.3333333 &gt;\"\n[3] \"Numeric: lengths (4635, 3090) differ\"                                  \n\n# identical(original_coords, ppp_coords)\n\nNote original_coords and ppp_coords have different precision of the coordinates when converted in this way.\n\n\nQuick View of PPP Plot\n\nplot(childcare_ppp)\n\n\n\n\n\n\n\n\n\n\nSee Summary Stats of Planar Point Point (PPP)\n\nsummary(childcare_ppp)\n\nPlanar point pattern:  1545 points\nAverage intensity 1.91145e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 11 decimal places\n\nWindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n                    (34200 x 23630 units)\nWindow area = 808287000 square units\n\n\n(Copy from Exercise) Prof’s Note: Notice the warning message about duplicates. In spatial point patterns analysis an issue of significant is the presence of duplicates. The statistical methodology used for spatial point patterns processes is based largely on the assumption that process are simple, that is, that the points cannot be coincident.\n\n\n\nHandling duplicated points\n\nany(duplicated(childcare_ppp))\n\n[1] TRUE\n\n\n(Copy from Exercise) To count the number of co-indicence point, we will use the multiplicity() function as shown in the code chunk below.\n\nmultiplicity(childcare_ppp)\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n   1    1    1    3    1    1    1    1    2    1    1    1    1    1    1    1 \n  17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 \n   1    1    1    1    1    1    1    1    1    1    9    1    1    1    1    1 \n  33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n  49   50   51   52   53   54   55   56   57   58   59   60   61   62   63   64 \n   1    1    1    1    1    1    2    1    1    3    1    1    1    1    1    1 \n  65   66   67   68   69   70   71   72   73   74   75   76   77   78   79   80 \n   1    1    1    1    1    2    1    1    1    1    1    2    1    1    1    1 \n  81   82   83   84   85   86   87   88   89   90   91   92   93   94   95   96 \n   1    1    1    3    1    1    1    1    1    1    1    1    1    1    1    1 \n  97   98   99  100  101  102  103  104  105  106  107  108  109  110  111  112 \n   1    1    1    1    1    1    1    1    2    1    1    1    1    1    1    1 \n 113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128 \n   1    1    1    1    1    1    2    1    1    1    3    1    1    1    2    1 \n 129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144 \n   1    1    1    1    1    2    1    1    1    1    1    1    1    1    3    2 \n 145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160 \n   1    2    1    1    1    2    2    3    1    5    1    5    1    1    1    2 \n 161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176 \n   1    1    1    1    2    1    1    1    1    1    1    2    1    1    1    1 \n 177  178  179  180  181  182  183  184  185  186  187  188  189  190  191  192 \n   1    4    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208 \n   1    1    1    1    1    2    2    1    1    1    1    2    1    4    1    1 \n 209  210  211  212  213  214  215  216  217  218  219  220  221  222  223  224 \n   2    1    1    1    1    1    1    1    1    1    1    1    2    1    1    1 \n 225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  240 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 241  242  243  244  245  246  247  248  249  250  251  252  253  254  255  256 \n   1    1    1    1    2    1    1    1    1    1    1    1    1    1    1    1 \n 257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    1    1    3 \n 273  274  275  276  277  278  279  280  281  282  283  284  285  286  287  288 \n   1    1    1    1    1    1    3    1    1    1    1    1    1    1    1    1 \n 289  290  291  292  293  294  295  296  297  298  299  300  301  302  303  304 \n   1    1    1    1    1    1    1    9    1    1    2    1    1    1    1    1 \n 305  306  307  308  309  310  311  312  313  314  315  316  317  318  319  320 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 321  322  323  324  325  326  327  328  329  330  331  332  333  334  335  336 \n   1    1    1    5    1    1    1    1    1    2    1    1    2    2    1    1 \n 337  338  339  340  341  342  343  344  345  346  347  348  349  350  351  352 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    2    2    1 \n 353  354  355  356  357  358  359  360  361  362  363  364  365  366  367  368 \n   1    1    1    1    9    1    1    1    1    1    1    1    1    1    1    1 \n 369  370  371  372  373  374  375  376  377  378  379  380  381  382  383  384 \n   1    3    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 385  386  387  388  389  390  391  392  393  394  395  396  397  398  399  400 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 401  402  403  404  405  406  407  408  409  410  411  412  413  414  415  416 \n   1    1    2    1    1    1    1    1    1    1    2    1    1    1    1    1 \n 417  418  419  420  421  422  423  424  425  426  427  428  429  430  431  432 \n   1    1    1    1    1    1    1    2    1    1    2    1    1    1    1    1 \n 433  434  435  436  437  438  439  440  441  442  443  444  445  446  447  448 \n   1    1    1    1    2    1    1    1    1    1    1    1    1    1    1    1 \n 449  450  451  452  453  454  455  456  457  458  459  460  461  462  463  464 \n   1    1    9    9    1    1    1    1    1    1    1    1    1    1    2    1 \n 465  466  467  468  469  470  471  472  473  474  475  476  477  478  479  480 \n   2    1    1    1    1    1    1    1    1    1    1    1    2    2    1    1 \n 481  482  483  484  485  486  487  488  489  490  491  492  493  494  495  496 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 497  498  499  500  501  502  503  504  505  506  507  508  509  510  511  512 \n   1    1    1    1    1    1    2    1    1    1    1    1    1    1    1    2 \n 513  514  515  516  517  518  519  520  521  522  523  524  525  526  527  528 \n   1    1    1    1    1    1    1    1    1    1    1    2    1    1    3    1 \n 529  530  531  532  533  534  535  536  537  538  539  540  541  542  543  544 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 545  546  547  548  549  550  551  552  553  554  555  556  557  558  559  560 \n   1    1    1    1    1    1    1    1    1    3    1    1    1    1    1    1 \n 561  562  563  564  565  566  567  568  569  570  571  572  573  574  575  576 \n   2    2    2    1    1    1    1    2    1    1    2    1    1    1    2    1 \n 577  578  579  580  581  582  583  584  585  586  587  588  589  590  591  592 \n   1    2    1    1    1    1    1    9    1    4    1    2    1    1    1    1 \n 593  594  595  596  597  598  599  600  601  602  603  604  605  606  607  608 \n   2    1    1    1    1    1    1    1    2    1    2    1    1    1    1    1 \n 609  610  611  612  613  614  615  616  617  618  619  620  621  622  623  624 \n   1    1    1    1    1    1    1    1    1    2    1    2    1    1    1    1 \n 625  626  627  628  629  630  631  632  633  634  635  636  637  638  639  640 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 641  642  643  644  645  646  647  648  649  650  651  652  653  654  655  656 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    4 \n 657  658  659  660  661  662  663  664  665  666  667  668  669  670  671  672 \n   1    1    1    1    1    1    1    3    1    1    1    1    1    1    1    1 \n 673  674  675  676  677  678  679  680  681  682  683  684  685  686  687  688 \n   1    1    1    1    1    4    1    1    1    1    1    4    1    1    1    1 \n 689  690  691  692  693  694  695  696  697  698  699  700  701  702  703  704 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 705  706  707  708  709  710  711  712  713  714  715  716  717  718  719  720 \n   1    1    2    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 721  722  723  724  725  726  727  728  729  730  731  732  733  734  735  736 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 737  738  739  740  741  742  743  744  745  746  747  748  749  750  751  752 \n   1    2    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 753  754  755  756  757  758  759  760  761  762  763  764  765  766  767  768 \n   1    1    1    1    1    2    1    1    1    1    1    1    1    1    1    1 \n 769  770  771  772  773  774  775  776  777  778  779  780  781  782  783  784 \n   1    1    1    1    1    1    1    1    1    4    1    1    1    1    1    1 \n 785  786  787  788  789  790  791  792  793  794  795  796  797  798  799  800 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 801  802  803  804  805  806  807  808  809  810  811  812  813  814  815  816 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 817  818  819  820  821  822  823  824  825  826  827  828  829  830  831  832 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 833  834  835  836  837  838  839  840  841  842  843  844  845  846  847  848 \n   1    1    1    1    1    1    1    2    1    1    1    1    1    1    1    1 \n 849  850  851  852  853  854  855  856  857  858  859  860  861  862  863  864 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 865  866  867  868  869  870  871  872  873  874  875  876  877  878  879  880 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    2 \n 881  882  883  884  885  886  887  888  889  890  891  892  893  894  895  896 \n   3    1    1    1    2    1    1    1    3    1    1    3    1    1    1    1 \n 897  898  899  900  901  902  903  904  905  906  907  908  909  910  911  912 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 913  914  915  916  917  918  919  920  921  922  923  924  925  926  927  928 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 929  930  931  932  933  934  935  936  937  938  939  940  941  942  943  944 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 945  946  947  948  949  950  951  952  953  954  955  956  957  958  959  960 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    2 \n 961  962  963  964  965  966  967  968  969  970  971  972  973  974  975  976 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 977  978  979  980  981  982  983  984  985  986  987  988  989  990  991  992 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 993  994  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 \n   1    1    1    1    1    1    1    1    1    2    2    1    1    1    1    1 \n1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 \n   1    1    1    1    1    2    1    1    1    1    1    1    1    1    1    1 \n1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 \n   1    1    1    1    1    1    1    1    2    2    1    1    1    5    1    1 \n1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    1    1    1 \n1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 \n   1    9    1    2    2    1    1    1    2    1    1    1    1    1    1    1 \n1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 \n   1    1    1    1    2    1    1    1    3    1    1    1    1    1    1    1 \n1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 \n   9    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    2 \n1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 \n   1    1    1    2    1    2    1    1    1    2    2    2    1    1    1    1 \n1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 \n   1    1    2    1    1    1    1    1    1    1    1    1    2    1    1    1 \n1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 \n   1    1    1    1    3    1    1    1    1    1    1    1    1    1    1    1 \n1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 \n   1    1    1    1    1    1    1    1    4    1    1    1    1    1    2    1 \n1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 \n   1    1    1    1    1    1    1    1    1    9    1    1    1    1    1    1 \n1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    2    1 \n1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 \n   1    2    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    1    1    1 \n1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 \n   1    1    1    1    1    1    2    1    1    1    1    1    1    1    1    1 \n1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 \n   1    1    1    1    1    1    1    1    1    1    5    1    1    1    1    1 \n1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 \n   1    1    1    1    1    2    1    1    1    1    2    1    1    1    1    3 \n1537 1538 1539 1540 1541 1542 1543 1544 1545 \n   1    1    1    1    1    1    2    1    1 \n\n\n(Copy from Exercise) If we want to know how many locations have more than one point event, we can use the code chunk below.\n\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n[1] 128\n\n\nThe output shows that there are 128 duplicated point events.\n\nduplicated_coords &lt;- st_coordinates(childcare_sf) %&gt;% duplicated()\nduplicated_points &lt;- childcare_sf[duplicated_coords,]\nnot_duplicated_points &lt;- childcare_sf[!duplicated_coords,]\n\ntmap_mode('view')\n\ntm_shape(childcare_sf) +\n  tm_dots(alpha=0.4,col =\"black\",size=0.05) +\ntm_shape(duplicated_points) +\n  tm_dots(alpha=1,col =\"red\",size=0.05)\n\n\n\n\ntmap_mode('plot')\n\n(Copied from Exercise) There are three ways to overcome this problem.\n\nDelete duplicates. But that will also mean that some useful point events will be lost.\nUse jittering, which will add a small perturbation to the duplicate points so that they do not occupy the exact same space.\nMake each point “unique” and then attach the duplicates of the points to the patterns as marks, as attributes of the points. Then you would need analytical techniques that take into account these marks.\n\nThe code chunk below implements the jittering approach.\n\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\nsum(duplicated(childcare_ppp_jit))\n\n[1] 0\n\nany(duplicated(childcare_ppp_jit))\n\n[1] FALSE\n\n\n\n\nCreating owin object\n(Copied from Exercise) When analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area like Singapore boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to covert sg SpatialPolygon object into owin object of spatstat.\n\nsg_owin &lt;- as.owin(sg_sf)\nplot(sg_owin)\n\n\n\n\n\n\n\n\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\n\n\nCombining point events object and owin object\n(Copied from Exercise) In this last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the code chunk below\n\nchildcareSG_ppp = childcare_ppp[sg_owin]\nsummary(childcareSG_ppp)\n\nPlanar point pattern:  1545 points\nAverage intensity 2.129929e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 11 decimal places\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\n\nplot(childcareSG_ppp)",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "03 (Part 1) ‘1st Order Spatial Point Pattern Analysis’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex03/hands-on-ex03_01.html#first-order-spatial-point-patterns-analysis",
    "href": "hands-on-exercise/ex03/hands-on-ex03_01.html#first-order-spatial-point-patterns-analysis",
    "title": "03 (Part 1) ‘1st Order Spatial Point Patterns Analysis Methods’",
    "section": "4 First-order Spatial Point Patterns Analysis",
    "text": "4 First-order Spatial Point Patterns Analysis\nIn this section, you will learn how to perform first-order SPPA by using spatstat package. The hands-on exercise will focus on:\n\nderiving kernel density estimation (KDE) layer for visualising and exploring the intensity of point processes,\nperforming Confirmatory Spatial Point Patterns Analysis by using Nearest Neighbour statistics.\n\n\nKernel Density Estiation (KDE)\n\nkde_childcareSG_bw &lt;- density(childcareSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                              kernel=\"gaussian\")\n\n\nplot(kde_childcareSG_bw)\n\n\n\n\n\n\n\n\nNote: The density values of the output range from 0 to 0.000035 which is way too small to comprehend. This is because the default unit of measurement of svy21 is in meters. As a result, the density values computed is in “number of points per square meter”.\n\n# Retrieving bandwidth of the kde layer\n\nbw &lt;- bw.diggle(childcareSG_ppp)\nbw\n\n   sigma \n298.4095 \n\n\n\n\nRescaling KDE Values (since current values are too small)\n\nchildcareSG_ppp.km &lt;- rescale.ppp(childcareSG_ppp, 1000, \"km\")\n\n\nkde_childcareSG.bw &lt;- density(childcareSG_ppp.km, \n                              sigma=bw.diggle, \n                              edge=TRUE, \n                              kernel=\"gaussian\")\n\n\nplot(kde_childcareSG.bw)\n\n\n\n\n\n\n\n\nThe value now ranges from 0 to 25+, showing number of points per km square.\n\n\nWorking with different automatic bandwidth methods\n\nbw.diggle(childcareSG_ppp.km)\n\n    sigma \n0.2984095 \n\n\n\nbw.CvL(childcareSG_ppp.km)\n\n   sigma \n4.543278 \n\n\n\nbw.scott(childcareSG_ppp.km)\n\n sigma.x  sigma.y \n2.224898 1.450966 \n\n\n\nbw.ppl(childcareSG_ppp.km)\n\n    sigma \n0.3897114 \n\n\n\nComparing Plots between using bw.diggle vs bw.ppl\n\nkde_childcareSG.ppl &lt;- density(childcareSG_ppp.km, \n                               sigma=bw.ppl, \n                               edge=TRUE,\n                               kernel=\"gaussian\")\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.ppl, main = \"bw.ppl\")\n\n\n\n\n\n\n\n\n\n\n\nWorking with different kernel methods\nBy default, the kernel method used in density.ppp() is gaussian. But there are three other options, namely: Epanechnikov, Quartic and Dics.\n\npar(mfrow=c(2,2))\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"gaussian\"), \n     main=\"Gaussian\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"epanechnikov\"), \n     main=\"Epanechnikov\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"quartic\"), \n     main=\"Quartic\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"disc\"), \n     main=\"Disc\")",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "03 (Part 1) ‘1st Order Spatial Point Pattern Analysis’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex03/hands-on-ex03_01.html#fixed-and-adaptive-kde",
    "href": "hands-on-exercise/ex03/hands-on-ex03_01.html#fixed-and-adaptive-kde",
    "title": "03 (Part 1) ‘1st Order Spatial Point Patterns Analysis Methods’",
    "section": "5 Fixed and Adaptive KDE",
    "text": "5 Fixed and Adaptive KDE\nNext, you will compute a KDE layer by defining a bandwidth of 600 meter. Notice that in the code chunk below, the sigma value used is 0.6. This is because the unit of measurement of childcareSG_ppp.km object is in kilometer, hence the 600m is 0.6km.\n\nkde_childcareSG_600 &lt;- \n  density(childcareSG_ppp.km, \n          sigma=0.6, \n          edge=TRUE, \n          kernel=\"gaussian\")\n\nplot(kde_childcareSG_600)\n\n\n\n\n\n\n\n\n\nComputing KDE by using adaptive bandwidth\nFixed bandwidth method is very sensitive to highly skew distribution of spatial point patterns over geographical units for example **urban** versus **rural**. One way to overcome this problem is by using adaptive bandwidth instead.\n\nIn this section, you will learn how to derive adaptive kernel density estimation by using [density.adaptive()](https://rdrr.io/cran/spatstat/man/adaptive.density.html) of **spatstat**.\n\nkde_childcareSG_adaptive &lt;- \n  adaptive.density(childcareSG_ppp.km, \n                   method=\"kernel\")\n\n\nplot(kde_childcareSG_adaptive)\n\n\n\n\n\n\n\n\nWe can compare the fixed and adaptive kernel density estimation outputs by using the code chunk below.\n\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\n\n\n\n\n\nConverting KDE output into grid object\n\nclass(kde_childcareSG.bw)\n\n[1] \"im\"\n\n\n\ngridded_kde_childcareSG_bw &lt;- \n  #raster(kde_childcareSG.bw) %&gt;% \n  as(kde_childcareSG.bw,\"SpatialGridDataFrame\")\n  \n# as.SpatialGridDataFrame.im(kde_childcareSG.bw) \n# this uses maptools, use maptools::as.SpatialGridDataFrame.im(kde_childcareSG.bw)\n\nspplot(gridded_kde_childcareSG_bw)\n\n\n\n\n\n\n\n\n\n\nConverting gridded output into raster\n\nkde_childcareSG_bw_raster &lt;- raster(kde_childcareSG.bw)\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : layer \nvalues     : -8.476185e-15, 28.51831  (min, max)\n\n\nNotice that the crs property is NA.\n\n\nAssigning projection systems\n\n# function to set the CRS of a Raster* object\nprojection(kde_childcareSG_bw_raster) &lt;- CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +units=m +no_defs \nsource     : memory\nnames      : layer \nvalues     : -8.476185e-15, 28.51831  (min, max)\n\n\n\n\nVisualising the RasterLayer in tmap\nFinally, we will display the raster in cartographic quality map using tmap package.\n\ntmap_mode(\"plot\")\ntm_shape(kde_childcareSG_bw_raster) + \n  tm_raster(col = \"layer\", palette = \"viridis\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = TRUE)\n\n\n\n\n\n\n\n\n\n\nComparing the Spatial Point Patterns using KDE\nIn this section, you will learn how to compare KDE of childcare at Ponggol, Tampines, Chua Chu Kang and Jurong West planning areas.\nWe will extract the relevant Study Areas.\n\npunggol &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"PUNGGOL\")\ntampines &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"TAMPINES\")\nchoachukang &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\njurongwest &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"JURONG WEST\")\n\n\n# par(mfrow=c(2,2))\n# plot(punggol, main=\"PUNGGOL\")\nplot(punggol)\n\n\n\n\n\n\n\n\n\n# plot(tm, main = \"Tampines\")\nplot(tampines)\n\n\n\n\n\n\n\n\n\n# plot(ck, main = \"Choa Chu Kang\")\nplot(choachukang)\n\n\n\n\n\n\n\n\n\n# plot(jw, main = \"Jurong West\")\nplot(jurongwest)\n\n\n\n\n\n\n\n\n\npg_owin = as.owin(punggol)\ntm_owin = as.owin(tampines)\nck_owin = as.owin(choachukang)\njw_owin = as.owin(jurongwest)\n\n\n\nCombining childcare points and the study area\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\n\nchildcare_pg_ppp_km = rescale.ppp(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp_km = rescale.ppp(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp_km = rescale.ppp(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp_km = rescale.ppp(childcare_jw_ppp, 1000, \"km\")\n\n\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp_km, main=\"Punggol\")\nplot(childcare_tm_ppp_km, main=\"Tampines\")\nplot(childcare_ck_ppp_km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp_km, main=\"Jurong West\")\n\n\n\n\n\n\n\n\n\n\nComputing KDE\n\npar(mfrow=c(2,2))\nplot(density(childcare_pg_ppp_km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp_km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tempines\")\nplot(density(childcare_ck_ppp_km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Choa Chu Kang\")\nplot(density(childcare_jw_ppp_km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"JUrong West\")\n\n\n\n\n\n\n\n\n\n\nComputing fixed bandwidth KDE\nFor comparison purposes, we will use 250m as the bandwidth.\n\npar(mfrow=c(2,2))\nplot(density(childcare_ck_ppp_km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Chou Chu Kang\")\nplot(density(childcare_jw_ppp_km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\nplot(density(childcare_pg_ppp_km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp_km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "03 (Part 1) ‘1st Order Spatial Point Pattern Analysis’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex03/hands-on-ex03_01.html#nearest-neighbour-analysis",
    "href": "hands-on-exercise/ex03/hands-on-ex03_01.html#nearest-neighbour-analysis",
    "title": "03 (Part 1) ‘1st Order Spatial Point Patterns Analysis Methods’",
    "section": "6 Nearest Neighbour Analysis",
    "text": "6 Nearest Neighbour Analysis\nIn this section, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using [*clarkevans.test()*](https://www.rdocumentation.org/packages/spatstat/versions/1.63-3/topics/clarkevans.test) of **statspat**.\n\nThe test hypotheses are:\n\nHo = The distribution of childcare services are randomly distributed.\n\nH1= The distribution of childcare services are not randomly distributed.\n\nThe 95% confident interval will be used.\n\nTesting spatial point patterns using Clark and Evans Test\nUsing childcareSG_PPP ’\n\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcareSG_ppp\nR = 0.55631, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\nUsing childcare_ck_ppp\n\nclarkevans.test(childcare_ck_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_ck_ppp\nR = 0.90679, p-value = 0.1637\nalternative hypothesis: two-sided\n\n\n\n\nClark and Evans Test:Tampines planning area\n\nclarkevans.test(childcare_tm_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_tm_ppp\nR = 0.79782, p-value = 0.0002432\nalternative hypothesis: two-sided\n\n\nTO BE COMPLETED",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "03 (Part 1) ‘1st Order Spatial Point Pattern Analysis’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex03/hands-on-ex03_02.html",
    "href": "hands-on-exercise/ex03/hands-on-ex03_02.html",
    "title": "03 (Part 2) ‘2nd Order Spatial Point Patterns Analysis Methods’",
    "section": "",
    "text": "This is the Part 2 of the Point Pattern Analysis\nSame data will be used as Hands-on Exercise 3 (Part 1), namely:",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "03 (Part 2) ‘2nd Order Spatial Point Pattern Analysis’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex03/hands-on-ex03_02.html#importing-packages-and-data",
    "href": "hands-on-exercise/ex03/hands-on-ex03_02.html#importing-packages-and-data",
    "title": "03 (Part 2) ‘2nd Order Spatial Point Patterns Analysis Methods’",
    "section": "1 Importing Packages and Data",
    "text": "1 Importing Packages and Data\n\npacman::p_load(sf, spatstat, raster, tmap, rvest, tidyverse)\n\n\n# childcare services data\nchildcare_sf &lt;- st_read(dsn=\"data/geospatial/child-care-services-geojson.geojson\")\n\n# coastal outline data\nsg_sf &lt;- st_read(dsn=\"data/geospatial\", layer=\"CostalOutline\")\n\n# master plan subzone boundary data\nmpsz_sf &lt;- st_read(dsn=\"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\n# notice childcare_sf is in WGS84, transform it into SV21:\nchildcare_sf &lt;- childcare_sf %&gt;% st_transform(crs=3414)\n\n\n# childcare_sf\n# sg_sf\n# mpsz_sf\n\n\n# sg_sf # need to set CRS, no need transform\n# mpsz_sf # need to set CRS, no need transform\n\n\n# change CRS to EPSG:3414\nst_crs(sg_sf) &lt;- 3414\nst_crs(mpsz_sf) &lt;- 3414\n\nst_crs(sg_sf) # need to set CRS, no need transform\nst_crs(mpsz_sf) # need to set CRS, no need transform\n\n\nGeospatial Data Wrangling\n\nchildcare_ppp &lt;- \n  as.ppp(st_coordinates(childcare_sf), \n         st_bbox(childcare_sf)) %&gt;% \n  unmark()\nchildcare_ppp\n\nPlanar point pattern: 1545 points\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\n\nplot(childcare_ppp)\n\n\n\n\n\n\n\nsummary(childcare_ppp)\n\nPlanar point pattern:  1545 points\nAverage intensity 1.91145e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 11 decimal places\n\nWindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n                    (34200 x 23630 units)\nWindow area = 808287000 square units\n\n\n\n# any(duplicated(childcare_ppp))\n# sum(duplicated(childcare_ppp))\n# multiplicity(childcare_ppp)\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n[1] 128\n\n\n\ntmap_mode('view')\ntm_shape(childcare_sf) +\n  tm_dots(alpha=0.5, \n          size=0.05)\n\n\n\n\ntmap_mode('plot')\n\n\n# jittering, which will add a small perturbation to the duplicate points so that they do not occupy the exact same space.\nchildcare_ppp_jit &lt;- \n  rjitter(childcare_ppp, \n  retry=TRUE, \n  nsim=1, \n  drop=TRUE)\n\n\n# check for any duplicated points\nany(duplicated(childcare_ppp_jit))\n\n[1] FALSE\n\nsum(duplicated(childcare_ppp_jit))\n\n[1] 0\n\n# multiplicity(childcare_ppp_jit)\nsum(multiplicity(childcare_ppp_jit) &gt; 1)\n\n[1] 0\n\n\n\n\nCreate SG Owin object (Window) to Extract Regional PPP\n\nsg_owin &lt;- as.owin(sg_sf)\nplot(sg_owin)\n\n\n\n\n\n\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\n\n# Combines point events object and Owin object \n# Extract only childcare point events within Singapore's boundaries\nchildcareSG_ppp &lt;- childcare_ppp_jit[sg_owin]\n\n## Observe, any difference?\nchildcare_ppp\n\nPlanar point pattern: 1545 points\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\nchildcareSG_ppp\n\nPlanar point pattern: 1545 points\nwindow: polygonal boundary\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n\nclass(childcare_ppp)\n\n[1] \"ppp\"\n\nclass(childcareSG_ppp)\n\n[1] \"ppp\"\n\n\n\nsummary(childcareSG_ppp)\n\nPlanar point pattern:  1545 points\nAverage intensity 2.129929e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\n\nplot(childcareSG_ppp)\n\n\n\n\n\n\n\n\n\n# Extract Planning Areas\npg &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"PUNGGOL\")\ntm &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"TAMPINES\")\nck &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\njw &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"JURONG WEST\")\n\n\nplot(pg, main=\"OVERVIEW OF PUNGGOL DATA\")\n\n\n\n\n\n\n\nplot(tm, main = \"Tampines\")\n\n\n\n\n\n\n\nplot(ck, main = \"Choa Chu Kang\")\n\n\n\n\n\n\n\nplot(jw, main = \"Jurong West\")\n\n\n\n\n\n\n\n\n\n# Convert into Owin objects (to extract PPP for each planning areas):\npg_owin = as.owin(pg)\ntm_owin = as.owin(tm)\nck_owin = as.owin(ck)\njw_owin = as.owin(jw)\n\nclass(pg_owin)\n\n[1] \"owin\"\n\nclass(tm_owin)\n\n[1] \"owin\"\n\nclass(ck_owin)\n\n[1] \"owin\"\n\nclass(jw_owin)\n\n[1] \"owin\"\n\n\n\n# After selecting the Owin object, we can extract ppp within Owin object.\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\n\n# Transform by 1000 times, from m to km, for KDE later\nchildcare_pg_ppp.km = rescale(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale(childcare_jw_ppp, 1000, \"km\")\n\n\n# Display plot overview\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "03 (Part 2) ‘2nd Order Spatial Point Pattern Analysis’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex03/hands-on-ex03_02.html#analysing-spatial-point-process-using-g-function",
    "href": "hands-on-exercise/ex03/hands-on-ex03_02.html#analysing-spatial-point-process-using-g-function",
    "title": "03 (Part 2) ‘2nd Order Spatial Point Patterns Analysis Methods’",
    "section": "2 Analysing Spatial Point Process Using G-Function",
    "text": "2 Analysing Spatial Point Process Using G-Function\n\nChoa Chu Kang planning area\n\nNearest Neighbour Distance Function G\nThe G function measures the distribution of the distances from an arbitrary event to its nearest event. In this section, you will learn how to compute G-function estimation by using Gest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\nGest(X, r=NULL, breaks=NULL, …,\n    correction=c(“rs”, “km”, “han”),\n    domain=NULL)\n\n# Compute G function, using Gest() of spatat package\nG_CK = Gest(childcare_ck_ppp, correction = \"border\")\nplot(G_CK, xlim=c(0,500), main=\"G-Function, Choa Chu Kang\")\n\n\n\n\n\n\n\n\n\n\nPerforming Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with G-function\n\nG_CK.csr &lt;- envelope(childcare_ck_ppp, Gest, nsim = 999)\n\n\nplot(G_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\nTampines planning area\n\nNearest Neighbour Distance Function G\n\nG_tm = Gest(childcare_tm_ppp, correction = \"all\")\nplot(G_tm)\n\n\n\n\n\n\n\n\n\n\nPerforming Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nG_tm.csr &lt;- envelope(childcare_tm_ppp, Gest, correction = \"all\", nsim = 999)\n\n\nplot(G_tm.csr)",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "03 (Part 2) ‘2nd Order Spatial Point Pattern Analysis’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex03/hands-on-ex03_02.html#analysing-spatial-point-process-using-f-function",
    "href": "hands-on-exercise/ex03/hands-on-ex03_02.html#analysing-spatial-point-process-using-f-function",
    "title": "03 (Part 2) ‘2nd Order Spatial Point Patterns Analysis Methods’",
    "section": "3 Analysing Spatial Point Process Using F-Function",
    "text": "3 Analysing Spatial Point Process Using F-Function\n\nChoa Chu Kang planning area\n\nEstimate the Empty Space Function or its Hazard Rate Function F(r) or h(r)\nThe F function estimates the empty space function F(r) or its hazard rate h(r) from a point pattern in a window of arbitrary shape. In this section, you will learn how to compute F-function estimation by using Fest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\nFest(X, …, eps, r=NULL, breaks=NULL,\n    correction=c(“rs”, “km”, “cs”),\n    domain=NULL)\n\nF_CK = Fest(childcare_ck_ppp)\nplot(F_CK)\n\n\n\n\n\n\n\n\n\n\nPerforming Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with F-fucntion\n\nF_CK.csr &lt;- envelope(childcare_ck_ppp, Fest, nsim = 999)\n\n\nplot(F_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\nTampines planning area\n\nEstimate the Empty Space Function or its Hazard Rate Function F(r) or h(r)\n\nF_tm = Fest(childcare_tm_ppp, correction = \"best\")\nplot(F_tm)\n\n\n\n\n\n\n\n\n\n\nPerforming Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nF_tm.csr &lt;- envelope(childcare_tm_ppp, Fest, correction = \"all\", nsim = 999)\n\n\nplot(F_tm.csr)",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "03 (Part 2) ‘2nd Order Spatial Point Pattern Analysis’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex03/hands-on-ex03_02.html#summary",
    "href": "hands-on-exercise/ex03/hands-on-ex03_02.html#summary",
    "title": "03 (Part 2) ‘2nd Order Spatial Point Patterns Analysis Methods’",
    "section": "Summary",
    "text": "Summary\nEstimate Functions we went through:\n\nGest(), Fest(), Kest(), Lest()\nThe G function measures the distribution of the distances from an arbitrary event to its nearest event.\nThe F function estimates the empty space function F(r) or its hazard rate h(r) from a point pattern in a window of arbitrary shape.\nK-function measures the number of events found up to a given distance of any particular event\nThe command Lest first calls Kest to compute the estimate of the K-function, and then applies the square root transformation. The transformation to LL was proposed by Besag (1977).\n\nFollowed by a Complete Spatial Randomness Test:\n\nIf the data exhibits complete spatial randomness, this implies that there is no underlying structure in the data.\nfirst compute simulation envelopes of a summary function.\nTO BE STUDIED",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "03 (Part 2) ‘2nd Order Spatial Point Pattern Analysis’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex03/hands-on-ex03_02.html#analysing-spatial-point-process-using-k-function",
    "href": "hands-on-exercise/ex03/hands-on-ex03_02.html#analysing-spatial-point-process-using-k-function",
    "title": "03 (Part 2) ‘2nd Order Spatial Point Patterns Analysis Methods’",
    "section": "4 Analysing Spatial Point Process Using K-Function",
    "text": "4 Analysing Spatial Point Process Using K-Function\nK-function measures the number of events found up to a given distance of any particular event. In this section, you will learn how to compute K-function estimates by using Kest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n Kest(X, …, r=NULL, rmax=NULL, breaks=NULL,\n    correction=c(“border”, “isotropic”, “Ripley”, “translate”),\n   nlarge=3000, domain=NULL, var.approx=FALSE, ratio=FALSE)\n\nChoa Chu Kang planning area\n\nEstimate K-Function\n\nK_ck = Kest(childcare_ck_ppp, correction = \"Ripley\")\nplot(K_ck, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n\n\n\nPerforming Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nK_ck.csr &lt;- envelope(childcare_ck_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\n\nplot(K_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"K(d)-r\")\n\n\n\n\n\n\n\n\n\n\n\nTampines planning area\n\nEstimate K-Function\n\nK_tm = Kest(childcare_tm_ppp, correction = \"Ripley\")\nplot(K_tm, . -r ~ r, \n     ylab= \"K(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n\n\n\n\nPerforming Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nK_tm.csr &lt;- envelope(childcare_tm_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\n\nplot(K_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"K(d)-r\", xlim=c(0,500))",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "03 (Part 2) ‘2nd Order Spatial Point Pattern Analysis’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex03/hands-on-ex03_02.html#analysing-spatial-point-process-using-l-function",
    "href": "hands-on-exercise/ex03/hands-on-ex03_02.html#analysing-spatial-point-process-using-l-function",
    "title": "03 (Part 2) ‘2nd Order Spatial Point Patterns Analysis Methods’",
    "section": "5 Analysing Spatial Point Process Using L-Function",
    "text": "5 Analysing Spatial Point Process Using L-Function\nIn this section, you will learn how to compute L-function estimation by using Lest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\nChoa Chu Kang planning area\n\nEstimate L Function\n\nL_ck = Lest(childcare_ck_ppp, correction = \"Ripley\")\nplot(L_ck, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n\n\n\nPerforming Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value if smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nL_ck.csr &lt;- envelope(childcare_ck_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\n\nplot(L_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\n\n\n\n\n\n\n\n\n\nTampines planning area\n\nEstimate L Function\n\nL_tm = Lest(childcare_tm_ppp, correction = \"Ripley\")\nplot(L_tm, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n\n\n\n\nPerforming Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below will be used to perform the hypothesis testing.\n\nL_tm.csr &lt;- envelope(childcare_tm_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\n\nplot(L_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"L(d)-r\", xlim=c(0,500))",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "03 (Part 2) ‘2nd Order Spatial Point Pattern Analysis’"
    ]
  },
  {
    "objectID": "in-class-exercise/ex03/in-class-ex03.html",
    "href": "in-class-exercise/ex03/in-class-ex03.html",
    "title": "03 In-class Exercise 3 (Review)",
    "section": "",
    "text": "Review on Spatial Point Patterns in Real World\nDistribution of diseases such as dengue fever – note recorded based on homeaddress, not where you’ve bitten\nDistribution of car collisions (traffic accidents) – note recorded on-site\nDistribution of education institution\nDistribution of social media data such as tweets through web crawling, and signal is from the celltower, not very exact.\nAlways take note:\n\nAlways know where the source of your data is from.\nYou cannot take a sample from the population from spatial point events and analyse it.\nYou can get a complete subset like “kindergarten” from a subset of “childcare centre”, but you cannot do a random sampling out of “kindergarten” like when you do usual statistical analysis.\n\nWhat we want to know:\nIs the point even randomly distributed? Almost never will we get random distribution in the real world, unless you are doing a very micro-scale study.\n\n\n\nTypes of spatial points analysis:\n– 1st order: exploratory data analysis – visualise data points – 2nd order:\nWe are not able to objectively visualise the intensity on a map (interpretation can change depending on how big your points are). To do so objectively one method is to use Kernal Density Estimation(KDE).\nWhy analysis? Using KDE:\n– We are determining spatial distribution of poins are Either random or not random. – If not random, either clustered or uniform (dispersed)\n\n\n\nFixed Bandwidths – we can fixed search radius for our KDE, but this is not always realistic.\nAdaptive Bandwidths – More realistically, we know that not every place has the same geographical/physical size. Tampines is larger than Choa Chu Kang, do you use the same search distance for both area and determine the density, or use a larger bandwidth for Tampines and smaller for Choa Chu Kang?\n– Having different bandwidths, which one is accurate? They are all accurate! – Interpretation? Which one best shows you the most appropriate interpretation?\n\n\n\n\n\nKernal Interpolation.. Note the differences between Quartic and Gaussian - They are similar, but they give different results - Gaussian may result in negative results (e.g. -1.0 kernel density estimation for childcare -&gt; doesn’t make sense realistically) - If you want to avoid negative results, but want to use a normal-distributinon-like interpolation function, use Quartic instead.\n\n\n\nTo declare whether the points events are random, we say:\n\nNull Hypothesis, H0 = Observed point process (or point events) resemble complete Spatial Random Distribution.\nAlternative Hypothesis, H1 = Observed point process (or point events) do not resemble complete Spatial Random Distribution.\n\nHomogenous distribution–complete spatial randomness–is an ‘ideal’ situation. It should not happen. If we happen to not have enough evidence to reject the null hypothesis, it does not mean the null hypothesis is correct. It simply means our evidence is not enough – either error in sampling, or sampling size is too small such that it does not represent the population.\n\n\n\nWhat is this “population” that we are comparing with? We basically do a Monte Carlo simulation (multiple simulations) to come up with a spatially randomly distributed point events within our study region.\n– For each simulated point pattern, estimate G(r) and use the maximum (95th) and minimum (5th) of these functions for the simulated patterns to define an upper and lower simulation envelope. (the max and min can be varied, need not be 95%)\n– If the estimated G(r) lies above the upper envelope or below the lower envelope, the estimated G(r) is statistically significant.\n\n\n\nIn spatstat’s simulations, minimum number of simulation for 95% confidence is 40 (R counts from 0, use 39 in code) simulations. (according to spatstat’s documentation):\n\nHaving larger number of sampling increases our confidence in the final result.\nUsually it’s easier to remember and cleaner to use a easier to read number like 99 (which produces 100 simulations)\nRemember to always use a seed value for the random generation, so that you get a reproduceable result.\nYou just need to set a seed for all algorithms that deals with randomness at the top of the document, no need to repeat it for every code chunk.\n\n\n\n\nBefore you begin your analysis, always set your Confidence Interval – 90%, 95%, 99%. This will help to determine the alpha value for your analysis, where alpha = 1 - (confidence interval/100)\n\nWhy? Note that not everything is certain in the real world, you have to know that there is a limit where we can no longer be confident in our result.\np-value by itself does not mean anything other than the probability of the occurrence of a given event. This probability has to be significantly higher (or lower) than a certain value (in here, the alpha value) to be determined significant.\n\n\n\n\n\n\nGrey Zone indicates the confidence envelop (95%), and anything that falls within we are not confident that point spatial distribution is not random.\nThe red diagonal line (break line) separates at 50% the upper confidence envelope and lower confidence envelope.\nOutside the Grey Zone, if – In above break line, then there are signs of clustering – In below break line, then there are signs of uniform distribution\nNote we also look out for from which distance to which distance where we are confident clustering occurs.\n\nGenerally, we use G and K estimation functions (similar to K is L function, which is a standardised function of K), do you know why? TO BE COMPLETED\n– Extra Notes: L function is a transformation of K function. We divide by pi (as each study area is a circle), we further subtract away the radius of each study area to straighten the break line into a straight line. – Extra Notes: Difference between G and K:\n\nBoth functions are zonal, they search a radius around each point.\nG function uses a fixed set of radius (fixed bandwidth)\nK function uses a search radius relative to surrounding points, e.g. the nearest point is 100m away, the first search is 100m around point, the next nearest point is 200m away, we do a search 200m around point.",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "03 In-class Exercise 3 (Review)"
    ]
  },
  {
    "objectID": "in-class-exercise/ex03/in-class-ex03.html#in-class",
    "href": "in-class-exercise/ex03/in-class-ex03.html#in-class",
    "title": "03 Exercise Review 3",
    "section": "",
    "text": "Review on Spatial Point Patterns in Real World\nDistribution of diseases such as dengue fever – note recorded based on homeaddress, not where you’ve bitten\nDistribution of car collisions (traffic accidents) – note recorded on-site\nDistribution of education institution\nDistribution of social media data such as tweets through web crawling, and signal is from the celltower, not very exact.\nAlways take note:\n\nAlways know where the source of your data is from.\nYou cannot take a sample from the population from spatial point events and analyse it.\nYou can get a complete subset like “kindergarten” from a subset of “childcare centre”, but you cannot do a random sampling out of “kindergarten” like when you do usual statistical analysis.\n\nWhat we want to know:\nIs the point even randomly distributed? Almost never will we get random distribution in the real world, unless you are doing a very micro-scale study.\nTypes of spatial points:\n– 1st order: exploratory data analysis – visualise data points – 2nd order:\nWe are not able to objectively visualise the intensity on a map (interpretation can change depending on how big your points are). To do so objectively one method is to use Kernal Density Estimation(KDE).\nFrom KDE:\n– Either random or not random. – If not random, either clustered or uniform (dispersed)\nKDE:\nFixed Bandwidths Adaptive Bandwidths – Not every place has the same geographical/physical size. Tampines is larger than Choa Chu Kang, do you use the same search distance for both area and determine the density, or use a larger bandwidth for Tampines and smaller for Choa Chu Kang?\n– Having different bandwidths, which one is accurate? They are all accurate! – Interpretation? Which one best shows you the most appropriate interpretation?",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "03 Exercise Review 3"
    ]
  },
  {
    "objectID": "in-class-exercise/ex03/in-class-ex03.html#in-class-review",
    "href": "in-class-exercise/ex03/in-class-ex03.html#in-class-review",
    "title": "03 Exercise Review 3",
    "section": "",
    "text": "Review on Spatial Point Patterns in Real World\nDistribution of diseases such as dengue fever – note recorded based on homeaddress, not where you’ve bitten\nDistribution of car collisions (traffic accidents) – note recorded on-site\nDistribution of education institution\nDistribution of social media data such as tweets through web crawling, and signal is from the celltower, not very exact.\nAlways take note:\n\nAlways know where the source of your data is from.\nYou cannot take a sample from the population from spatial point events and analyse it.\nYou can get a complete subset like “kindergarten” from a subset of “childcare centre”, but you cannot do a random sampling out of “kindergarten” like when you do usual statistical analysis.\n\nWhat we want to know:\nIs the point even randomly distributed? Almost never will we get random distribution in the real world, unless you are doing a very micro-scale study.\n\n\n\nTypes of spatial points analysis:\n– 1st order: exploratory data analysis – visualise data points – 2nd order:\nWe are not able to objectively visualise the intensity on a map (interpretation can change depending on how big your points are). To do so objectively one method is to use Kernal Density Estimation(KDE).\nWhy analysis? Using KDE:\n– We are determining spatial distribution of poins are Either random or not random. – If not random, either clustered or uniform (dispersed)\n\n\n\nFixed Bandwidths – we can fixed search radius for our KDE, but this is not always realistic.\nAdaptive Bandwidths – More realistically, we know that not every place has the same geographical/physical size. Tampines is larger than Choa Chu Kang, do you use the same search distance for both area and determine the density, or use a larger bandwidth for Tampines and smaller for Choa Chu Kang?\n– Having different bandwidths, which one is accurate? They are all accurate! – Interpretation? Which one best shows you the most appropriate interpretation?\n\n\n\n\n\nKernal Interpolation.. Note the differences between Quartic and Gaussian - They are similar, but they give different results - Gaussian may result in negative results (e.g. -1.0 kernel density estimation for childcare -&gt; doesn’t make sense realistically) - If you want to avoid negative results, but want to use a normal-distributinon-like interpolation function, use Quartic instead.\n\n\n\nTo declare whether the points events are random, we say:\n\nNull Hypothesis, H0 = Observed point process (or point events) resemble complete Spatial Random Distribution.\nAlternative Hypothesis, H1 = Observed point process (or point events) do not resemble complete Spatial Random Distribution.\n\nHomogenous distribution–complete spatial randomness–is an ‘ideal’ situation. It should not happen. If we happen to not have enough evidence to reject the null hypothesis, it does not mean the null hypothesis is correct. It simply means our evidence is not enough – either error in sampling, or sampling size is too small such that it does not represent the population.\n\n\n\nWhat is this “population” that we are comparing with? We basically do a Monte Carlo simulation (multiple simulations) to come up with a spatially randomly distributed point events within our study region.\n– For each simulated point pattern, estimate G(r) and use the maximum (95th) and minimum (5th) of these functions for the simulated patterns to define an upper and lower simulation envelope. (the max and min can be varied, need not be 95%)\n– If the estimated G(r) lies above the upper envelope or below the lower envelope, the estimated G(r) is statistically significant.\n\n\n\nIn spatstat’s simulations, minimum number of simulation for 95% confidence is 40 (R counts from 0, use 39 in code) simulations. (according to spatstat’s documentation):\n\nHaving larger number of sampling increases our confidence in the final result.\nUsually it’s easier to remember and cleaner to use a easier to read number like 99 (which produces 100 simulations)\nRemember to always use a seed value for the random generation, so that you get a reproduceable result.\nYou just need to set a seed for all algorithms that deals with randomness at the top of the document, no need to repeat it for every code chunk.\n\n\n\n\nBefore you begin your analysis, always set your Confidence Interval – 90%, 95%, 99%. This will help to determine the alpha value for your analysis, where alpha = 1 - (confidence interval/100)\n\nWhy? Note that not everything is certain in the real world, you have to know that there is a limit where we can no longer be confident in our result.\np-value by itself does not mean anything other than the probability of the occurrence of a given event. This probability has to be significantly higher (or lower) than a certain value (in here, the alpha value) to be determined significant.\n\n\n\n\n\n\nGrey Zone indicates the confidence envelop (95%), and anything that falls within we are not confident that point spatial distribution is not random.\nThe red diagonal line (break line) separates at 50% the upper confidence envelope and lower confidence envelope.\nOutside the Grey Zone, if – In above break line, then there are signs of clustering – In below break line, then there are signs of uniform distribution\nNote we also look out for from which distance to which distance where we are confident clustering occurs.\n\nGenerally, we use G and K estimation functions (similar to K is L function, which is a standardised function of K), do you know why? TO BE COMPLETED\n– Extra Notes: L function is a transformation of K function. We divide by pi (as each study area is a circle), we further subtract away the radius of each study area to straighten the break line into a straight line. – Extra Notes: Difference between G and K:\n\nBoth functions are zonal, they search a radius around each point.\nG function uses a fixed set of radius (fixed bandwidth)\nK function uses a search radius relative to surrounding points, e.g. the nearest point is 100m away, the first search is 100m around point, the next nearest point is 200m away, we do a search 200m around point.",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "03 Exercise Review 3"
    ]
  },
  {
    "objectID": "in-class-exercise/ex03/in-class-ex03.html#in-class-review-5-items",
    "href": "in-class-exercise/ex03/in-class-ex03.html#in-class-review-5-items",
    "title": "03 In-class Exercise 3 (Review)",
    "section": "",
    "text": "Review on Spatial Point Patterns in Real World\nDistribution of diseases such as dengue fever – note recorded based on homeaddress, not where you’ve bitten\nDistribution of car collisions (traffic accidents) – note recorded on-site\nDistribution of education institution\nDistribution of social media data such as tweets through web crawling, and signal is from the celltower, not very exact.\nAlways take note:\n\nAlways know where the source of your data is from.\nYou cannot take a sample from the population from spatial point events and analyse it.\nYou can get a complete subset like “kindergarten” from a subset of “childcare centre”, but you cannot do a random sampling out of “kindergarten” like when you do usual statistical analysis.\n\nWhat we want to know:\nIs the point even randomly distributed? Almost never will we get random distribution in the real world, unless you are doing a very micro-scale study.\n\n\n\nTypes of spatial points analysis:\n– 1st order: exploratory data analysis – visualise data points – 2nd order:\nWe are not able to objectively visualise the intensity on a map (interpretation can change depending on how big your points are). To do so objectively one method is to use Kernal Density Estimation(KDE).\nWhy analysis? Using KDE:\n– We are determining spatial distribution of poins are Either random or not random. – If not random, either clustered or uniform (dispersed)\n\n\n\nFixed Bandwidths – we can fixed search radius for our KDE, but this is not always realistic.\nAdaptive Bandwidths – More realistically, we know that not every place has the same geographical/physical size. Tampines is larger than Choa Chu Kang, do you use the same search distance for both area and determine the density, or use a larger bandwidth for Tampines and smaller for Choa Chu Kang?\n– Having different bandwidths, which one is accurate? They are all accurate! – Interpretation? Which one best shows you the most appropriate interpretation?\n\n\n\n\n\nKernal Interpolation.. Note the differences between Quartic and Gaussian - They are similar, but they give different results - Gaussian may result in negative results (e.g. -1.0 kernel density estimation for childcare -&gt; doesn’t make sense realistically) - If you want to avoid negative results, but want to use a normal-distributinon-like interpolation function, use Quartic instead.\n\n\n\nTo declare whether the points events are random, we say:\n\nNull Hypothesis, H0 = Observed point process (or point events) resemble complete Spatial Random Distribution.\nAlternative Hypothesis, H1 = Observed point process (or point events) do not resemble complete Spatial Random Distribution.\n\nHomogenous distribution–complete spatial randomness–is an ‘ideal’ situation. It should not happen. If we happen to not have enough evidence to reject the null hypothesis, it does not mean the null hypothesis is correct. It simply means our evidence is not enough – either error in sampling, or sampling size is too small such that it does not represent the population.\n\n\n\nWhat is this “population” that we are comparing with? We basically do a Monte Carlo simulation (multiple simulations) to come up with a spatially randomly distributed point events within our study region.\n– For each simulated point pattern, estimate G(r) and use the maximum (95th) and minimum (5th) of these functions for the simulated patterns to define an upper and lower simulation envelope. (the max and min can be varied, need not be 95%)\n– If the estimated G(r) lies above the upper envelope or below the lower envelope, the estimated G(r) is statistically significant.\n\n\n\nIn spatstat’s simulations, minimum number of simulation for 95% confidence is 40 (R counts from 0, use 39 in code) simulations. (according to spatstat’s documentation):\n\nHaving larger number of sampling increases our confidence in the final result.\nUsually it’s easier to remember and cleaner to use a easier to read number like 99 (which produces 100 simulations)\nRemember to always use a seed value for the random generation, so that you get a reproduceable result.\nYou just need to set a seed for all algorithms that deals with randomness at the top of the document, no need to repeat it for every code chunk.\n\n\n\n\nBefore you begin your analysis, always set your Confidence Interval – 90%, 95%, 99%. This will help to determine the alpha value for your analysis, where alpha = 1 - (confidence interval/100)\n\nWhy? Note that not everything is certain in the real world, you have to know that there is a limit where we can no longer be confident in our result.\np-value by itself does not mean anything other than the probability of the occurrence of a given event. This probability has to be significantly higher (or lower) than a certain value (in here, the alpha value) to be determined significant.\n\n\n\n\n\n\nGrey Zone indicates the confidence envelop (95%), and anything that falls within we are not confident that point spatial distribution is not random.\nThe red diagonal line (break line) separates at 50% the upper confidence envelope and lower confidence envelope.\nOutside the Grey Zone, if – In above break line, then there are signs of clustering – In below break line, then there are signs of uniform distribution\nNote we also look out for from which distance to which distance where we are confident clustering occurs.\n\nGenerally, we use G and K estimation functions (similar to K is L function, which is a standardised function of K), do you know why? TO BE COMPLETED\n– Extra Notes: L function is a transformation of K function. We divide by pi (as each study area is a circle), we further subtract away the radius of each study area to straighten the break line into a straight line. – Extra Notes: Difference between G and K:\n\nBoth functions are zonal, they search a radius around each point.\nG function uses a fixed set of radius (fixed bandwidth)\nK function uses a search radius relative to surrounding points, e.g. the nearest point is 100m away, the first search is 100m around point, the next nearest point is 200m away, we do a search 200m around point.",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "03 In-class Exercise 3 (Review)"
    ]
  },
  {
    "objectID": "in-class-exercise/ex03/in-class-ex03.html#in-class-exercise",
    "href": "in-class-exercise/ex03/in-class-ex03.html#in-class-exercise",
    "title": "03 In-class Exercise 3 (Review)",
    "section": "2 In Class Exercise",
    "text": "2 In Class Exercise\n\nImporting Packages\n\npacman::p_load(tidyverse, sf, spatstat, tmap, rvest, maptools)\n\n\nIssue 1: Installing maptools\nNote that maptools is retired, so we have issues converting from simplefeatures object to spatstat’s ppp object. For subsequent use, don’t re-install everytime, include ‘eval: false’ once installed.\n\ninstall.packages(\"maptools\",\n                 repos = \"https://packagemanager.posit.co/cran/2023-10-13\")\n\n\n\n\nImport Data set\n\n# childcare services data\nchildcare_sf &lt;- st_read(dsn=\"data/geospatial/child-care-services-geojson.geojson\")\n\n# masterplan subzone data\nmpsz_sf &lt;- st_read(dsn=\"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nIssue 2: Getting Coastal Outline using Subzone Layer\nIn sf package, there are two methods to combine multiple sf features into one simple feature object.\n\nst_combine() – combine without unioning or resolving internal boundaries\nst_union() – combine and remove all itnernal boundaries\n\nAlways note that the data we want is not always provided in the most suitable form or format. Therefore, we should always look into manipulating and ‘massaging’ our data to extract whatever useful information we need.\n\nsg_sf &lt;- mpsz_sf %&gt;% st_union()\nsg_sf\n\nGeometry set for 1 feature \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n\nData Wrangling\n\n# Ensure all data sets are in the same CRS throughout\n\nst_transform(childcare_sf, crs = 3414)\n\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 11203.01 ymin: 25667.6 xmax: 45404.24 ymax: 49300.88\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n     Name\n1   kml_1\n2   kml_2\n3   kml_3\n4   kml_4\n5   kml_5\n6   kml_6\n7   kml_7\n8   kml_8\n9   kml_9\n10 kml_10\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Description\n1                     &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;760742&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;742, YISHUN AVENUE 5, #01 - 470, SINGAPORE 760742&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;AVERBEL CHILD DEVELOPMENT CENTRE PTE LTD&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;AEA27114446235CE&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n2                                                        &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;159053&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;20, LENGKOK BAHRU, #02 - 05, SINGAPORE 159053&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;AWWA LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;86B24416FB1663C6&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n3                            &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;556912&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;22, LI HWAN VIEW, GOLDEN HILL ESTATE, SINGAPORE 556912&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;BABIES BY-THE-PARK PTE. LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;F971CBBA973E1AE5&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n4                     &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;569139&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;3, ANG MO KIO STREET 62, #01 - 36, LINK@AMK, SINGAPORE 569139&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;Baby Elk Infant Care Pte Ltd&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;86A4F25D1C7C9D85&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n5                                               &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;467961&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;22A, KEW DRIVE, SINGAPORE 467961&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;BABYPLANET MONTESSORI PTE. LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;CFE3F056F8171C7B&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n6                                           &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;598523&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;3 Jalan Kakatua, JURONG PARK, SINGAPORE 598523&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;BAMBINI CHILDCARE LLP&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;2B4F0B285ED28C4A&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n7                              &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;160131&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;131, JALAN BUKIT MERAH, #01 - 1591, SINGAPORE 160131&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;BAMBINI MONTESSORI PTE. LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;F62A225197813BBD&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n8                        &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;543319&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;319C, ANCHORVALE DRIVE, #01 - 66, SINGAPORE 543319&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;BERRY TREE PRESCHOOL PRIVATE LIMITED&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;AE242159867D5EB2&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n9  &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;750511&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;511, CANBERRA ROAD, #03 - 02, SEMBAWANG MART, SINGAPORE 750511&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;BERRY TREE PRESCHOOL@SEMBAWANG PRIVATE LIMITED&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;C1456F97A17ED64A&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n10                    &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;823195&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;195C, PUNGGOL ROAD, #01 - 532, THE PERIWINKLE, SINGAPORE 823195&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;BERRY TREE@PUNGGOL PTE LTD&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;4F6A8FCA467C3437&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n                        geometry\n1   POINT Z (27976.73 45716.7 0)\n2     POINT Z (25824 29900.09 0)\n3  POINT Z (31399.04 37416.36 0)\n4   POINT Z (29268.43 40942.1 0)\n5  POINT Z (41217.74 33554.94 0)\n6  POINT Z (20644.07 36118.78 0)\n7  POINT Z (27427.95 29182.36 0)\n8  POINT Z (34378.47 41423.03 0)\n9  POINT Z (26467.04 48384.34 0)\n10 POINT Z (36173.81 42550.33 0)\n\nst_transform(mpsz_sf, crs = 3414)\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\nst_transform(sg_sf, crs = 3414)\n\nGeometry set for 1 feature \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\n\n\n\nst_crs(mpsz_sf) &lt;- 3414\nst_crs(childcare_sf) &lt;- 3414\nst_crs(sg_sf) &lt;- 3414\n\n\n\nIssue 3: Conflicts with Maptools\n\ngridded_kde_childcareSG_bw &lt;- maptools::as.SpatialGridDataFrame.im(kde_childcareSG.bw) \n# this uses maptools, use maptools::as.SpatialGridDataFrame.im(kde_childcareSG.bw)\nspplot(gridded_kde_childcareSG_bw)",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "03 In-class Exercise 3 (Review)"
    ]
  },
  {
    "objectID": "in-class-exercise/ex03/in-class-ex03.html#briefing-on-take-home-exercise-01",
    "href": "in-class-exercise/ex03/in-class-ex03.html#briefing-on-take-home-exercise-01",
    "title": "03 In-class Exercise 3 (Review)",
    "section": "Briefing on Take-Home Exercise 01",
    "text": "Briefing on Take-Home Exercise 01\nGeospatial Analytics for Social Goods.\nFocus: Myanmar Armed Conflict\n\nacled_sf &lt;- read_csv(\"data/ACLED_Myanmar.csv\")\n\nNote that event_date is recorded as a character filed\nuse this instead:\n\n# Convert into simplefeature, using long,lat as coordinates, source crs=4326, use projected crs = 32647, which is WGS 84 / UTM zone 47N. \n\nacled_sf &lt;- read_csv(\"data/ACLED_Myanmar.csv\") %&gt;%\n  st_as_sf(coords =c(\n    \"longitude\", \"latitude\"),\n    crs=4326) %&gt;%\n    st_transform(crs = 32647) %&gt;%\n    mutate(event_date = dmy(event_date))\n\n\ntmap_mode('plot')\nacled_sf %&gt;%\n  filter(year == 2023 |\n           event_type == \"Political violence\") %&gt;%\n  tm_shape() +\n  tm_dots()\n\n\n\n\n\n\n\ntmap_mode(\"plot\")",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "03 In-class Exercise 3 (Review)"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html",
    "href": "take-home-exercise/ex01/take-home-ex01.html",
    "title": "01 Take Home Exercise 1",
    "section": "",
    "text": "Geospatial analytics hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply spatial point patterns analysis methods to discover the spatial and spatio-temporal distribution of armed conflict in Myanmar.",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/take-home-ex-intro.html",
    "href": "take-home-exercise/take-home-ex-intro.html",
    "title": "00 Take Home Exercise Introduction",
    "section": "",
    "text": "There are three take-home exercises that are due throughout the term. They aim to provide students the opportunities to apply the methods learned in class by working through mini real-world cases. Each take-home exercise is an extension of the hands-on and in-class exercises.\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "00 Take Home Exercise Introduction"
    ]
  },
  {
    "objectID": "in-class-exercise/ex04/in-class-ex04.html",
    "href": "in-class-exercise/ex04/in-class-ex04.html",
    "title": "04 In-class Exercise 4 (Review)",
    "section": "",
    "text": "Eval set to False",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "04 In-class Exercise 4 (Review)"
    ]
  },
  {
    "objectID": "in-class-exercise/ex04/in-class-ex04.html#issue-1-rendering-ice3-map",
    "href": "in-class-exercise/ex04/in-class-ex04.html#issue-1-rendering-ice3-map",
    "title": "04 In-class Exercise 4 (Review)",
    "section": "",
    "text": "# rendering tmap view for ICE3\n\n# problem is in the leaflet library packages\n\n# try development version 3.99.9 -&gt; rather than version 4\n# but it is likely to contain unexpected bugs, feel free to try it out\n\nExpose you to different data portals, open sources, open data access Get used to different data types, knowing what to use, when to use.",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "04 In-class Exercise 4 (Review)"
    ]
  },
  {
    "objectID": "in-class-exercise/ex04/in-class-ex04.html#starting-you-with-takehome-exercise-1",
    "href": "in-class-exercise/ex04/in-class-ex04.html#starting-you-with-takehome-exercise-1",
    "title": "04 In-class Exercise 4 (Review)",
    "section": "Starting you with Takehome Exercise 1",
    "text": "Starting you with Takehome Exercise 1\nGeneral knowledge on Myanmar-Singapore economy:\n\nSand import from Myanmar\nJade import from Myanmar\n\n–&gt; refer to Take Home Exercise 01 for further notes.",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "04 In-class Exercise 4 (Review)"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex04/hands-on-ex04.html",
    "href": "hands-on-exercise/ex04/hands-on-ex04.html",
    "title": "04 ‘Network Constrained Spatial Point Patterns Analysis’",
    "section": "",
    "text": "None.\nRefer to In-class Exercise 04.\nTopic: Spatio-Temporal Point Patterns Analysis\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "04 ‘Network Constrained Spatial Point Patterns Analysis’"
    ]
  },
  {
    "objectID": "in-class-exercise/ex04/data/rawdata/Kepulauan_Bangka_Belitung.html",
    "href": "in-class-exercise/ex04/data/rawdata/Kepulauan_Bangka_Belitung.html",
    "title": "R for Geospatial Analytics",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     \n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "in-class-exercise/ex04/in-class-ex04.html#install-and-loading-r-packages",
    "href": "in-class-exercise/ex04/in-class-ex04.html#install-and-loading-r-packages",
    "title": "04 In-class Exercise 4 (Review)",
    "section": "0.0 Install and Loading R Packages",
    "text": "0.0 Install and Loading R Packages\nThe main package we want to use for spatial-temporal point pattern analysis is sparr:\nsparr provides functions to estimate fixed and adaptive kernel-smoothed spatial relative risk surfaces via the density-ratio method and perform subsequent inference. Fixed-bandwidth spatiotemporal density and relative risk estimation is also supported.\n\n##| eval: false\n\npacman::p_load(sf, tidyverse, tmap, sparr, spatstat, raster)\n\nSpatio-Temporal Patterns\nNote that when we are studying patterns, we are not necessarily studying causation, we want to identify whether there are any underlying patterns of the spread of spatial-temporal points, and whether there are correlation between the factors; such factors can be socio-economic levels, terrain, culture, etc.",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "04 In-class Exercise 4 (Review)"
    ]
  },
  {
    "objectID": "in-class-exercise/ex04/in-class-ex04.html#importing-data",
    "href": "in-class-exercise/ex04/in-class-ex04.html#importing-data",
    "title": "04 In-class Exercise 4 (Review)",
    "section": "1.0 Importing Data",
    "text": "1.0 Importing Data\n(Copied from Exercise) The data:\nFor the purpose of this exercise, two data sets are used, they are:\n\nforestfires, a csv file provides locations of forest fire detected from the Moderate Resolution Imaging Spectroradiometer (MODIS) sensor data. The data are downloaded from Fire Information for Resource Management System. For the purpose of this exercise, only forest fires within Kepulauan Bangka Belitung will be used.\nKepulauan_Bangka_Belitung, an ESRI shapefile showing the sub-district (i.e. kelurahan) boundary of Kepulauan Bangka Belitung. The data set was downloaded from Indonesia Geospatial portal. The original data covers the whole Indonesia. For the purpose of this exercise, only sub-districts within Kepulauan Bangka Belitung are extracted.\n\n\n1.1 Load in the Study Area Shapefile\n\n##| eval: false\n# load in the study area. Note Kota refer to urban village/city. Desa refer to rural village.\n# note the Polygon contains a Z dimension, which will interfere with analysis if you try to convert to PPP.\n# you can drop away the Z if not needed.\n# kbb_raw &lt;- st_read(dsn = \"data/rawdata/\", layer = \"Kepulauan_Bangka_Belitung\")\n\nstudyArea_sf &lt;- st_read(dsn = \"data/rawdata/\", layer = \"Kepulauan_Bangka_Belitung\")  %&gt;%\n  st_union() %&gt;% # union all the boundaries so you only have an outline\n  st_zm(drop = TRUE, what = \"ZM\") %&gt;% # remove Z dimension\n  st_transform(crs= 32748) # transform to the appropriate projected CRS\n\n\n\n1.2 Load in the Forest Fire Data\n\n##| eval: false\n# load in the the forest fire points.\n# forestFire_raw &lt;- read_csv(file=\"data/rawdata/forestfires.csv\")\n\nforestFire_sf &lt;- read_csv(file=\"data/rawdata/forestfires.csv\") %&gt;%\n  st_as_sf(coords = c(\"longitude\",\"latitude\"),\n           crs = 4326) %&gt;%\n  st_transform(crs=32748)\n\nCaution: when you use read.csv, your fieldnames cannot contain whitespaces, any whitespaces, it will insert a dot; your fieldnames will change. Instead, when you use read_csv from the readr package (part of tidyverse), the output is a tibble dataframe, though without geometry (which we will assign when we transform it into sf). In this case, since all the fieldnames contain no whitespaces, there are no issues with using read.csv()\n\n# forestfire_test &lt;- read.csv(file=\"data/rawdata/forestfires.csv\")\n# class(forestfire_test)",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "04 In-class Exercise 4 (Review)"
    ]
  },
  {
    "objectID": "in-class-exercise/ex04/in-class-ex04.html#data-wrangling",
    "href": "in-class-exercise/ex04/in-class-ex04.html#data-wrangling",
    "title": "04 In-class Exercise 4 (Review)",
    "section": "2.0 Data Wrangling",
    "text": "2.0 Data Wrangling\n\n2.1 Generate the window Owin Class of Study Area.\n\nstudyArea_owin &lt;- as.owin(studyArea_sf)\nstudyArea_owin\nclass(studyArea_owin)\n\n\nforestFire_sf &lt;- forestFire_sf %&gt;%\n  mutate(DayofYear = yday(acq_date)) %&gt;% # day of the year from 1 to 365\n  mutate(Month_num = month(acq_date)) %&gt;% # month of the year returned as numbers from 1 to 12\n  mutate(Month_fac = month(acq_date, # month of the year returned as factors and not abbreviated -&gt; January, February\n                           label = TRUE,\n                           abbr = FALSE))\n\nThe tool we are using to handle the dates is called lubridate(part of tidyverse) -&gt; useful for all sorts of manipulations on dates and time, including quarter years, ewidth, and so on.\nThe tool for data manipulation (e.g. mutate) is called dyplr-&gt; useful for data manipulations\n\n\n2.2 Quick Glance at the Data\ntmap allows us to make cartographic quality map, including interactive maps (not always loadable)\n\ntmap_mode(\"plot\")\n\ntm_shape(studyArea_sf) +\n    tm_polygons() +\n  tm_shape(forestFire_sf) +\n    tm_dots(size= 0.1) + # make sizes a bit bigger so that it can be seen\n  tm_facets(by=\"Month_fac\",\n            free.coords = TRUE,# if TRUE, all maps will be zoomed in on data points extent\n            drop.units = TRUE)\n  tmap_mode(\"plot\")\n\n\ntmap_mode(\"plot\")\n\ntm_shape(studyArea_sf) +\n    tm_polygons() +\n  tm_shape(forestFire_sf) +\n    tm_dots(size= 0.1) + # make sizes a bit bigger so that it can be seen\n  tm_facets(by=\"Month_fac\",\n            free.coords = FALSE,# if TRUE, all maps will be zoomed in on data points extent\n            drop.units = TRUE)\n  tmap_mode(\"plot\")",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "04 In-class Exercise 4 (Review)"
    ]
  },
  {
    "objectID": "in-class-exercise/ex04/in-class-ex04.html#computing-stkde-by-month",
    "href": "in-class-exercise/ex04/in-class-ex04.html#computing-stkde-by-month",
    "title": "04 In-class Exercise 4 (Review)",
    "section": "3.0 Computing STKDE by Month",
    "text": "3.0 Computing STKDE by Month\nsparr is designed for events vs control comparison (used in medical science)\n\nfire_month &lt;- forestFire_sf %&gt;%\n  select(`Month_num`)\n\n\nfire_month_ppp &lt;- as.ppp(fire_month)\nfire_month_ppp\n\n\nsummary(fire_month_ppp)\n\n\nfire_month_owin &lt;- fire_month_ppp[studyArea_owin]\nsummary(fire_month_owin)\n\n\n3.1 Computing Spatio-Temporal KDE\nReading the results:\nfunction gives you bandwidths:\nh (spatial) -&gt; 15102.47 metres\nlambda (temporal) -&gt; 0.03months\n\nst_kde &lt;- spattemp.density(fire_month_owin)\nsummary(st_kde)\n\n\n#| fig-width: 12\n#| fig-height: 10\n\n# you can specify your plot area so that the graph features are not crammed together\n\ntrims &lt;- c(7,8,9,10,11,12) # trim, since we saw that most fires only started July onwards\n\npar(mfcol=c(2,3)) # result will have 2 rows, 3 columns\n\nfor(i in trims){\n  plot(st_kde, i,\n       override.par=FALSE,\n       fix.range=TRUE,\n       main=paste(\"KDE at month\", i))\n}\n\nReading the results:\n\nkde_yday &lt;- spattemp.density(\n  fire_yday_owin)\n\nsummary(kde_yday)\n\nRather than visualising a static image, try to plot it on an animated image:\nrefer to https://tilmandavies.github.io/sparr/articles/fmd_animation/fmd_animation.html\nrefer to https://github.com/tilmandavies/sparr/blob/HEAD/vignettes/fmd_animation/fmd_animation.Rmd",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "04 In-class Exercise 4 (Review)"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#importing-libraries-packages",
    "href": "take-home-exercise/ex01/take-home-ex01.html#importing-libraries-packages",
    "title": "01 Take Home Exercise 1",
    "section": "1.1 Importing Libraries / Packages",
    "text": "1.1 Importing Libraries / Packages\n\npacman::p_load(tidyverse, sf, spatstat, ggplot2, tmap, gifski)",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#importing-raw-data",
    "href": "take-home-exercise/ex01/take-home-ex01.html#importing-raw-data",
    "title": "01 Take Home Exercise 1",
    "section": "1.2 Importing Raw Data",
    "text": "1.2 Importing Raw Data\n\n1.2.1 ACLED Data Set\nBefore we start manipulating the data, it is important that we understand what each data column and data type mean and how valuable it is for our analysis.\nTo begin, let us refer to the ACLED data set codebook\nThe data was retrieved from the ACLED data portal. Specifically, the following types of data was retrieved:\n\nviolence-against-civilians.csv\nstrategic-developments.csv\nbattles.csv\nexplosion-or-remoteviolence.csv\n\nIn terms of event types, four main event types were retrieved: Battles, Explosion/Remote violence, Strategic developments, and Violence against civilians.\nIn terms of study period, we are focused on quarterly armed conflict events from January 2021 until June 2024.\nThe code chunk below imports the data for our analysis.\n\nviolence_against_civilians &lt;- read_csv(\"data/raw/aspatial/ACLED/violence-against-civilians.csv\") %&gt;%\n  st_as_sf(coords =c(\n    \"longitude\", \"latitude\"),\n    crs=4326) %&gt;%\n    st_transform(crs = 32647) %&gt;%\n    mutate(event_date = dmy(event_date))\n\nstrategic_developments &lt;- read_csv(\"data/raw/aspatial/ACLED/strategic-developments.csv\") %&gt;%\n  st_as_sf(coords =c(\n    \"longitude\", \"latitude\"),\n    crs=4326) %&gt;%\n    st_transform(crs = 32647) %&gt;%\n    mutate(event_date = dmy(event_date))\n\nbattles &lt;- read_csv(\"data/raw/aspatial/ACLED/battles.csv\") %&gt;%\n  st_as_sf(coords =c(\n    \"longitude\", \"latitude\"),\n    crs=4326) %&gt;%\n    st_transform(crs = 32647) %&gt;%\n    mutate(event_date = dmy(event_date))\n\nexplosion_or_remoteviolence &lt;- read_csv(\"data/raw/aspatial/ACLED/explosion-or-remoteviolence.csv\") %&gt;%\n  st_as_sf(coords =c(\n    \"longitude\", \"latitude\"),\n    crs=4326) %&gt;%\n    st_transform(crs = 32647) %&gt;%\n    mutate(event_date = dmy(event_date))\n\n\n\n1.2.2 Administrative Boundary Dataset\n\n\n\n\n\n\n\nMyamar’s Administrative Division Hierachy (source: Wikipedia)\n\n\nIn this exercise, we will only analyse geographical distribution of point data up to the 3rd degree administrative division, i.e. up to Township level.\nThe following data are the administrative boundaries of Myanmar, obtained from MIMU Vector Boundaries, a common data and information repository by a NGO (related to UN).\nWhile many data formats exist, we will choose .csv files when downloading as they are easier to read and work with. A suitable alternative is the .shp file format available on the website.\nDatasets:\n\nmmr_polbnda_adm0_250k_mimu_1.csv National boundary of Myanmar\nmmr_polbnda_adm1_250k_mimu_1.csv Region/State/Union Territory level boundary of Myanmar\nmmr_polbnda2_adm1_250k_mimu_1.csv Sub-Region/State/Union Territory level boundary of Myanmar; Sub-region divides a region into smaller divisions, such as “Bago (East)” and “Bago (West)” instead of “Bago”\nmmr_polbnda_adm2_250k_mimu.csv District/Self-Administered Zone level boundary of Myanmar\nmmr_polbnda_adm3_250k_mimu_1.csv Township level boundary of Myanmar\n\n\n# kml file not easy to read due to nested columns for attributes data\nadmin_0 &lt;- read_csv(\"data/raw/geospatial/mimu_admin_boundary/mmr_polbnda_adm0_250k_mimu_1.csv\") %&gt;%\n  st_as_sf(wkt = \"the_geom\",\n    crs=4326) %&gt;%\n    st_transform(crs = 32647)\n    \nadmin_1 &lt;- read_csv(\"data/raw/geospatial/mimu_admin_boundary/mmr_polbnda_adm1_250k_mimu_1.csv\") %&gt;%\n  st_as_sf(wkt = \"the_geom\",\n    crs=4326) %&gt;%\n    st_transform(crs = 32647)\n\nadmin_1_sub &lt;- read_csv(\"data/raw/geospatial/mimu_admin_boundary/mmr_polbnda2_adm1_250k_mimu_1.csv\") %&gt;%\n  st_as_sf(wkt = \"the_geom\",\n    crs=4326) %&gt;%\n    st_transform(crs = 32647)\n\nadmin_2 &lt;- read_csv(\"data/raw/geospatial/mimu_admin_boundary/mmr_polbnda_adm2_250k_mimu.csv\") %&gt;%\n  st_as_sf(wkt = \"the_geom\",\n    crs=4326) %&gt;%\n    st_transform(crs = 32647)\n\nadmin_3 &lt;- read_csv(\"data/raw/geospatial/mimu_admin_boundary/mmr_polbnda_adm3_250k_mimu_1.csv\") %&gt;%\n  st_as_sf(wkt = \"the_geom\",\n    crs=4326) %&gt;%\n    st_transform(crs = 32647)\n\n\n\n1.2.3 Quick Plot To Visualise Data Sets\nBefore we continue, let’s get a visual sense of the data by plotting it on a map to ensure we are working with the right dataset. In this step, we will also try to see if there are any erranous data that we have to clean later.\nCheck the Administrative Boundaries Data Set\nFrom the left to right, we are able to see the increase in degree of administrative boundary division; respectively, they represent\n\n“National”,\n“Region/State/Union Territory”,\n“Sub-Region/State/Union Territory”,\n“District/Self-Administered Zone”, and\n“Township”\n\n\ntmap_mode(\"plot\")\ntmap_style(\"classic\")\n\ntmap_arrange(\n  qtm(admin_0, title=\"National\"), \n  qtm(admin_1, title=\"Region/State/Union\\nTerritory\"), \n  qtm(admin_1_sub, title=\"Sub-Region/State/Union\\nTerritory\"), \n  qtm(admin_2, title=\"District/Self-\\nAdministered Zone\"), \n  qtm(admin_3, title=\"Township\"), \n  ncol = 5\n)\n\n\n\n\nPlot of Admin Boundaries of Myanmar Raw Data\n\n\nConclusion: Administrative Boundaries Looks Okay\nLooking at both the attribute table and the plot, the administrative boundaries data seem quite alright, so we might not need to do any cleaning. Anyway, the precision of the polygons it not as critical as the precision of the ACLED data (which will be used for our spatial point patterns analysis).\nCheck ACLED Data Set\nSince the ACLED data is our main focus, let us now plot the data points onto the map of Myanmar.\nIn the code chunk below, we plot out the various types of data points from ACLED, namely (from left to right):\n\n“Violence against Civilians”,\n“Strategic Developments”,\n“Explosion/Remote Violence”, and\n“Battles”\n\n\ntmap_mode(\"plot\")\ntmap_style(\"cobalt\")\n\ntmap_arrange(\n  qtm(admin_0) + qtm(violence_against_civilians, title=\"Violence against\\nCivilians\"), \n  qtm(admin_0) + qtm(strategic_developments, title=\"Strategic\\nDevelopments\"), \n  qtm(admin_0) + qtm(explosion_or_remoteviolence, title=\"Explosion/\\nRemote Violence\"), \n  qtm(admin_0) + qtm(battles, title=\"Battles\"), \n  ncol = 4\n)\n\n\n\n\nPlot of ACLED Raw Points Data\n\n\nObservation: ACLED Data Set needs further study, may need to clean\nNothing looks very off at a glance, but we should look into the attribute columns to see if the data set is truly clean. Let us refer to the codebook again, and see if we can spot any potentially critical problems.\nReferring to the attribute columns and the codebook, we can see potentially critical concerns:\n\ngeo_precision – The precision of the geocoded coordinates ranges from code 1 to code 3; where lower level implies higher precision.\n\nIn particular, “[if] a larger region is mentioned, the closest natural location noted in reporting (like “border area,” “forest,” or “sea,” among others) – or a provincial capital is used if no other information at all is available – is chosen to represent the region, and ‘Geo-precision’ code 3 is recorded.” (page 36)\n\ntime_precision – The precision of the recorded datetime ranges from code 1 to code 3; where lower level implies higher precision.\n\nIn particular, “if the source material only indicates that an event took place sometime during a month (i.e. in the past two or three weeks, or in January), without reference to the particular date, the month mid-point is chosen. If the beginning or end of the month is noted, the first and last date is used, respectively. In both of these cases, a ‘Time precision’ code of 3 is recorded.” (page 36-37)\n\n\nIn both cases, ACLED do not include events with less spatial or temporal precision.\nIdentifying code 3 precisions\nGiven that precision of our point data is crucial to our spatial-temporal point patterns analysis, we should see how much of our data is imprecise, and whether we should keep the imprecise data points.\n\n# Violence against Civilians\nviolence_against_civilians %&gt;% count(geo_precision)\nviolence_against_civilians %&gt;% count(time_precision)\n\n# Strategic Developments\nstrategic_developments %&gt;% count(geo_precision)\nstrategic_developments %&gt;% count(time_precision)\n\n# Explosion or Remote Violence\nexplosion_or_remoteviolence %&gt;% count(geo_precision)\nexplosion_or_remoteviolence %&gt;% count(time_precision)\n\n# Battles\nbattles %&gt;% count(geo_precision)\nbattles %&gt;% count(time_precision)\n\nConclusion: Drop time and spatial precision code 3 data values from ACLED Data set\nFrom this analysis, we can see that the count of precision code 3 in both time and spatial precision is actually very low, it might be worth dropping the values with low time and spatial precision.\nLet us start cleaning the data",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#extract-quarterly-data-from-acled-dataset",
    "href": "take-home-exercise/ex01/take-home-ex01.html#extract-quarterly-data-from-acled-dataset",
    "title": "01 Take Home Exercise 1",
    "section": "2.1 Extract Quarterly Data from ACLED Dataset",
    "text": "2.1 Extract Quarterly Data from ACLED Dataset\nNow, since we want to analyse the quarterly events, let us add a new column within the tibble DataFrame called quarter to represent the quarter of each date in numerical format, e.g. (1, 2, 3, or 4). After that, let us create another column called year_quarter to represent the quarter of every year in string format, e.g. (“2021-Q1”, “2023-Q4”).\nThe package we will be using here is called lubridate, a package within the tidyverse library.\n\nviolence_against_civ_filtered &lt;- violence_against_civ_filtered %&gt;% \n  mutate(quarter = quarter(event_date)) %&gt;%\n  mutate(year_quarter = paste0(year, \"-Q\", quarter))\n\nstrategic_dev_filtered &lt;- strategic_dev_filtered %&gt;% \n  mutate(quarter = quarter(event_date)) %&gt;%\n  mutate(year_quarter = paste0(year, \"-Q\", quarter))\n\nexplosion_or_remote_vio_filtereed &lt;- explosion_or_remote_vio_filtereed %&gt;% \n  mutate(quarter = quarter(event_date)) %&gt;%\n  mutate(year_quarter = paste0(year, \"-Q\", quarter))\n\nbattles_filtered &lt;- battles_filtered %&gt;% \n  mutate(quarter = quarter(event_date)) %&gt;%\n  mutate(year_quarter = paste0(year, \"-Q\", quarter))",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#export-data-sets-rds-file-format",
    "href": "take-home-exercise/ex01/take-home-ex01.html#export-data-sets-rds-file-format",
    "title": "01 Take Home Exercise 1",
    "section": "3.5 Export Data Sets (RDS file format)",
    "text": "3.5 Export Data Sets (RDS file format)\nBefore we continue, let’s export our cleaned data sets so that our changes are saved. We will export the data sets in the RDS format.\nRDS stands for R Data Serialization. It’s a binary serialization format in R used to save R objects to a file. This format preserves the class, attributes, and structure of the R object, making it useful for saving and loading data while maintaining its integrity.\n\n\n\n\n\n\nTip\n\n\n\n\n\nBy exporting the data, and importing it again, it also serves as a checkpoint for our analysis. We will be able to stop loading old variables in our environment, and only load in the new variables. It also allows readers who are trying to reproduce the analysis verify their own results with our analysis results.\n\n\n\n\n# Save the sf object to an RDS file\nsaveRDS(violence_against_civilians_filtered, \"data/rds/violence_against_civilians.rds\")\nsaveRDS(strategic_developments_filtered, \"data/rds/strategic_developments.rds\")\nsaveRDS(explosion_or_remoteviolence_filtered, \"data/rds/explosion_or_remoteviolence.rds\")\nsaveRDS(battles_filtered, \"data/rds/battles.rds\")\n\n\n# Save the sf object to an RDS file\nsaveRDS(admin_0, \"data/rds/admin_boundary_national.rds\")\nsaveRDS(admin_1, \"data/rds/admin_boundary_region-state-unionTerritory.rds\")\nsaveRDS(admin_1_sub, \"data/rds/admin_boundary_subRegion-state-unionTerritory.rds\")\nsaveRDS(admin_2, \"data/rds/admin_boundary_district-selfAdministeredZone.rds\")\nsaveRDS(admin_3, \"data/rds/admin_boundary_township.rds\")\n\nNow, let’s continue to the next section.",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#import-data-sets-rds-file-format",
    "href": "take-home-exercise/ex01/take-home-ex01.html#import-data-sets-rds-file-format",
    "title": "01 Take Home Exercise 1",
    "section": "2.1 Import Data Sets (RDS file format)",
    "text": "2.1 Import Data Sets (RDS file format)\nLet us import the data sets back.\nThe code chunk below imports the cleaned and filtered ACLED data that was previously exported into RDS file format.\n\nviolence_against_civilians &lt;- readRDS(\"data/rds/violence_against_civilians.rds\")\nstrategic_developments &lt;- readRDS(\"data/rds/strategic_developments.rds\")\nexplosion_or_remoteviolence &lt;- readRDS(\"data/rds/explosion_or_remoteviolence.rds\")\nbattles &lt;- readRDS(\"data/rds/battles.rds\")\n\nThe code chunk below imports the transformed admin boundaries data that was previously exported into RDS file format. Note that we have yet to decide on a study area, so we will continue to import all the data for our analysis.\n\nadmin_0 &lt;- readRDS(\"data/rds/admin_boundary_national.rds\")\nadmin_1 &lt;- readRDS(\"data/rds/admin_boundary_region-state-unionTerritory.rds\")\nadmin_1_sub &lt;- readRDS(\"data/rds/admin_boundary_subRegion-state-unionTerritory.rds\")\nadmin_2 &lt;- readRDS(\"data/rds/admin_boundary_district-selfAdministeredZone.rds\")\nadmin_3 &lt;- readRDS(\"data/rds/admin_boundary_township.rds\")",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#data-cleaning",
    "href": "take-home-exercise/ex01/take-home-ex01.html#data-cleaning",
    "title": "01 Take Home Exercise 1",
    "section": "3.3 Data Cleaning",
    "text": "3.3 Data Cleaning\nLet us filter out the data we want (time_precision and geo_precision codes &lt; 3), and see how many rows we have removed.\n\n# Extract all rows where either geo_precision or time_precision is not = 3 \nviolence_against_civilians_filtered &lt;- violence_against_civilians %&gt;%\n  filter(!(geo_precision == 3 | time_precision == 3))\n\nstrategic_developments_filtered &lt;- strategic_developments %&gt;%\n  filter(!(geo_precision == 3 | time_precision == 3))\n\nexplosion_or_remoteviolence_filtered &lt;- explosion_or_remoteviolence %&gt;%\n  filter(!(geo_precision == 3 | time_precision == 3))\n\nbattles_filtered &lt;- battles %&gt;%\n  filter(!(geo_precision == 3 | time_precision == 3))\n\ncat(\"Number of rows dropped:\", nrow(violence_against_civilians) - nrow(violence_against_civilians_filtered) , \"\\n\")\ncat(\"Number of rows dropped:\", nrow(strategic_developments) - nrow(strategic_developments_filtered) , \"\\n\")\ncat(\"Number of rows dropped:\", nrow(explosion_or_remoteviolence) - nrow(explosion_or_remoteviolence_filtereed) , \"\\n\")\ncat(\"Number of rows dropped:\", nrow(battles) - nrow(battles_filtered) , \"\\n\")\n\nOverall, the result seems satisfactory. Let us continue with our data cleaning. by keeping only columns that are important for our analysis.\nWe will only keep the following columns as other columns are not relevant to our study:\n\nevent_date,\nyear,\ndisorder_type,\nsub_event_type,\nadmin1,\nadmin2,\nadmin3\n(also including the geometry data)\n\n\nfilter_columns &lt;- c(\"event_date\",\"year\",\"disorder_type\",\"sub_event_type\",\"admin1\",\"admin2\",\"admin3\")\n\nviolence_against_civilians_filtered &lt;- violence_against_civilians_filtered %&gt;%\n  select(all_of(filter_columns))\n\nstrategic_developments_filtered &lt;- strategic_developments_filtered %&gt;%\n  select(all_of(filter_columns))\n\nexplosion_or_remoteviolence_filtered &lt;- explosion_or_remoteviolence_filtered %&gt;%\n  select(all_of(filter_columns))\n\nbattles_filtered &lt;- battles_filtered %&gt;%\n  select(all_of(filter_columns))",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#data-extraction",
    "href": "take-home-exercise/ex01/take-home-ex01.html#data-extraction",
    "title": "01 Take Home Exercise 1",
    "section": "3.4 Data Extraction",
    "text": "3.4 Data Extraction\n\n3.4.1 Extract Quarterly Data from ACLED Dataset\nNow, since we want to analyse the quarterly events, let us add a new column within the tibble DataFrame called quarter to represent the quarter of each date in numerical format, e.g. (1, 2, 3, or 4). After that, let us create another column called year_quarter to represent the quarter of every year in string format, e.g. (“2021-Q1”, “2023-Q4”).\nThe package we will be using here is called lubridate, a package within the tidyverse library.\n\nviolence_against_civilians_filtered &lt;- violence_against_civilians_filtered %&gt;% \n  mutate(quarter = quarter(event_date)) %&gt;%\n  mutate(year_quarter = paste0(year, \"-Q\", quarter))\n\nstrategic_developments_filtered &lt;- strategic_developments_filtered %&gt;% \n  mutate(quarter = quarter(event_date)) %&gt;%\n  mutate(year_quarter = paste0(year, \"-Q\", quarter))\n\nexplosion_or_remoteviolence_filtered &lt;- explosion_or_remoteviolence_filtered %&gt;% \n  mutate(quarter = quarter(event_date)) %&gt;%\n  mutate(year_quarter = paste0(year, \"-Q\", quarter))\n\nbattles_filtered &lt;- battles_filtered %&gt;% \n  mutate(quarter = quarter(event_date)) %&gt;%\n  mutate(year_quarter = paste0(year, \"-Q\", quarter))",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "in-class-exercise/ex05/in-class-ex05.html",
    "href": "in-class-exercise/ex05/in-class-ex05.html",
    "title": "05 In-class Exercise (Review)",
    "section": "",
    "text": "Geographically referenced attributes\n-&gt; entities are geographically referenced/have a location -&gt; either polygons or points.\nPreliminary Visualisation -&gt; Just map it out on a map, then obtain\n\n\nTypes of Relationships to define Spatial Weights:\n\nAdjacent relationships (common boundary), also called Adjacency.\n\nif polygon data, there are also concerns that if a polygon is very long, then they will have a high adjacency count.\n\nDistance based relationships;\n\nif polygon data, the algorithm will find the centroids of each polygon. However, limitation: large and irregularly shaped polygons will result in centroids being extremely far away from others. To ‘fix’ this, we can shift the centroids of these large polygons closer to the other neighbours (depends on context.)\nif multipolygon/multipoint data, then you should choose the only necessary points so that your centroids or points are not in the middle of nowhere.\nIf points, it will be easier –&gt; just distance between points.\n\n\n\n\n\nwe can use binary metrics (whether within a search radius/distance)\na continuous metrics (higher weights if near, lower weights if further)\n\n\n\n\n\nSee: Rooks Case, Bishops Case, Queens/Kings Case\nLagged Adjacency for continuity metric, see first order adjacency, second order adjacency, i.e. (neighbour of neighbour)\n\n\n\n\n\nIn practice, we will not use spatial weights as-is, we will standardise the weights by row or by columns (gives the same final results as the matrix is symmetrical).\nThe summation of standardised weights will therefore be an average average.\n\n*GDPPC –&gt; GDP per capita",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "05 In-class Exercise 5 (Review)"
    ]
  },
  {
    "objectID": "in-class-exercise/ex05/in-class-ex05.html#spatial-weights",
    "href": "in-class-exercise/ex05/in-class-ex05.html#spatial-weights",
    "title": "05 In-class Exercise (Review)",
    "section": "",
    "text": "Types of Relationships to define Spatial Weights:\n\nAdjacent relationships (common boundary), also called Adjacency.\n\nif polygon data, there are also concerns that if a polygon is very long, then they will have a high adjacency count.\n\nDistance based relationships;\n\nif polygon data, the algorithm will find the centroids of each polygon. However, limitation: large and irregularly shaped polygons will result in centroids being extremely far away from others. To ‘fix’ this, we can shift the centroids of these large polygons closer to the other neighbours (depends on context.)\nif multipolygon/multipoint data, then you should choose the only necessary points so that your centroids or points are not in the middle of nowhere.\nIf points, it will be easier –&gt; just distance between points.\n\n\n\n\n\nwe can use binary metrics (whether within a search radius/distance)\na continuous metrics (higher weights if near, lower weights if further)\n\n\n\n\n\nSee: Rooks Case, Bishops Case, Queens/Kings Case\nLagged Adjacency for continuity metric, see first order adjacency, second order adjacency, i.e. (neighbour of neighbour)\n\n\n\n\n\nIn practice, we will not use spatial weights as-is, we will standardise the weights by row or by columns (gives the same final results as the matrix is symmetrical).\nThe summation of standardised weights will therefore be an average average.\n\n*GDPPC –&gt; GDP per capita",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "05 In-class Exercise 5 (Review)"
    ]
  },
  {
    "objectID": "in-class-exercise/ex05/in-class-ex05.html#importing-the-necessary",
    "href": "in-class-exercise/ex05/in-class-ex05.html#importing-the-necessary",
    "title": "05 In-class Exercise (Review)",
    "section": "2.1 Importing the necessary",
    "text": "2.1 Importing the necessary\nWe will be using a different version called GWmodel; Geographically-Weighted Models The latest date as of writing, is 2.4-1.\n\npacman::p_load(sf,spdep, tmap, tidyverse, knitr, GWmodel)\n\n\nhunan_sf &lt;- st_read(dsn=\"data/geospatial\", layer=\"Hunan\")\nhunan_2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nIf we have a dataset where we do not exactly know the projection CRS, there is no choice but to use non-projected CRS. But in this case, we should search for the EPSG Code for projected CRS at Hunan, China\n\n2.1.1 Join and Filter Out Unwanted Attributes\n\nhunan &lt;- left_join(hunan_sf, hunan_2012, by=\"County\") %&gt;%\n  select(1:3,7,15,16,31,32) # Selecting NAME_2, ID_3, NAME_3, County, GDPPC, GIO, Agri, Service\n\n\n\n2.1.2 Exporting our data\nOnce done, we will export our cleaned data set as a RDS file, so that we only to load in our final\n\nwrite_rds(hunan,\"data/rds/hunan_sf.rds\")\n\n\n\n2.1.3 Reading back our data\n\n\n2.1.4 Converting to SpatialPolygonDataFrame\nNote: if we try to run GWmodel, we realise that GWmodel is built around the older sp and not sf formats for handling spatial data in R.\nIn sp, we have multiple lists –&gt; data polygons proj4string\nLooking through, we are able to see that the attributes are\n\nhunan_sp &lt;- hunan_sf %&gt;% as_Spatial()\n\n\n\n2.1.5 Determine adaptive bandwidth.\nNote that .gwr is used for regression, but we just want to use their model. Without providing an actual function, we write GDPPC ~ 1 –&gt; which means GDPPC is a function of 1, i.e. GDPPC = GDPPC.\n\n\n\n\n\n\nCaution\n\n\n\nOur data is in lat,long, when we pass it through the algorithm, the algorithm will use the Great Circle projection. The output will be in kilometers (rather than metres)!\n\n\nGDPPC ~ 1\n\nbw_AIC &lt;- bw.gwr(GDPPC ~ 1,\n                 data= hunan_sp,\n                 approach = \"AIC\",    # or use CV (Cross-Validation) the AI models\n                 adaptive = TRUE,     # calculate \n                 kernel = \"bisquare\", #\n                 longlat = TRUE)      # Given that our data is in latlong, the great circle  \n\nAdaptive bandwidth (number of nearest neighbours): 62 AICc value: 1923.156 \nAdaptive bandwidth (number of nearest neighbours): 46 AICc value: 1920.469 \nAdaptive bandwidth (number of nearest neighbours): 36 AICc value: 1917.324 \nAdaptive bandwidth (number of nearest neighbours): 29 AICc value: 1916.661 \nAdaptive bandwidth (number of nearest neighbours): 26 AICc value: 1914.897 \nAdaptive bandwidth (number of nearest neighbours): 22 AICc value: 1914.045 \nAdaptive bandwidth (number of nearest neighbours): 22 AICc value: 1914.045 \n\n\nNote that we can see 22 Nearest Neighbours\n\nbw_CV &lt;- bw.gwr(GDPPC ~ 1,\n                 data= hunan_sp,\n                 approach = \"CV\",\n                 adaptive = TRUE,     # calculate \n                 kernel = \"bisquare\", #\n                 longlat = TRUE)      # Given that our data is in latlong, the great circle  \n\nAdaptive bandwidth: 62 CV score: 15515442343 \nAdaptive bandwidth: 46 CV score: 14937956887 \nAdaptive bandwidth: 36 CV score: 14408561608 \nAdaptive bandwidth: 29 CV score: 14198527496 \nAdaptive bandwidth: 26 CV score: 13898800611 \nAdaptive bandwidth: 22 CV score: 13662299974 \nAdaptive bandwidth: 22 CV score: 13662299974 \n\n\n\nbw_CV_fix &lt;- bw.gwr(GDPPC ~ 1,\n                 data= hunan_sp,\n                 approach = \"CV\",\n                 adaptive = FALSE,\n                 kernel = \"bisquare\", #\n                 longlat = TRUE)      # Given that our data is in latlong, the great circle\n\nFixed bandwidth: 357.4897 CV score: 16265191728 \nFixed bandwidth: 220.985 CV score: 14954930931 \nFixed bandwidth: 136.6204 CV score: 14134185837 \nFixed bandwidth: 84.48025 CV score: 13693362460 \nFixed bandwidth: 52.25585 CV score: Inf \nFixed bandwidth: 104.396 CV score: 13891052305 \nFixed bandwidth: 72.17162 CV score: 13577893677 \nFixed bandwidth: 64.56447 CV score: 14681160609 \nFixed bandwidth: 76.8731 CV score: 13444716890 \nFixed bandwidth: 79.77877 CV score: 13503296834 \nFixed bandwidth: 75.07729 CV score: 13452450771 \nFixed bandwidth: 77.98296 CV score: 13457916138 \nFixed bandwidth: 76.18716 CV score: 13442911302 \nFixed bandwidth: 75.76323 CV score: 13444600639 \nFixed bandwidth: 76.44916 CV score: 13442994078 \nFixed bandwidth: 76.02523 CV score: 13443285248 \nFixed bandwidth: 76.28724 CV score: 13442844774 \nFixed bandwidth: 76.34909 CV score: 13442864995 \nFixed bandwidth: 76.24901 CV score: 13442855596 \nFixed bandwidth: 76.31086 CV score: 13442847019 \nFixed bandwidth: 76.27264 CV score: 13442846793 \nFixed bandwidth: 76.29626 CV score: 13442844829 \nFixed bandwidth: 76.28166 CV score: 13442845238 \nFixed bandwidth: 76.29068 CV score: 13442844678 \nFixed bandwidth: 76.29281 CV score: 13442844691 \nFixed bandwidth: 76.28937 CV score: 13442844698 \nFixed bandwidth: 76.2915 CV score: 13442844676 \nFixed bandwidth: 76.292 CV score: 13442844679 \nFixed bandwidth: 76.29119 CV score: 13442844676 \nFixed bandwidth: 76.29099 CV score: 13442844676 \nFixed bandwidth: 76.29131 CV score: 13442844676 \nFixed bandwidth: 76.29138 CV score: 13442844676 \nFixed bandwidth: 76.29126 CV score: 13442844676 \nFixed bandwidth: 76.29123 CV score: 13442844676 \n\n\n\nbw_AIC_fix &lt;- bw.gwr(GDPPC ~ 1,\n                 data= hunan_sp,\n                 approach = \"AIC\",\n                 adaptive = FALSE,\n                 kernel = \"bisquare\", #\n                 longlat = TRUE)      # Given that our data is in latlong, the great circle\n\nFixed bandwidth: 357.4897 AICc value: 1927.631 \nFixed bandwidth: 220.985 AICc value: 1921.547 \nFixed bandwidth: 136.6204 AICc value: 1919.993 \nFixed bandwidth: 84.48025 AICc value: 1940.603 \nFixed bandwidth: 168.8448 AICc value: 1919.457 \nFixed bandwidth: 188.7606 AICc value: 1920.007 \nFixed bandwidth: 156.5362 AICc value: 1919.41 \nFixed bandwidth: 148.929 AICc value: 1919.527 \nFixed bandwidth: 161.2377 AICc value: 1919.392 \nFixed bandwidth: 164.1433 AICc value: 1919.403 \nFixed bandwidth: 159.4419 AICc value: 1919.393 \nFixed bandwidth: 162.3475 AICc value: 1919.394 \nFixed bandwidth: 160.5517 AICc value: 1919.391 \n\n\n\n\n2.1.6 Computing geographically weighted summary statistics\nWe will now calculate the summary statistics. Note that your parameters must be the same! If you used adaptive bandwidth, your parameter here should be parameter as well. Otherwise,\n\nNote, under the list SDF &gt; data, we open the table and see:\n\nL means Local (note remember that we have 22 nearest neighbours)\nLM means local mean, LSD means local standard deviation\n\ngwstat &lt;- gwss(data= hunan_sp,\n               vars= \"GDPPC\",\n               bw = bw_AIC,\n               kernel = \"bisquare\",\n               adaptive = TRUE,\n               longlat = TRUE)\n\n\ngwstat_df &lt;- as.data.frame(gwstat$SDF)\nhunan_gstat &lt;- cbind(hunan_sf, gwstat_df) # we are appending both tables together based on their index. CAUTION: not to change the sequence of your data during this process.\n\n\n\n2.1.7 Visual Map of the Summary Statistics (Mean)\n\ntmap_mode(\"plot\")\ntm_shape(hunan_gstat) + #\n  tm_fill(\"GDPPC_LM\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders( alpha = 0.5 ) +\n  tm_layout(main.title = \"Distribution of geographically weighted mean\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.text.size = 0.7,\n            legend.height = 1,\n            legend.width = 1,\n            frame = TRUE)\n\n\n\n\n\n\n\n\nFurther notes for Take-Home-Exercise01\nYou do not need to do the analysis for the whole of Myanmar. It will require an extremely large computational power.\nYou can scale down the study area into specific regions, and try to find out what are the localised spatial point patterns.\nIn fact, that will allow us to see the localised patterns better. See previous work on Take-Home Exercises.",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "05 In-class Exercise 5 (Review)"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#visualising-the-wrangled-acled-data",
    "href": "take-home-exercise/ex01/take-home-ex01.html#visualising-the-wrangled-acled-data",
    "title": "01 Take Home Exercise 1",
    "section": "2.2 Visualising the wrangled ACLED data",
    "text": "2.2 Visualising the wrangled ACLED data\nNow that we have categorised the ACLED points data into specific yearly quarters, let us plot them onto the map of myanmar to see if there are any obvious patterns.\nSo far, we can have up to 14 temporal categories, with 4 quarters per year in Year 2021, 2022, 2023, and 2 quarters in the first half of 2024.\n\ntmap_mode(\"plot\")\n\nmap_quarterly_ViolAgstCiv &lt;- tm_shape(admin_1) +\n  tm_style(\"white\") +\n  tm_polygons() +\n  tm_shape(violence_against_civilians) +\n  tm_dots(size= 0.1) + # make sizes a bit bigger so that it can be seen\n  tm_facets(along = \"year_quarter\", # 1 separate map for each year quarter\n              free.coords = FALSE,\n              drop.units = TRUE)\n\nmap_quarterly_StratDev &lt;- tm_shape(admin_1) +\n  tm_style(\"white\") +\n  tm_polygons() +\n  tm_shape(strategic_developments) +\n  tm_dots(size= 0.1) + # make sizes a bit bigger so that it can be seen\n  tm_facets(along = \"year_quarter\", # 1 separate map for each year quarter\n              free.coords = FALSE,\n              drop.units = TRUE)\n\nmap_quarterly_ExploOrRemoViol &lt;- tm_shape(admin_1) +\n  tm_style(\"white\") +\n  tm_polygons() +\n  tm_shape(explosion_or_remoteviolence) +\n  tm_dots(size= 0.1) + # make sizes a bit bigger so that it can be seen\n  tm_facets(along = \"year_quarter\", # 1 separate map for each year quarter\n              free.coords = FALSE,\n              drop.units = TRUE)\n\nmap_quarterly_Battles &lt;- tm_shape(admin_1) +\n  tm_style(\"white\") +\n  tm_polygons() +\n  tm_shape(battles) +\n  tm_dots(size= 0.1) + # make sizes a bit bigger so that it can be seen\n  tm_facets(along = \"year_quarter\", # 1 separate map for each year quarter\n              free.coords = FALSE,\n              drop.units = TRUE)\n\ntmap_animation(map_quarterly_ViolAgstCiv,\n               filename=\"images/quarterly_violenceAgainstCivilians.gif\", delay=100)\n\ntmap_animation(map_quarterly_StratDev,\n               filename=\"images/quarterly_strategicDevelopments.gif\", delay=100)\n\ntmap_animation(map_quarterly_ExploOrRemoViol,\n               filename=\"images/quarterly_explosionOrRemoteViolence.gif\", delay=100)\n\ntmap_animation(map_quarterly_Battles,\n               filename=\"images/quarterly_battles.gif\", delay=100)\n\n\n\n\nQuarterly events: Violence Against Civilians\n\n\n\n\n\nQuarterly events: Strategic Developments\n\n\n\n\n\nQuarterly events: Explosion / Remote Violence\n\n\n\n\n\nQuarterly events: Battles\n\n\nAfter a brief observation of the",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#identifying-study-area",
    "href": "take-home-exercise/ex01/take-home-ex01.html#identifying-study-area",
    "title": "01 Take Home Exercise 1",
    "section": "2.3 Identifying Study Area",
    "text": "2.3 Identifying Study Area\n\ntm_shape(admin_1) +\n  tm_style(\"white\") +\n  tm_polygons(\"ST\", palette = \"Set3\", title = \"States/Regions\") +\n  tm_shape(st_centroid(admin_1)) + # Find the centroids\n  tm_text(\"ST\", size = 0.6, col = \"black\", shadow = TRUE, just=\"center\") + # Add labels at centroids\n  tm_layout(title = \"Administrative Boundaries Level 1\",\n            legend.outside = TRUE,\n            # asp = 0) # Adjust aspect ratio\n            outer.margins = c(0, 0, 0, 0), # Remove outer margins\n            inner.margins = c(0, 0, 0, 0)) # Remove inner margins",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#import-libraries-packages",
    "href": "take-home-exercise/ex01/take-home-ex01.html#import-libraries-packages",
    "title": "01 Take Home Exercise 1",
    "section": "3.1 Import Libraries / Packages",
    "text": "3.1 Import Libraries / Packages\n\npacman::p_load(tidyverse, sf, spatstat, ggplot2, tmap, gifski, sparr, raster)",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#import-raw-data",
    "href": "take-home-exercise/ex01/take-home-ex01.html#import-raw-data",
    "title": "01 Take Home Exercise 1",
    "section": "3.2 Import Raw Data",
    "text": "3.2 Import Raw Data\n\n3.2.1 ACLED Data Set\nBefore we start manipulating the data, it is important that we understand what each data column and data type mean and how valuable it is for our analysis.\nTo begin, let us refer to the ACLED data set codebook\nUseful links:\n\nData Set Columns Information\nData Set Myanmar Overview\n\nThe data was retrieved from the ACLED data portal. Specifically, the following types of data was retrieved:\n\nviolence-against-civilians.csv\nstrategic-developments.csv\nbattles.csv\nexplosion-or-remoteviolence.csv\n\nIn terms of event types, four main event types were retrieved: Battles, Explosion/Remote violence, Strategic developments, and Violence against civilians.\nIn terms of study period, we are focused on quarterly armed conflict events from January 2021 until June 2024.\nThe code chunk below imports the data for our analysis.\n\nviolence_against_civilians &lt;- read_csv(\"data/raw/aspatial/ACLED/violence-against-civilians.csv\") %&gt;%\n  st_as_sf(coords =c(\n    \"longitude\", \"latitude\"),\n    crs=4326) %&gt;%\n    st_transform(crs = 32647) %&gt;%\n    mutate(event_date = dmy(event_date))\n\nstrategic_developments &lt;- read_csv(\"data/raw/aspatial/ACLED/strategic-developments.csv\") %&gt;%\n  st_as_sf(coords =c(\n    \"longitude\", \"latitude\"),\n    crs=4326) %&gt;%\n    st_transform(crs = 32647) %&gt;%\n    mutate(event_date = dmy(event_date))\n\nbattles &lt;- read_csv(\"data/raw/aspatial/ACLED/battles.csv\") %&gt;%\n  st_as_sf(coords =c(\n    \"longitude\", \"latitude\"),\n    crs=4326) %&gt;%\n    st_transform(crs = 32647) %&gt;%\n    mutate(event_date = dmy(event_date))\n\nexplosion_or_remoteviolence &lt;- read_csv(\"data/raw/aspatial/ACLED/explosion-or-remoteviolence.csv\") %&gt;%\n  st_as_sf(coords =c(\n    \"longitude\", \"latitude\"),\n    crs=4326) %&gt;%\n    st_transform(crs = 32647) %&gt;%\n    mutate(event_date = dmy(event_date))\n\n\n\n3.2.2 Administrative Boundary Dataset\n\n\n\n\n\n\n\nMyamar’s Administrative Division Hierachy (source: Wikipedia)\n\n\nIn this exercise, we will only analyse geographical distribution of point data up to the 3rd degree administrative division, i.e. up to Township level.\nThe following data are the administrative boundaries of Myanmar, obtained from MIMU Vector Boundaries, a common data and information repository by a NGO (related to UN).\nWhile many data formats exist, we will choose .csv files when downloading as they are easier to read and work with. A suitable alternative is the .shp file format available on the website.\nDatasets:\n\nmmr_polbnda_adm0_250k_mimu_1.csv National boundary of Myanmar\nmmr_polbnda_adm1_250k_mimu_1.csv Region/State/Union Territory level boundary of Myanmar\nmmr_polbnda2_adm1_250k_mimu_1.csv Sub-Region/State/Union Territory level boundary of Myanmar; Sub-region divides a region into smaller divisions, such as “Bago (East)” and “Bago (West)” instead of “Bago”\nmmr_polbnda_adm2_250k_mimu.csv District/Self-Administered Zone level boundary of Myanmar\nmmr_polbnda_adm3_250k_mimu_1.csv Township level boundary of Myanmar\n\n\n# kml file not easy to read due to nested columns for attributes data\nadmin_0 &lt;- read_csv(\"data/raw/geospatial/mimu_admin_boundary/mmr_polbnda_adm0_250k_mimu_1.csv\") %&gt;%\n  st_as_sf(wkt = \"the_geom\",\n    crs=4326) %&gt;%\n    st_transform(crs = 32647)\n    \nadmin_1 &lt;- read_csv(\"data/raw/geospatial/mimu_admin_boundary/mmr_polbnda_adm1_250k_mimu_1.csv\") %&gt;%\n  st_as_sf(wkt = \"the_geom\",\n    crs=4326) %&gt;%\n    st_transform(crs = 32647)\n\nadmin_1_sub &lt;- read_csv(\"data/raw/geospatial/mimu_admin_boundary/mmr_polbnda2_adm1_250k_mimu_1.csv\") %&gt;%\n  st_as_sf(wkt = \"the_geom\",\n    crs=4326) %&gt;%\n    st_transform(crs = 32647)\n\nadmin_2 &lt;- read_csv(\"data/raw/geospatial/mimu_admin_boundary/mmr_polbnda_adm2_250k_mimu.csv\") %&gt;%\n  st_as_sf(wkt = \"the_geom\",\n    crs=4326) %&gt;%\n    st_transform(crs = 32647)\n\nadmin_3 &lt;- read_csv(\"data/raw/geospatial/mimu_admin_boundary/mmr_polbnda_adm3_250k_mimu_1.csv\") %&gt;%\n  st_as_sf(wkt = \"the_geom\",\n    crs=4326) %&gt;%\n    st_transform(crs = 32647)\n\n\n\n3.2.3 Quick Plot To Visualise Data Sets\nBefore we continue, let’s get a visual sense of the data by plotting it on a map to ensure we are working with the right dataset. In this step, we will also try to see if there are any erranous data that we have to clean later.\nCheck the Administrative Boundaries Data Set\nFrom the left to right, we are able to see the increase in degree of administrative boundary division; respectively, they represent\n\n“National”,\n“Region/State/Union Territory”,\n“Sub-Region/State/Union Territory”,\n“District/Self-Administered Zone”, and\n“Township”\n\n\ntmap_mode(\"plot\")\ntmap_style(\"classic\")\n\ntmap_arrange(\n  qtm(admin_0, title=\"National\"), \n  qtm(admin_1, title=\"Region/State/Union\\nTerritory\"), \n  qtm(admin_1_sub, title=\"Sub-Region/State/Union\\nTerritory\"), \n  qtm(admin_2, title=\"District/Self-\\nAdministered Zone\"), \n  qtm(admin_3, title=\"Township\"), \n  ncol = 5\n)\n\n\n\n\nPlot of Admin Boundaries of Myanmar Raw Data\n\n\nConclusion: Administrative Boundaries Looks Okay\nLooking at both the attribute table and the plot, the administrative boundaries data seem quite alright, so we might not need to do any cleaning. Anyway, the precision of the polygons it not as critical as the precision of the ACLED data (which will be used for our spatial point patterns analysis).\nCheck ACLED Data Set\nSince the ACLED data is our main focus, let us now plot the data points onto the map of Myanmar.\nIn the code chunk below, we plot out the various types of data points from ACLED, namely (from left to right):\n\n“Violence against Civilians”,\n“Strategic Developments”,\n“Explosion/Remote Violence”, and\n“Battles”\n\n\ntmap_mode(\"plot\")\ntmap_style(\"cobalt\")\n\ntmap_arrange(\n  qtm(admin_0) + qtm(violence_against_civilians, title=\"Violence against\\nCivilians\"), \n  qtm(admin_0) + qtm(strategic_developments, title=\"Strategic\\nDevelopments\"), \n  qtm(admin_0) + qtm(explosion_or_remoteviolence, title=\"Explosion/\\nRemote Violence\"), \n  qtm(admin_0) + qtm(battles, title=\"Battles\"), \n  ncol = 4\n)\n\n\n\n\nPlot of ACLED Raw Points Data\n\n\nObservation: ACLED Data Set needs further study, may need to clean\nNothing looks very off at a glance, but we should look into the attribute columns to see if the data set is truly clean. Let us refer to the codebook again, and see if we can spot any potentially critical problems.\nReferring to the attribute columns and the codebook, we can see potentially critical concerns:\n\ngeo_precision – The precision of the geocoded coordinates ranges from code 1 to code 3; where lower level implies higher precision.\n\nIn particular, “[if] a larger region is mentioned, the closest natural location noted in reporting (like “border area,” “forest,” or “sea,” among others) – or a provincial capital is used if no other information at all is available – is chosen to represent the region, and ‘Geo-precision’ code 3 is recorded.” (page 36)\n\ntime_precision – The precision of the recorded datetime ranges from code 1 to code 3; where lower level implies higher precision.\n\nIn particular, “if the source material only indicates that an event took place sometime during a month (i.e. in the past two or three weeks, or in January), without reference to the particular date, the month mid-point is chosen. If the beginning or end of the month is noted, the first and last date is used, respectively. In both of these cases, a ‘Time precision’ code of 3 is recorded.” (page 36-37)\n\n\nIn both cases, ACLED do not include events with less spatial or temporal precision.\nIdentifying code 3 precisions\nGiven that precision of our point data is crucial to our spatial-temporal point patterns analysis, we should see how much of our data is imprecise, and whether we should keep the imprecise data points.\n\n# Violence against Civilians\nviolence_against_civilians %&gt;% count(geo_precision)\nviolence_against_civilians %&gt;% count(time_precision)\n\n# Strategic Developments\nstrategic_developments %&gt;% count(geo_precision)\nstrategic_developments %&gt;% count(time_precision)\n\n# Explosion or Remote Violence\nexplosion_or_remoteviolence %&gt;% count(geo_precision)\nexplosion_or_remoteviolence %&gt;% count(time_precision)\n\n# Battles\nbattles %&gt;% count(geo_precision)\nbattles %&gt;% count(time_precision)\n\nConclusion: Drop time and spatial precision code 3 data values from ACLED Data set\nFrom this analysis, we can see that the count of precision code 3 in both time and spatial precision is actually very low, it might be worth dropping the values with low time and spatial precision.\nLet us start cleaning the data",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#identify-study-areas",
    "href": "take-home-exercise/ex01/take-home-ex01.html#identify-study-areas",
    "title": "01 Take Home Exercise 1",
    "section": "2.1 Identify Study Area(s)",
    "text": "2.1 Identify Study Area(s)\n\n2.1.1 Visualise ACLED data\nNow that we have categorised the ACLED points data into specific yearly quarters, let us plot them onto the map of myanmar to see if there are any obvious patterns.\nSo far, we can have up to 14 temporal categories, with 4 quarters per year in Year 2021, 2022, 2023, and 2 quarters in the first half of 2024.\n\ntmap_mode(\"plot\")\ntmap_style(\"classic\")\n\ntm_shape(admin_1) +\n  tm_polygons() +\n  tm_shape(violence_against_civilians) +\n  tm_dots(size= 0.05, col = \"black\", alpha = 0.5) +\n  tm_facets(by = \"year_quarter\",\n              free.coords = FALSE,\n              drop.units = TRUE) +\n  tm_layout(main.title=\"Violence Against Civilians (Quarterly)\")\n\ntm_shape(admin_1) +\n  tm_polygons() +\n  tm_shape(strategic_developments) +\n  tm_dots(size= 0.05, col = \"black\", alpha = 0.5) +\n  tm_facets(by = \"year_quarter\",\n              free.coords = FALSE,\n              drop.units = TRUE) +\n  tm_layout(main.title=\"Strategic Developments (Quarterly)\")\n\ntm_shape(admin_1) +\n  tm_polygons() +\n  tm_shape(explosion_or_remoteviolence) +\n  tm_dots(size= 0.05, col = \"black\", alpha = 0.5) +\n  tm_facets(by = \"year_quarter\",\n              free.coords = FALSE,\n              drop.units = TRUE) +\n  tm_layout(main.title=\"Explosion/Remote Violence (Quarterly)\")\n\ntm_shape(admin_1) +\n  tm_polygons() +\n  tm_shape(battles) +\n  tm_dots(size= 0.05, col = \"black\", alpha = 0.5) +\n  tm_facets(by = \"year_quarter\",\n              free.coords = FALSE,\n              drop.units = TRUE) +\n  tm_layout(main.title=\"Battles (Quarterly)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nmap_quarterly_ViolAgstCiv &lt;- tm_shape(admin_1) +\n  tm_style(\"white\") +\n  tm_polygons() +\n  tm_shape(violence_against_civilians) +\n  tm_dots(size= 0.1) + # make sizes a bit bigger so that it can be seen\n  tm_facets(along = \"year_quarter\", # 1 separate map for each year quarter\n              free.coords = FALSE,\n              drop.units = TRUE)\n\nmap_quarterly_StratDev &lt;- tm_shape(admin_1) +\n  tm_style(\"white\") +\n  tm_polygons() +\n  tm_shape(strategic_developments) +\n  tm_dots(size= 0.1) + # make sizes a bit bigger so that it can be seen\n  tm_facets(along = \"year_quarter\", # 1 separate map for each year quarter\n              free.coords = FALSE,\n              drop.units = TRUE)\n\nmap_quarterly_ExploOrRemoViol &lt;- tm_shape(admin_1) +\n  tm_style(\"white\") +\n  tm_polygons() +\n  tm_shape(explosion_or_remoteviolence) +\n  tm_dots(size= 0.1) + # make sizes a bit bigger so that it can be seen\n  tm_facets(along = \"year_quarter\", # 1 separate map for each year quarter\n              free.coords = FALSE,\n              drop.units = TRUE)\n\nmap_quarterly_Battles &lt;- tm_shape(admin_1) +\n  tm_style(\"white\") +\n  tm_polygons() +\n  tm_shape(battles) +\n  tm_dots(size= 0.1) + # make sizes a bit bigger so that it can be seen\n  tm_facets(along = \"year_quarter\", # 1 separate map for each year quarter\n              free.coords = FALSE,\n              drop.units = TRUE)\n\ntmap_animation(map_quarterly_ViolAgstCiv,\n               filename=\"images/quarterly_violenceAgainstCivilians.gif\", delay=100)\n\ntmap_animation(map_quarterly_StratDev,\n               filename=\"images/quarterly_strategicDevelopments.gif\", delay=100)\n\ntmap_animation(map_quarterly_ExploOrRemoViol,\n               filename=\"images/quarterly_explosionOrRemoteViolence.gif\", delay=100)\n\ntmap_animation(map_quarterly_Battles,\n               filename=\"images/quarterly_battles.gif\", delay=100)\n\n\n\n\nQuarterly events: Violence Against Civilians\n\n\n\n\n\nQuarterly events: Strategic Developments\n\n\n\n\n\nQuarterly events: Explosion / Remote Violence\n\n\n\n\n\nQuarterly events: Battles\n\n\n\n\n2.1.2 Decision: Study Area(s) in the Western Region (Rakhine, Chin, Magway)\nDeciding on a Study Area(s)\nAfter a brief observation of the points data, I’ve decided to focus on the western regions/states which seems to have quite a variety of point patterns throughout time. According to the map generated by the code chunk below, we can see that the places are called Rakhine, Chin, and Magway. Throughout the data sets, we see quite a number of conflict events occurring in the area in northern Rakhine. Let us focus on this area for our analysis hereon.\n\ntm_shape(admin_1) +\n  tm_style(\"white\") +\n  tm_polygons(\"ST\", palette = \"Set3\", title = \"States/Regions\") +\n  tm_shape(st_centroid(admin_1)) + # Find the centroids\n  tm_text(\"ST\", size = 0.6, col = \"black\", shadow = TRUE, just=\"center\") + # Add labels at centroids\n  tm_layout(title = \"Administrative Boundaries Level 1\",\n            legend.outside = TRUE)",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#select-study-areas-boundary",
    "href": "take-home-exercise/ex01/take-home-ex01.html#select-study-areas-boundary",
    "title": "01 Take Home Exercise 1",
    "section": "3.1 Select Study Area(s) Boundary",
    "text": "3.1 Select Study Area(s) Boundary\nLet us also select the study area(s) boundary\n\nstudyarea &lt;- admin_1 %&gt;% filter(ST %in% c(\"Rakhine\", \"Chin\", \"Magway\"))\nqtm(studyarea)",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#select-study-areas-acled-data",
    "href": "take-home-exercise/ex01/take-home-ex01.html#select-study-areas-acled-data",
    "title": "01 Take Home Exercise 1",
    "section": "3.2 Select Study Area(s) ACLED Data",
    "text": "3.2 Select Study Area(s) ACLED Data\nSince ACLED already came with attributes admin1, admin2, and admin3, let us use it to help us filter the points data.\n\n# Violence Against Civilians\nstudyarea_vac &lt;- violence_against_civilians %&gt;% \n  filter(admin1 %in% c(\"Rakhine\", \"Chin\", \"Magway\"))\n\n# Strategic Deployment\nstudyarea_stdp &lt;- strategic_developments %&gt;% \n  filter(admin1 %in% c(\"Rakhine\", \"Chin\", \"Magway\"))\n\n# Explosions / Remote violence\nstudyarea_erv &lt;- explosion_or_remoteviolence %&gt;% \n  filter(admin1 %in% c(\"Rakhine\", \"Chin\", \"Magway\"))\n\n# Battles\nstudyarea_bat &lt;- battles %&gt;% \n  filter(admin1 %in% c(\"Rakhine\", \"Chin\", \"Magway\"))\n\nLet us visualise the data again.\n\ntmap_mode(\"plot\")\ntmap_style(\"white\")\n\ntm_shape(studyarea) +\n  tm_polygons() +\n  tm_shape(studyarea_vac) +\n  tm_dots(size= 0.1, col = \"black\") +\n  tm_facets(by = \"year_quarter\",\n            free.coords = FALSE,\n            drop.units = TRUE,\n            ncol = 7) +\n  tm_layout(main.title=\"Violence Against Civilians (Quarterly)\")\n\ntm_shape(studyarea) +\n  tm_polygons() +\n  tm_shape(studyarea_stdp) +\n  tm_dots(size= 0.1, col = \"black\") +\n  tm_facets(by = \"year_quarter\",\n            free.coords = FALSE,\n            drop.units = TRUE,\n            ncol = 7) +\n  tm_layout(main.title=\"Strategic Developments (Quarterly)\")\n\ntm_shape(studyarea) +\n  tm_polygons() +\n  tm_shape(studyarea_erv) +\n  tm_dots(size= 0.1, col = \"black\") +\n  tm_facets(by = \"year_quarter\",\n            free.coords = FALSE,\n            drop.units = TRUE,\n            ncol = 7) +\n  tm_layout(main.title=\"Explosion/Remote Violence (Quarterly)\")\n\ntm_shape(studyarea) +\n  tm_polygons() +\n  tm_shape(studyarea_bat) +\n  tm_dots(size= 0.1, col = \"black\") +\n  tm_facets(by = \"year_quarter\",\n            free.coords = FALSE,\n            drop.units = TRUE,\n            ncol = 7) +\n  tm_layout(main.title=\"Battles (Quarterly)\")",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#export-and-save-data-sets-rds-file-format",
    "href": "take-home-exercise/ex01/take-home-ex01.html#export-and-save-data-sets-rds-file-format",
    "title": "01 Take Home Exercise 1",
    "section": "3.3 Export and Save Data Sets (RDS file format)",
    "text": "3.3 Export and Save Data Sets (RDS file format)\nBefore we continue, let us export and save our data.\n\n# Study Areas\nsaveRDS(studyarea, \"data/final_rds/studyarea.rds\")\n\n# ACLED Data\nsaveRDS(studyarea_vac, \n        \"data/final_rds/studyarea_violence_against_civilians.rds\")\nsaveRDS(studyarea_stdp, \n        \"data/final_rds/studyarea_strategic_developments.rds\")\nsaveRDS(studyarea_erv, \n        \"data/final_rds/studyarea_explosion_or_remoteviolence.rds\")\nsaveRDS(studyarea_bat, \n        \"data/final_rds/studyarea_battles.rds\")",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#import-previous-data",
    "href": "take-home-exercise/ex01/take-home-ex01.html#import-previous-data",
    "title": "01 Take Home Exercise 1",
    "section": "4.1 Import Previous Data",
    "text": "4.1 Import Previous Data\n\nstudyarea &lt;- readRDS(\"data/final_rds/studyarea.rds\")\nstudyarea_vac &lt;- readRDS(\"data/final_rds/studyarea_violence_against_civilians.rds\")\nstudyarea_stdp &lt;- readRDS(\"data/final_rds/studyarea_strategic_developments.rds\")\nstudyarea_erv &lt;- readRDS(\"data/final_rds/studyarea_explosion_or_remoteviolence.rds\")\nstudyarea_bat &lt;- readRDS(\"data/final_rds/studyarea_battles.rds\")",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#separate-analysis",
    "href": "take-home-exercise/ex01/take-home-ex01.html#separate-analysis",
    "title": "01 Take Home Exercise 1",
    "section": "4.2 Separate Analysis",
    "text": "4.2 Separate Analysis\n\n4.2.1 Create Owin Objects\n\n# Creating the Owin objects\nChin_owin &lt;- studyarea %&gt;% filter(ST == \"Chin\") %&gt;% as.owin()\nMagway_owin &lt;- studyarea %&gt;% filter(ST == \"Magway\") %&gt;% as.owin()\nRakhine_owin &lt;- studyarea %&gt;% filter(ST == \"Rakhine\") %&gt;% as.owin()\n\n\n#| output: false\n\n# Function to split any sf object into a list of sf by year_quarter\n# Outputs a list of ppp (converted from each sf in each year_quarter)\nsplit_by_year_quarter &lt;- function(sf_object) {\n  # Get unique year_quarter values\n  unique_quarters &lt;- unique(sf_object$year_quarter)\n  \n  # Split the sf object into a list of sf objects by year_quarter,\n  # Then, convert to ppp objects\n  # Then, jittle the points (to handle the presence of duplicated points)\n  split_list &lt;- lapply(unique_quarters, function(unique_quarters) {\n    sf_object %&gt;% filter(year_quarter == unique_quarters) %&gt;% \n      as.ppp() %&gt;% \n      rjitter(retry=TRUE, \n              nsim=1, \n              drop=TRUE)\n  })\n  \n  # Name the list elements by their year_quarter values\n  names(split_list) &lt;- unique_quarters\n  \n  return(split_list)\n}\n\nvac_ppp_list &lt;- split_by_year_quarter(studyarea_vac)\nstdp_ppp_list &lt;- split_by_year_quarter(studyarea_stdp)\nerv_ppp_list &lt;- split_by_year_quarter(studyarea_erv)\nbat_ppp_list &lt;- split_by_year_quarter(studyarea_bat)\n\n\n\n4.2.2 Separate ACLED Points by Region (using Owin)\nNow, let us use the Owin objects to filter the ACLED data points for each region.\n\n# Function to Extract ppp points within a specified Owin object\nfilter_ppp_within_window &lt;- function(ppp_list, window) {\n  filtered_list &lt;- lapply(ppp_list, function(ppp_item) {\n    ppp_item[window]\n  })\n  return(filtered_list)\n}\n\nChin_vac_ppp_list &lt;- filter_ppp_within_window(vac_ppp_list, Chin_owin)\nChin_stdp_ppp_list &lt;- filter_ppp_within_window(stdp_ppp_list, Chin_owin)\nChin_erv_ppp_list &lt;- filter_ppp_within_window(erv_ppp_list, Chin_owin)\nChin_bat_ppp_list &lt;- filter_ppp_within_window(bat_ppp_list, Chin_owin)\n\nMagway_vac_ppp_list &lt;- filter_ppp_within_window(vac_ppp_list, Magway_owin)\nMagway_stdp_ppp_list &lt;- filter_ppp_within_window(stdp_ppp_list, Magway_owin)\nMagway_erv_ppp_list &lt;- filter_ppp_within_window(erv_ppp_list, Magway_owin)\nMagway_bat_ppp_list &lt;- filter_ppp_within_window(bat_ppp_list, Magway_owin)\n\nRakhine_vac_ppp_list &lt;- filter_ppp_within_window(vac_ppp_list, Rakhine_owin)\nRakhine_stdp_ppp_list &lt;- filter_ppp_within_window(stdp_ppp_list, Rakhine_owin)\nRakhine_erv_ppp_list &lt;- filter_ppp_within_window(erv_ppp_list, Rakhine_owin)\nRakhine_bat_ppp_list &lt;- filter_ppp_within_window(bat_ppp_list, Rakhine_owin)\n\n\n\n4.2.3 Computing KDE (adaptive bandwidth) for each Region\n\n4.2.3.1 Choosing the right adaptive bandwidth methods\nThere are four types of adaptive bandwidth methods available: bw.CvL, bw.scott, bw.ppl, and bw.diggle.\nThese four bandwidth methods often result in different ‘smoothness’ of the resulting density map, therefore, let us try plotting out the density maps and try to see which one suits our purposes.\nWe will use the following data (selected randomly) for our experiment:\n\nViolence Against Civilians in Chin, in Year 2024, Quarter 1\nStrategic Developments in Rakhine, in Year 2023, Quarter 4\n\n\nchin_vac_2024Q1.CvL &lt;- density(Chin_vac_ppp_list$`2024-Q1` %&gt;% rescale.ppp(s=100000, unitname=\"km\"),\n        sigma=bw.CvL,\n        edge=TRUE,\n        kernel=\"gaussian\") %&gt;% plot()\n\n\n\n\n\n\n\nchin_vac_2024Q1.scott &lt;- density(Chin_vac_ppp_list$`2024-Q1` %&gt;% rescale.ppp(s=100000, unitname=\"km\"),\n        sigma=bw.scott,\n        edge=TRUE,\n        kernel=\"gaussian\") %&gt;% plot()\n\n\n\n\n\n\n\nchin_vac_2024Q1.ppl &lt;- density(Chin_vac_ppp_list$`2024-Q1` %&gt;% rescale.ppp(s=100000, unitname=\"km\"),\n        sigma=bw.ppl,\n        edge=TRUE,\n        kernel=\"gaussian\") %&gt;% plot()\n\n\n\n\n\n\n\nchin_vac_2024Q1.diggle &lt;- density(Chin_vac_ppp_list$`2024-Q1` %&gt;% rescale.ppp(s=100000, unitname=\"km\"),\n        sigma=bw.diggle,\n        edge=TRUE,\n        kernel=\"gaussian\") %&gt;% plot()\n\n\n\n\n\n\n\npar(mfrow=c(1,4))\nplot(chin_vac_2024Q1.CvL, main = \"bw.CvL\")\nplot(chin_vac_2024Q1.scott, main = \"bw.scott\")\nplot(chin_vac_2024Q1.ppl, main = \"bw.ppl\")\nplot(chin_vac_2024Q1.diggle, main = \"bw.diggle\")\n\n\n\n\n\n\n\n\n\ndensity(Chin_vac_ppp_list$`2024-Q2` %&gt;% rescale.ppp(s=100000, unitname=\"km\"),\n        sigma=bw.diggle,\n        edge=TRUE,\n        kernel=\"gaussian\") %&gt;% plot()\n\n\n\n\n\n\n\n\n\nbw.CvL(Chin_vac_ppp_list$`2024-Q2` %&gt;% rescale.ppp(s=100000, unitname=\"100km\"))\n\n    sigma \n0.4419175 \n\nbw.scott(Chin_vac_ppp_list$`2024-Q2` %&gt;% rescale.ppp(s=100000, unitname=\"100km\"))\n\n  sigma.x   sigma.y \n0.3335642 0.8146054 \n\nbw.ppl(Chin_vac_ppp_list$`2024-Q2` %&gt;% rescale.ppp(s=100000, unitname=\"100km\"))\n\n     sigma \n0.07448888 \n\nbw.diggle(Chin_vac_ppp_list$`2024-Q2` %&gt;% rescale.ppp(s=100000, unitname=\"100km\"))\n\n     sigma \n0.05851526 \n\n\n\n\n\n4.2.4 Separating Study Areas\n\n# # Function to filter sf object by admin1 values \n# filter_by_admin1 &lt;- function(sf_object, regions) {\n#   filtered_list &lt;- lapply(regions, function(region)) {\n#     sf_object %&gt;% filter(admin1 == region)}}\n# \n#   # Name the list elements by their region names\n#   names(filtered_list) &lt;- regions\n# \n#   return(filtered_list)\n#  \n#   \n# # Define the regions to filter by \n# regions &lt;- c(\"Chin\", \"Magway\", \"Rakhine\")\n#   \n# # Each list is a list of lists of sfs for each region \n# vac_list &lt;- filter_by_admin1(studyarea_vac, regions)\n# stdp_list &lt;- filter_by_admin1(studyarea_stdp, regions)\n# erv_list &lt;- filter_by_admin1(studyarea_erv, regions)\n# bat_list &lt;- filter_by_admin1(studyarea_bat, regions)\n# \n# Chin_vac &lt;- vac_list[[\"Chin\"]]\n# Chin_stdp &lt;- stdp_list[[\"Chin\"]]\n# Chin_erv &lt;- erv_list[[\"Chin\"]]\n# Chin_bat &lt;- bat_vac_list[[\"Chin\"]]\n# \n# Magway_vac &lt;- vac_list[[\"Magway\"]]\n# Magway_stdp &lt;- stdp_list[[\"Magway\"]]\n# Magway_erv &lt;- erv_list[[\"Magway\"]]\n# Magway_bat &lt;- bat_vac_list[[\"Magway\"]]\n# \n# Rakhine_vac &lt;- vac_list[[\"Rakhine\"]]\n# Rakhine_stdp &lt;- stdp_list[[\"Rakhine\"]]\n# Rakhine_erv &lt;- erv_list[[\"Rakhine\"]]\n# Rakhine_bat &lt;- bat_vac_list[[\"Rakhine\"]]",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_01.html",
    "href": "hands-on-exercise/ex06/hands-on-ex06_01.html",
    "title": "06 (Part 1) ‘Global Measures of Spatial Autocorrelation’",
    "section": "",
    "text": "(Copied from Exercise)\nIn this hands-on exercise, you will learn how to compute Global Measures of Spatial Autocorrelation (GMSA) by using spdep package. By the end to this hands-on exercise, you will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\nprovide statistically correct interpretation of GSA statistics.\n\n\n\n(Copied from Exercise)\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 1) ‘Global Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_01.html#import-packages",
    "href": "hands-on-exercise/ex06/hands-on-ex06_01.html#import-packages",
    "title": "06 (Part 1) ‘Global Measures of Spatial Autocorrelation’",
    "section": "Import packages",
    "text": "Import packages\n\npacman::p_load(sf, spdep, tmap, tidyverse)\n\n\nImport Data\n\n# geospatial polygon feature layer\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\") # Note that crs is in WGS84, need to convert (TO BE COMPLETED)\n\nReading layer `Hunan' from data source \n  `C:\\hengkuanxin\\SMU_Geospatial_Analytics\\hands-on-exercise\\ex06\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n# statistics data\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n# do a relational join to add statistics to sf object\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\nVisualising Regional Development Indicator\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 1) ‘Global Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_01.html#spatial-weights-queen",
    "href": "hands-on-exercise/ex06/hands-on-ex06_01.html#spatial-weights-queen",
    "title": "06 (Part 1) ‘Global Measures of Spatial Autocorrelation’",
    "section": "Spatial Weights (Queen)",
    "text": "Spatial Weights (Queen)\n\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 1) ‘Global Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_01.html#standardise-weights-row",
    "href": "hands-on-exercise/ex06/hands-on-ex06_01.html#standardise-weights-row",
    "title": "06 (Part 1) ‘Global Measures of Spatial Autocorrelation’",
    "section": "Standardise Weights (Row)",
    "text": "Standardise Weights (Row)\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 1) ‘Global Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_01.html#conduct-a-monte-carlo-simulation-of-morans-i-test",
    "href": "hands-on-exercise/ex06/hands-on-ex06_01.html#conduct-a-monte-carlo-simulation-of-morans-i-test",
    "title": "06 (Part 1) ‘Global Measures of Spatial Autocorrelation’",
    "section": "Conduct a Monte-Carlo simulation of Moran’s I test",
    "text": "Conduct a Monte-Carlo simulation of Moran’s I test\n\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 1) ‘Global Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_01.html#visualising-monte-carlo-morans-i",
    "href": "hands-on-exercise/ex06/hands-on-ex06_01.html#visualising-monte-carlo-morans-i",
    "title": "06 (Part 1) ‘Global Measures of Spatial Autocorrelation’",
    "section": "Visualising Monte Carlo Moran’s I",
    "text": "Visualising Monte Carlo Moran’s I\n\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\n\n\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\")",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 1) ‘Global Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_01.html#gearys-c-test",
    "href": "hands-on-exercise/ex06/hands-on-ex06_01.html#gearys-c-test",
    "title": "06 (Part 1) ‘Global Measures of Spatial Autocorrelation’",
    "section": "Geary’s C test",
    "text": "Geary’s C test\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q   \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 1) ‘Global Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_01.html#computing-monte-carlo-gearys-c",
    "href": "hands-on-exercise/ex06/hands-on-ex06_01.html#computing-monte-carlo-gearys-c",
    "title": "06 (Part 1) ‘Global Measures of Spatial Autocorrelation’",
    "section": "Computing Monte Carlo Geary’s C",
    "text": "Computing Monte Carlo Geary’s C\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 1) ‘Global Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_01.html#visualising-the-monte-carlo-gearys-c",
    "href": "hands-on-exercise/ex06/hands-on-ex06_01.html#visualising-the-monte-carlo-gearys-c",
    "title": "06 (Part 1) ‘Global Measures of Spatial Autocorrelation’",
    "section": "Visualising the Monte Carlo Geary’s C",
    "text": "Visualising the Monte Carlo Geary’s C\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\n\n\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Geary c\")\n\nabline(v=1, col=\"red\")",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 1) ‘Global Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_01.html#compute-morans-i-correlogram",
    "href": "hands-on-exercise/ex06/hands-on-ex06_01.html#compute-morans-i-correlogram",
    "title": "06 (Part 1) ‘Global Measures of Spatial Autocorrelation’",
    "section": "Compute Moran’s I correlogram",
    "text": "Compute Moran’s I correlogram\n\n# Compute a 6 lag correlogram\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\n\n\n\n\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 1) ‘Global Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_01.html#compute-gearys-c-correlogram-and-plo",
    "href": "hands-on-exercise/ex06/hands-on-ex06_01.html#compute-gearys-c-correlogram-and-plo",
    "title": "06 (Part 1) ‘Global Measures of Spatial Autocorrelation’",
    "section": "Compute Geary’s C correlogram and plo",
    "text": "Compute Geary’s C correlogram and plo\n\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\n\n\n\n\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 1) ‘Global Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_02.html",
    "href": "hands-on-exercise/ex06/hands-on-ex06_02.html",
    "title": "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’",
    "section": "",
    "text": "(Copied from Exercise)\nLocal Measures of Spatial Autocorrelation (LMSA) focus on the relationships between each observation and its surroundings, rather than providing a single summary of these relationships across the map. In this sense, they are not summary statistics but scores that allow us to learn more about the spatial structure in our data. The general intuition behind the metrics however is similar to that of global ones. Some of them are even mathematically connected, where the global version can be decomposed into a collection of local ones. One such example are Local Indicators of Spatial Association (LISA). Beside LISA, Getis-Ord’s Gi-statistics will be introduce as an alternative LMSA statistics that present complementary information or allow us to obtain similar insights for geographically referenced data.\nIn this hands-on exercise, you will learn how to compute Local Measures of Spatial Autocorrelation (LMSA) by using spdep package. By the end to this hands-on exercise, you will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Local Indicator of Spatial Association (LISA) statistics for detecting clusters and outliers by using appropriate functions spdep package;\ncompute Getis-Ord’s Gi-statistics for detecting hot spot or/and cold spot area by using appropriate functions of spdep package; and\nto visualise the analysis output by using tmap package.\n\n\n\n(Copied from Exercise)\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_02.html#import-packages",
    "href": "hands-on-exercise/ex06/hands-on-ex06_02.html#import-packages",
    "title": "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’",
    "section": "Import packages",
    "text": "Import packages\n\npacman::p_load(sf, spdep, tmap, tidyverse)\n\n\nImport Data\n\n# geospatial polygon feature layer\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\") # Note that crs is in WGS84, need to convert (TO BE COMPLETED)\n\nReading layer `Hunan' from data source \n  `C:\\hengkuanxin\\SMU_Geospatial_Analytics\\hands-on-exercise\\ex06\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n# statistics data\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n# do a relational join to add statistics to sf object\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\nVisualising Regional Development Indicator\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_02.html#computing-spatial-weights-queens",
    "href": "hands-on-exercise/ex06/hands-on-ex06_02.html#computing-spatial-weights-queens",
    "title": "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’",
    "section": "Computing Spatial Weights (Queens’)",
    "text": "Computing Spatial Weights (Queens’)\n\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_02.html#standardise-weights-by-row",
    "href": "hands-on-exercise/ex06/hands-on-ex06_02.html#standardise-weights-by-row",
    "title": "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’",
    "section": "Standardise Weights (by row)",
    "text": "Standardise Weights (by row)\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_02.html#computing-local-morans-i",
    "href": "hands-on-exercise/ex06/hands-on-ex06_02.html#computing-local-morans-i",
    "title": "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’",
    "section": "Computing local Moran’s I",
    "text": "Computing local Moran’s I\n\nfips &lt;- order(hunan$County)\nlocalMI &lt;- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan$County[fips]),\n  check.names=FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_02.html#mapping-the-local-morans-i",
    "href": "hands-on-exercise/ex06/hands-on-ex06_02.html#mapping-the-local-morans-i",
    "title": "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’",
    "section": "Mapping the local Moran’s I",
    "text": "Mapping the local Moran’s I\n\nhunan.localMI &lt;- cbind(hunan,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\",\n          midpoint = NA) +\n  tm_borders(alpha = 0.5)",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_02.html#mapping-local-morans-i-p-values",
    "href": "hands-on-exercise/ex06/hands-on-ex06_02.html#mapping-local-morans-i-p-values",
    "title": "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’",
    "section": "Mapping local Moran’s I p-values",
    "text": "Mapping local Moran’s I p-values\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_02.html#mapping-both-local-morans-i-values-and-p-values",
    "href": "hands-on-exercise/ex06/hands-on-ex06_02.html#mapping-both-local-morans-i-values-and-p-values",
    "title": "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’",
    "section": "Mapping both local Moran’s I values and p-values",
    "text": "Mapping both local Moran’s I values and p-values\n\nlocalMI.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_02.html#scatter-plot",
    "href": "hands-on-exercise/ex06/hands-on-ex06_02.html#scatter-plot",
    "title": "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’",
    "section": "Scatter Plot",
    "text": "Scatter Plot\n\nnci &lt;- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_02.html#plotting-moran-scatterplot-with-standardised-variable",
    "href": "hands-on-exercise/ex06/hands-on-ex06_02.html#plotting-moran-scatterplot-with-standardised-variable",
    "title": "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’",
    "section": "Plotting Moran scatterplot with standardised variable",
    "text": "Plotting Moran scatterplot with standardised variable\n\nhunan$Z.GDPPC &lt;- scale(hunan$GDPPC) %&gt;% \n  as.vector \n\nnci2 &lt;- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\", \n                   ylab=\"Spatially Lag z-GDPPC 2012\")",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_02.html#preparing-lisa-map-classes",
    "href": "hands-on-exercise/ex06/hands-on-ex06_02.html#preparing-lisa-map-classes",
    "title": "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’",
    "section": "Preparing LISA map classes",
    "text": "Preparing LISA map classes\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\n\n# Next, derives the spatially lagged variable of interest (i.e. GDPPC) and centers the spatially lagged variable around its mean.\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC) \n\n# This is follow by centering the local Moran’s around the mean.\nLM_I &lt;- localMI[,1] - mean(localMI[,1])\n\n# Next, we will set a statistical significance level for the local Moran.\nsignif &lt;- 0.05\n\n# These four command lines define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4      \n\n# Lastly, places non-significant Moran in the category 0.\nquadrant[localMI[,5]&gt;signif] &lt;- 0",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_02.html#plotting-lisa-map",
    "href": "hands-on-exercise/ex06/hands-on-ex06_02.html#plotting-lisa-map",
    "title": "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’",
    "section": "Plotting LISA map",
    "text": "Plotting LISA map\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_02.html#plotting-together-with-the-p-value-plot",
    "href": "hands-on-exercise/ex06/hands-on-ex06_02.html#plotting-together-with-the-p-value-plot",
    "title": "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’",
    "section": "Plotting together with the p-value plot",
    "text": "Plotting together with the p-value plot\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, \n             asp=1, ncol=2)",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_02.html#deriving-distance-based-weight-matrix",
    "href": "hands-on-exercise/ex06/hands-on-ex06_02.html#deriving-distance-based-weight-matrix",
    "title": "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’",
    "section": "Deriving distance-based weight matrix",
    "text": "Deriving distance-based weight matrix\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\ncoords &lt;- cbind(longitude, latitude)\n\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\n\nComputing Fixed Distance Matrix\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\n\nwm62_lw &lt;- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\n\n\nComputing adaptive distance weight matrix\n\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\n\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_02.html#gi-statistics-using-fixed-distance",
    "href": "hands-on-exercise/ex06/hands-on-ex06_02.html#gi-statistics-using-fixed-distance",
    "title": "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’",
    "section": "Gi statistics using fixed distance",
    "text": "Gi statistics using fixed distance\n\nfips &lt;- order(hunan$County)\ngi.fixed &lt;- localG(hunan$GDPPC, wm62_lw)\n# gi.fixed\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_02.html#mapping-gi-values-with-fixed-distance-weights",
    "href": "hands-on-exercise/ex06/hands-on-ex06_02.html#mapping-gi-values-with-fixed-distance-weights",
    "title": "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’",
    "section": "Mapping Gi values with fixed distance weights",
    "text": "Mapping Gi values with fixed distance weights\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nGimap &lt;-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp=1, ncol=2)",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_02.html#gi-statistics-using-adaptive-distance",
    "href": "hands-on-exercise/ex06/hands-on-ex06_02.html#gi-statistics-using-adaptive-distance",
    "title": "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’",
    "section": "Gi statistics using adaptive distance",
    "text": "Gi statistics using adaptive distance\n\nfips &lt;- order(hunan$County)\ngi.adaptive &lt;- localG(hunan$GDPPC, knn_lw)\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "hands-on-exercise/ex06/hands-on-ex06_02.html#mapping-gi-values-with-adaptive-distance-weights",
    "href": "hands-on-exercise/ex06/hands-on-ex06_02.html#mapping-gi-values-with-adaptive-distance-weights",
    "title": "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’",
    "section": "Mapping Gi values with adaptive distance weights",
    "text": "Mapping Gi values with adaptive distance weights\n\ngdppc&lt;- qtm(hunan, \"GDPPC\")\n\nGimap &lt;- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, \n             Gimap, \n             asp=1, \n             ncol=2)",
    "crumbs": [
      "Home",
      "Hands-on Exercises",
      "06 (Part 2) ‘Local Measures of Spatial Autocorrelation’"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#visualise-the-myanmar-states-boundaries",
    "href": "take-home-exercise/ex01/take-home-ex01.html#visualise-the-myanmar-states-boundaries",
    "title": "01 Take Home Exercise 1",
    "section": "4.1 Visualise the Myanmar States Boundaries",
    "text": "4.1 Visualise the Myanmar States Boundaries\n\ntm_shape(admin_1) +\n  tm_style(\"white\") +\n  tm_polygons(\"ST\", palette = \"Set3\", title = \"States/Regions\") +\n  tm_shape(st_centroid(admin_1)) + # Find the centroids\n  tm_text(\"ST\", size = 0.6, col = \"black\", shadow = TRUE, just=\"center\") + # Add labels at centroids\n  tm_layout(title = \"Administrative Boundaries Level 1\",\n            legend.outside = TRUE)",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#visualise-main-event-acled-data",
    "href": "take-home-exercise/ex01/take-home-ex01.html#visualise-main-event-acled-data",
    "title": "01 Take Home Exercise 1",
    "section": "4.2 Visualise Main Event ACLED data",
    "text": "4.2 Visualise Main Event ACLED data\nNow that we have categorised the ACLED points data into specific yearly quarters, let us plot them onto the map of myanmar to see if there are any obvious patterns.\nSo far, we can have up to 14 temporal categories, with 4 quarters per year in Year 2021, 2022, 2023, and 2 quarters in the first half of 2024.\n\ntmap_mode(\"plot\")\ntmap_style(\"classic\")\n\ntm_shape(admin_1) +\n  tm_polygons() +\n  tm_shape(violence_against_civilians) +\n  tm_dots(size= 0.05, col = \"black\", alpha = 0.5) +\n  tm_facets(by = \"year_quarter\",\n              free.coords = FALSE,\n              drop.units = TRUE) +\n  tm_layout(main.title=\"Violence Against Civilians (Quarterly)\")\n\ntm_shape(admin_1) +\n  tm_polygons() +\n  tm_shape(strategic_developments) +\n  tm_dots(size= 0.05, col = \"black\", alpha = 0.5) +\n  tm_facets(by = \"year_quarter\",\n              free.coords = FALSE,\n              drop.units = TRUE) +\n  tm_layout(main.title=\"Strategic Developments (Quarterly)\")\n\ntm_shape(admin_1) +\n  tm_polygons() +\n  tm_shape(explosion_or_remoteviolence) +\n  tm_dots(size= 0.05, col = \"black\", alpha = 0.5) +\n  tm_facets(by = \"year_quarter\",\n              free.coords = FALSE,\n              drop.units = TRUE) +\n  tm_layout(main.title=\"Explosion/Remote Violence (Quarterly)\")\n\ntm_shape(admin_1) +\n  tm_polygons() +\n  tm_shape(battles) +\n  tm_dots(size= 0.05, col = \"black\", alpha = 0.5) +\n  tm_facets(by = \"year_quarter\",\n              free.coords = FALSE,\n              drop.units = TRUE) +\n  tm_layout(main.title=\"Battles (Quarterly)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nmap_quarterly_ViolAgstCiv &lt;- tm_shape(admin_1) +\n  tm_style(\"white\") +\n  tm_polygons() +\n  tm_shape(violence_against_civilians) +\n  tm_dots(size= 0.1) + # make sizes a bit bigger so that it can be seen\n  tm_facets(along = \"year_quarter\", # 1 separate map for each year quarter\n              free.coords = FALSE,\n              drop.units = TRUE)\n\nmap_quarterly_StratDev &lt;- tm_shape(admin_1) +\n  tm_style(\"white\") +\n  tm_polygons() +\n  tm_shape(strategic_developments) +\n  tm_dots(size= 0.1) + # make sizes a bit bigger so that it can be seen\n  tm_facets(along = \"year_quarter\", # 1 separate map for each year quarter\n              free.coords = FALSE,\n              drop.units = TRUE)\n\nmap_quarterly_ExploOrRemoViol &lt;- tm_shape(admin_1) +\n  tm_style(\"white\") +\n  tm_polygons() +\n  tm_shape(explosion_or_remoteviolence) +\n  tm_dots(size= 0.1) + # make sizes a bit bigger so that it can be seen\n  tm_facets(along = \"year_quarter\", # 1 separate map for each year quarter\n              free.coords = FALSE,\n              drop.units = TRUE)\n\nmap_quarterly_Battles &lt;- tm_shape(admin_1) +\n  tm_style(\"white\") +\n  tm_polygons() +\n  tm_shape(battles) +\n  tm_dots(size= 0.1) + # make sizes a bit bigger so that it can be seen\n  tm_facets(along = \"year_quarter\", # 1 separate map for each year quarter\n              free.coords = FALSE,\n              drop.units = TRUE)\n\ntmap_animation(map_quarterly_ViolAgstCiv,\n               filename=\"images/quarterly_violenceAgainstCivilians.gif\", delay=100)\n\ntmap_animation(map_quarterly_StratDev,\n               filename=\"images/quarterly_strategicDevelopments.gif\", delay=100)\n\ntmap_animation(map_quarterly_ExploOrRemoViol,\n               filename=\"images/quarterly_explosionOrRemoteViolence.gif\", delay=100)\n\ntmap_animation(map_quarterly_Battles,\n               filename=\"images/quarterly_battles.gif\", delay=100)\n\n\n\n\nQuarterly events: Violence Against Civilians\n\n\n\n\n\nQuarterly events: Strategic Developments\n\n\n\n\n\nQuarterly events: Explosion / Remote Violence\n\n\n\n\n\nQuarterly events: Battles\n\n\n\n4.2.1 Conclusion: Filter Again\nLooking at the data, while we are able to see some trends along the 4 data sets, they are roughly the same. So let us filter our data sets and focus on sub-event types.\nReferring to ACLED’s codebook again, we see that event types are further categorised as sub-event types. For the purpose of this analysis, I have chosen the following event types from each data set:\n\nAbduction/forced disappearance from Violence Against Civilians (901 data points)\nChange to group/activity from Strategic Developments (1230 data points)\nShelling/artillery/missile attack from Explosion/Remote Violence (3650 data points)\nNon-state actor overtakes territory from Battles (274 data points) (can also be further broken down into different types of takeover, but I have chosen to ignore it this time)\n\nIf you are interested, the description of the event types are as follows (copied from the ACLED codebook):\n\nAbduction/forced disappearance: This sub-event type is used when an actor engages in the abduction or forced disappearance of civilians, without reports of further violence. If fatalities or serious injuries are reported during the abduction or forced disappearance, the event is recorded as an ‘Attack’ event instead. If such violence is reported in later periods during captivity, this is recorded as an additional ‘Attack’ event. Note that multiple people can be abducted in a single ‘Abduction/forced disappearance’ event. Arrests by non-state groups and extrajudicial detentions by state forces are considered ‘Abduction/forced disappearance’. Arrests conducted by state forces within the standard judicial process are, however, considered ‘Arrests’.\nChange to group/activity: This sub-event type is used to record significant changes in the activity or structure of armed groups. It can cover anything from the creation of a new rebel group or a paramilitary wing of the security forces, ‘voluntary’ recruitment drives, movement of forces, or any other non-violent security measures enacted by armed actors. This sub-event type can also be used if one armed group is absorbed into a different armed group or to track large-scale defections.\nShelling/artillery/missile attack: This sub-event type captures the use of long-range artillery, missile systems, or other heavy weapons platforms in the absence of any other engagement. When two armed groups exchange long-range fire, it is recorded as an ‘Armed clash’. ‘Shelling/artillery/missile attack’ events include attacks described as shelling, the use of artillery and cannons, mortars, guided missiles, rockets, grenade launchers, and other heavy weapons platforms. Crewed aircraft shot down by long-range systems fall under this sub-event type. Uncrewed armed drones that are shot down, however, are recorded as interceptions under ‘Disrupted weapons use’ because people are not targeted.\nNon-state actor overtakes territory: This sub-event type is used when a non-state actor (excluding those operating directly on behalf of the government) or a foreign state actor, through armed interaction, captures territory from an opposing government or non-state actor; as a result, they are regarded as having a monopoly of force within that territory. Short-lived and/or small-scale territorial exchanges that do not last for more than one day are recorded as ‘Armed clash’ events. In cases where non- state forces fight with opposing actors in a location many times before gaining control, only the final territorial acquisition is recorded as ‘Non-state actor overtakes territory’. All other battles in that location are recorded as ‘Armed clash’.\n\nSome Extra Observations\nIt would appear that in the main event type Explosion / Remote Violence, there are also Sub-event categories such as Suicide bombs and Chemical weapons, but their occurrences are not high. There are 2 recorded cases of suicide bombs and 1 case of chemical weapons in the study period in Myanmar.\nThere are also other instances of these strange events available in the data sets.\nA reason I have not chosen these seemingly interesting events are that they are too rare and do not provide sufficient amount of data for our analysis. Though, of course, the fact that they are rare is a good sign, from the perspective of a layman.\nReasons\nI have chosen to focus on the mentioned 4 sub-event types simply because they sound interesting, and they provide enough data points for our analysis later on, which includes first and second order point pattern analysis, as well as spatio-temporal point pattern analysis.\nPreviously, I have attempted to analyse all event types by selecting a study area, but I realise I was not able to draw out much conclusion. Refer to the section “Previous Attempts on This Exercise” for a glance of my previous attempts.\n\n\n4.2.2 Extracting Sub-event Types from Each Main Event Type Data Set\nLet us now extract the data points that we want from each data set and visualise the data points over time.\n\nabduction_sf &lt;- violence_against_civilians %&gt;% \n  filter(sub_event_type == \"Abduction/forced disappearance\")\n\n# changeGroup_sf &lt;- strategic_developments %&gt;% \n#   filter(sub_event_type == \"Change to group/activity\")\n# \n# shelling_sf &lt;- explosion_or_remoteviolence %&gt;%\n#   filter(sub_event_type == \"Shelling/artillery/missile attack\")\n# \n# overtake_sf &lt;- battles %&gt;%\n#   filter(sub_event_type == \"Non-state actor overtakes territory\")",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#visualising-the-sub-event-acled-data-set",
    "href": "take-home-exercise/ex01/take-home-ex01.html#visualising-the-sub-event-acled-data-set",
    "title": "01 Take Home Exercise 1",
    "section": "4.3 Visualising the Sub-Event ACLED Data Set",
    "text": "4.3 Visualising the Sub-Event ACLED Data Set\n\ntmap_mode(\"plot\")\ntmap_style(\"classic\")\n\ntm_shape(admin_1) +\n  tm_polygons() +\n  tm_shape(abduction_sf) +\n  tm_dots(size= 0.1, col = \"black\", alpha = 0.5) +\n  tm_facets(by = \"year_quarter\",\n              free.coords = FALSE,\n              drop.units = TRUE) +\n  tm_layout(main.title=\"Abduction/forced disappearance (Quarterly)\")\n\ntm_shape(admin_1) +\n  tm_polygons() +\n  tm_shape(changeGroup_sf) +\n  tm_dots(size= 0.1, col = \"black\", alpha = 0.5) +\n  tm_facets(by = \"year_quarter\",\n              free.coords = FALSE,\n              drop.units = TRUE) +\n  tm_layout(main.title=\"Change to group/activity (Quarterly)\")\n\ntm_shape(admin_1) +\n  tm_polygons() +\n  tm_shape(shelling_sf) +\n  tm_dots(size= 0.1, col = \"black\", alpha = 0.5) +\n  tm_facets(by = \"year_quarter\",\n              free.coords = FALSE,\n              drop.units = TRUE) +\n  tm_layout(main.title=\"Shelling/artillery/missile attack (Quarterly)\")\n\ntm_shape(admin_1) +\n  tm_polygons() +\n  tm_shape(overtake_sf) +\n  tm_dots(size= 0.1, col = \"black\", alpha = 0.5) +\n  tm_facets(by = \"year_quarter\",\n              free.coords = FALSE,\n              drop.units = TRUE) +\n  tm_layout(main.title=\"Non-state actor overtakes territory (Quarterly)\")\n\n\n\n\n\n\n4.3.1 Conclusion: Data set seems ready.\nOverall, it seems like there are more obvious trends and the data is easier to work with now. Some things we can see at a glance is that.\n\nThere seems to be quite a number of abduction cases in the central part of Myanmar (in states Mandalay and Sagaing). These cases seem to be happening along the border of the states, but more remains to be seen.\nChanges to group / activity seem to also be happening in the same central areas, no big trends so far.\nShelling seems to be occuring everywhere, not much pattern to see.\nNon-state actors overtaking seems to be occuring in northen Rakhine in the latter parts of the data set periods (from 2023 Q4 to 2024 Q2). If we were to compare this with the other event types, we can also see similar trend. In fact, at a glance, we can see that these shellings and artillery strikes seem to be happening at the same period of these territory takeover events in the northern Rakhine state.\n\nSince all the events seem to be happening in the central/upper parts of Myanmar, let us select those states as our study area. The study area includes: “Magway”, “Chin”, “Sagaing”.\n\n\n\n\n\n\nNote\n\n\n\nI do not have enough time for the analysis, so I will only focus on the abduction cases in these 3 states.\n\n\n\n\n4.3.2 Final Extract of Data Sets\n\nstudyarea_sf &lt;- admin_1 %&gt;% \n  filter(ST %in% c(\"Magway\", \"Chin\", \"Sagaing\"))\n\nabduction_sf &lt;- abduction_sf %&gt;% \n  filter(admin1 %in% c(\"Magway\", \"Chin\", \"Sagaing\"))\n\n# changeGroup_sf &lt;- changeGroup_sf %&gt;% \n#   filter(admin1 %in% c(\"Magway\", \"Chin\", \"Sagaing\"))\n# \n# shelling_sf &lt;- shelling_sf %&gt;% \n#   filter(admin1 %in% c(\"Magway\", \"Chin\", \"Sagaing\"))\n# \n# overtake_sf &lt;- overtake_sf %&gt;%\n#   filter(admin1 %in% c(\"Magway\", \"Chin\", \"Sagaing\"))",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#export-data-sets-rds-file-formats",
    "href": "take-home-exercise/ex01/take-home-ex01.html#export-data-sets-rds-file-formats",
    "title": "01 Take Home Exercise 1",
    "section": "4.4 Export Data Sets (RDS file formats)",
    "text": "4.4 Export Data Sets (RDS file formats)\n\nsaveRDS(abduction_sf, \"data/final_rds/abduction.rds\")\n# saveRDS(changeGroup_sf, \"data/final_rds/changeGroup.rds\")\n# saveRDS(shelling_sf, \"data/final_rds/shelling.rds\")\n# saveRDS(overtake_sf, \"data/final_rds/overtake.rds\")\nsaveRDS(studyarea_sf, \"data/final_rds/studyarea.rds\")",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#import-data-sets-rds-file-format-1",
    "href": "take-home-exercise/ex01/take-home-ex01.html#import-data-sets-rds-file-format-1",
    "title": "01 Take Home Exercise 1",
    "section": "5.1 Import Data Sets (RDS file format)",
    "text": "5.1 Import Data Sets (RDS file format)\nNote that currently, we are left with:\n\nabduction_sf: 326 data points\nchangeGroup_sf: 729 data points\novertake_sf: 179 data points\nshelling_sf: 1977 data points\n\n\nabduction_sf &lt;- readRDS(\"data/final_rds/abduction.rds\")\nstudyArea_sf &lt;- readRDS(\"data/final_rds/studyarea.rds\")",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#create-owin-objects-for-study-area",
    "href": "take-home-exercise/ex01/take-home-ex01.html#create-owin-objects-for-study-area",
    "title": "01 Take Home Exercise 1",
    "section": "5.2 Create Owin Objects for Study Area",
    "text": "5.2 Create Owin Objects for Study Area\nLet us first set our study area to the whole of Myanmar, simply because our number of data points of most of the sub-event types are not very large (except the Shelling/artillery/missile attack event)\n\nstudyArea_owin &lt;- studyArea_sf %&gt;% as.owin()\n\n\n5.2.1 Converting SF into PPP objects\nDue to time constraints on my side, let us compute the Kernel Density Estimate (KDE) on each event type throughout the whole event period. To do this, we will first need to convert our data points into PPP objects.\n\nabduction_ppp &lt;- abduction_sf %&gt;% as.ppp() %&gt;% \n               rjitter(retry = TRUE, nsim = 1, drop = TRUE)\n\n\nabduction_ppp_study &lt;- abduction_ppp[studyArea_owin]",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#choosing-adaptive-bandwidth-method",
    "href": "take-home-exercise/ex01/take-home-ex01.html#choosing-adaptive-bandwidth-method",
    "title": "01 Take Home Exercise 1",
    "section": "5.3 Choosing Adaptive Bandwidth Method",
    "text": "5.3 Choosing Adaptive Bandwidth Method\n\nbw.CvL(abduction_ppp_study\n       %&gt;% rescale.ppp(s=100000, unitname=\"100km\"))\nbw.scott(abduction_ppp_study\n       %&gt;% rescale.ppp(s=100000, unitname=\"100km\"))\nbw.ppl(abduction_ppp_study\n       %&gt;% rescale.ppp(s=100000, unitname=\"100km\"))\nbw.diggle(abduction_ppp_study\n       %&gt;% rescale.ppp(s=100000, unitname=\"100km\"))\n\nSeems like the bandwidth varies quite a bit. Let us visualise the density estimates and choose based on the visual results.\n\nabduction_ppp_100km &lt;- abduction_ppp_study %&gt;% rescale.ppp(s=100000, unitname=\"100km\")\n\n\nabduction_ppp_100km.CvL &lt;- density(abduction_ppp_100km,\n                                sigma=bw.CvL,\n                                edge=TRUE,\n                                kernel=\"gaussian\")\n\nabduction_ppp_100km.scott &lt;- density(abduction_ppp_100km,\n                                sigma=bw.scott,\n                                edge=TRUE,\n                                kernel=\"gaussian\")\n\nabduction_ppp_100km.ppl &lt;- density(abduction_ppp_100km,\n                                sigma=bw.ppl,\n                                edge=TRUE,\n                                kernel=\"gaussian\")\n\nabduction_ppp_100km.diggle &lt;- density(abduction_ppp_100km,\n                                sigma=bw.diggle,\n                                edge=TRUE,\n                                kernel=\"gaussian\")\n\npar(mfrow=c(2,2))\nplot(abduction_ppp_100km.CvL, main = \"bw.CvL\")\nplot(abduction_ppp_100km.scott, main = \"bw.scott\")\nplot(abduction_ppp_100km.ppl, main = \"bw.ppl\")\nplot(abduction_ppp_100km.diggle, main = \"bw.diggle\")\npar(mfrow=c(1,1))\n\n\nBased on the results, the method: bw.diggle is too detailed and not useful for our analysis. We should therefore try either bw.ppl for a more detailed visualisation, or bw.scott for a more balanced visualisation. In fact, let us try splitting our study area into the 3 states and see if there is a difference.\n\n5.3.1 Separate the Study Area\n\nchin_owin &lt;- studyArea_sf %&gt;% filter(ST==\"Chin\") %&gt;% as.owin()\nmagway_owin &lt;- studyArea_sf %&gt;% filter(ST==\"Magway\") %&gt;% as.owin()\nsagaing_owin &lt;- studyArea_sf %&gt;% filter(ST==\"Sagaing\") %&gt;% as.owin()\n\n\nabduction_ppp_chin.100km &lt;- abduction_ppp[chin_owin] %&gt;% \n  rescale.ppp(s=100000, unitname=\"100km\")\n\nabduction_ppp_magway.100km &lt;- abduction_ppp[magway_owin] %&gt;% \n  rescale.ppp(s=100000, unitname=\"100km\")\n\nabduction_ppp_sagaing.100km &lt;- abduction_ppp[sagaing_owin] %&gt;% \n  rescale.ppp(s=100000, unitname=\"100km\")\n\n\nabduction_ppp_chin.100km.CvL &lt;- density(abduction_ppp_chin.100km,\n                                sigma=bw.CvL,\n                                edge=TRUE,\n                                kernel=\"gaussian\")\n\nabduction_ppp_chin.100km.scott &lt;- density(abduction_ppp_chin.100km,\n                                sigma=bw.scott,\n                                edge=TRUE,\n                                kernel=\"gaussian\")\n\nabduction_ppp_chin.100km.ppl &lt;- density(abduction_ppp_chin.100km,\n                                sigma=bw.ppl,\n                                edge=TRUE,\n                                kernel=\"gaussian\")\n\nabduction_ppp_chin.100km.diggle &lt;- density(abduction_ppp_chin.100km,\n                                sigma=bw.diggle,\n                                edge=TRUE,\n                                kernel=\"gaussian\")\n\npar(mfrow=c(2,2))\nplot(abduction_ppp_chin.100km.CvL, main = \"bw.CvL\")\nplot(abduction_ppp_chin.100km.scott, main = \"bw.scott\")\nplot(abduction_ppp_chin.100km.ppl, main = \"bw.ppl\")\nplot(abduction_ppp_chin.100km.diggle, main = \"bw.diggle\")\npar(mfrow=c(1,1))\n\n\n\nabduction_ppp_magway.100km.CvL &lt;- density(abduction_ppp_magway.100km,\n                                sigma=bw.CvL,\n                                edge=TRUE,\n                                kernel=\"gaussian\")\n\nabduction_ppp_magway.100km.scott &lt;- density(abduction_ppp_magway.100km,\n                                sigma=bw.scott,\n                                edge=TRUE,\n                                kernel=\"gaussian\")\n\nabduction_ppp_magway.100km.ppl &lt;- density(abduction_ppp_magway.100km,\n                                sigma=bw.ppl,\n                                edge=TRUE,\n                                kernel=\"gaussian\")\n\nabduction_ppp_magway.100km.diggle &lt;- density(abduction_ppp_magway.100km,\n                                sigma=bw.diggle,\n                                edge=TRUE,\n                                kernel=\"gaussian\")\n\npar(mfrow=c(2,2))\nplot(abduction_ppp_magway.100km.CvL, main = \"bw.CvL\")\nplot(abduction_ppp_magway.100km.scott, main = \"bw.scott\")\nplot(abduction_ppp_magway.100km.ppl, main = \"bw.ppl\")\nplot(abduction_ppp_magway.100km.diggle, main = \"bw.diggle\")\npar(mfrow=c(1,1))\n\n\n\nabduction_ppp_sagaing.100km.CvL &lt;- density(abduction_ppp_sagaing.100km,\n                                sigma=bw.CvL,\n                                edge=TRUE,\n                                kernel=\"gaussian\")\n\nabduction_ppp_sagaing.100km.scott &lt;- density(abduction_ppp_sagaing.100km,\n                                sigma=bw.scott,\n                                edge=TRUE,\n                                kernel=\"gaussian\")\n\nabduction_ppp_sagaing.100km.ppl &lt;- density(abduction_ppp_sagaing.100km,\n                                sigma=bw.ppl,\n                                edge=TRUE,\n                                kernel=\"gaussian\")\n\nabduction_ppp_sagaing.100km.diggle &lt;- density(abduction_ppp_sagaing.100km,\n                                sigma=bw.diggle,\n                                edge=TRUE,\n                                kernel=\"gaussian\")\n\npar(mfrow=c(2,2))\nplot(abduction_ppp_sagaing.100km.CvL, main = \"bw.CvL\")\nplot(abduction_ppp_sagaing.100km.scott, main = \"bw.scott\")\nplot(abduction_ppp_sagaing.100km.ppl, main = \"bw.ppl\")\nplot(abduction_ppp_sagaing.100km.diggle, main = \"bw.diggle\")\npar(mfrow=c(1,1))\n\n\nChoice of Adaptive Bandwidth Method\nIn this case, bw.ppl seem like a more appropriate choice for our analysis, as it is neither overly detailed like the one given by bw.diggle, and also neither overly generic/smooth like the one given by bw.CvL.\nConclusion\nBased on the results so far, we can observe that most abduction cases seem to occur near the borders of the states. In the KDE above, we see abduction cases near the south and east of Sagaing, near the north of both Magway and Chin. Specifically, the abduction of civilians are observed along the borders between the states of Chin, Magway, and Sagaing.",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#chin-state",
    "href": "take-home-exercise/ex01/take-home-ex01.html#chin-state",
    "title": "01 Take Home Exercise 1",
    "section": "6.1 Chin State",
    "text": "6.1 Chin State\n\nL_chin = Lest(abduction_ppp_chin.100km, correction = \"Ripley\")\nplot(L_chin, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(100km)\")\n\n\n\nL_chin.csr &lt;- envelope(abduction_ppp_chin.100km, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nplot(L_chin.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#magway-state",
    "href": "take-home-exercise/ex01/take-home-ex01.html#magway-state",
    "title": "01 Take Home Exercise 1",
    "section": "6.2 Magway State",
    "text": "6.2 Magway State\n\nL_magway = Lest(abduction_ppp_magway.100km, correction = \"Ripley\")\nplot(L_magway, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(100km)\")\n\n\n\nL_magway.csr &lt;- envelope(abduction_ppp_magway.100km, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nplot(L_magway.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#sagaing-state",
    "href": "take-home-exercise/ex01/take-home-ex01.html#sagaing-state",
    "title": "01 Take Home Exercise 1",
    "section": "6.3 Sagaing State",
    "text": "6.3 Sagaing State\n\nL_sagaing = Lest(abduction_ppp_sagaing.100km, correction = \"Ripley\")\nplot(L_sagaing, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(100km)\")\n\n\n\nL_sagaing.csr &lt;- envelope(abduction_ppp_sagaing.100km, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nplot(L_sagaing.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\nThe overall results of the monte carlo tests for every study area also tells us that we are confident that the spatial points of the abduction events are clustered, and are not randomly distributed. This is seen from the black line (our test cases) exceeding the grey zone (confidence envelope).",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#selecting-year-quarters-of-events",
    "href": "take-home-exercise/ex01/take-home-ex01.html#selecting-year-quarters-of-events",
    "title": "01 Take Home Exercise 1",
    "section": "7.1 Selecting Year-Quarters of Events",
    "text": "7.1 Selecting Year-Quarters of Events\n\n# Function to convert year and quarter to numeric sequence\nconvert_year_quarter &lt;- function(year, quarter) {\n  # Calculate the numeric sequence\n  numeric_sequence &lt;- (year - 2021) * 4 + quarter\n  return(numeric_sequence)\n}\n\nabduction_sf$numeric_quarter &lt;- mapply(convert_year_quarter, abduction_sf$year, abduction_sf$quarter)\n\n\nabduction_yq &lt;- abduction_sf %&gt;% dplyr::select(`numeric_quarter`)\n\n\nabduction_yq.ppp &lt;- abduction_yq %&gt;% \n  as.ppp() %&gt;% \n  rjitter(retry = TRUE, nsim = 1, drop = TRUE)\nsummary(abduction_yq.ppp)\n\nMarked planar point pattern:  326 points\nAverage intensity 1.221499e-09 points per square unit\n\nCoordinates are given to 14 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   7.000   8.000   8.755  11.000  14.000 \n\nWindow: rectangle = [-141557.29, 241746.37] x [2108596.6, 2804872.7] units\n                    (383300 x 696300 units)\nWindow area = 2.66885e+11 square units",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#including-owin-objects",
    "href": "take-home-exercise/ex01/take-home-ex01.html#including-owin-objects",
    "title": "01 Take Home Exercise 1",
    "section": "7.2 Including Owin Objects",
    "text": "7.2 Including Owin Objects\n\nabduction_yq.ppp_chin &lt;- abduction_yq.ppp[chin_owin]\nabduction_yq.ppp_magway &lt;- abduction_yq.ppp[magway_owin] \nabduction_yq.ppp_sagaing &lt;- abduction_yq.ppp[sagaing_owin]",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#plotting-out-the-ppp-objects",
    "href": "take-home-exercise/ex01/take-home-ex01.html#plotting-out-the-ppp-objects",
    "title": "01 Take Home Exercise 1",
    "section": "7.3 Plotting out the PPP Objects",
    "text": "7.3 Plotting out the PPP Objects\n\nplot(abduction_yq.ppp_chin)\n\n\n\n\n\n\n\nplot(abduction_yq.ppp_magway)\n\n\n\n\n\n\n\nplot(abduction_yq.ppp_sagaing)",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#computing-stkde",
    "href": "take-home-exercise/ex01/take-home-ex01.html#computing-stkde",
    "title": "01 Take Home Exercise 1",
    "section": "7.4 Computing STKDE",
    "text": "7.4 Computing STKDE\n\n7.4.1 Chin State\n\nset.seed(1234)\nBOOT.spattemp(abduction_yq.ppp_chin) \n\n\nstkde_yq_chin &lt;- spattemp.density(\n  abduction_yq.ppp_chin,\n  h = 36220.355136,\n  lambda = 4.220534)\nsummary(stkde_yq_chin)\n\n\nplot(stkde_yq_chin, fix.range=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.4.2 Magway State\n\nset.seed(1234)\nBOOT.spattemp(abduction_yq.ppp_magway)\n\n\nstkde_yq_magway &lt;- spattemp.density(\n  abduction_yq.ppp_magway,\n  h = 56861.966963,\n  lambda = 1.035756)\nsummary(stkde_yq_magway)\n\n\nplot(stkde_yq_magway, fix.range=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.4.3 Saging State\n\nset.seed(1234)\nBOOT.spattemp(abduction_yq.ppp_sagaing)\n\n\nstkde_yq_sagaing &lt;- spattemp.density(\n  abduction_yq.ppp_sagaing,\n  h = 46988.081654,\n  lambda = 1.159582)\nsummary(stkde_yq_sagaing)\n\n\nplot(stkde_yq_sagaing, fix.range=TRUE)",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#convert-plots-into-rasters-layers",
    "href": "take-home-exercise/ex01/take-home-ex01.html#convert-plots-into-rasters-layers",
    "title": "01 Take Home Exercise 1",
    "section": "8.1 Convert Plots into Rasters layers",
    "text": "8.1 Convert Plots into Rasters layers\n\n# library(stars)\n# chin_KDE_sf &lt;- raster(abduction_ppp_chin.100km.ppl) %&gt;% st_as_stars()\n# class(chin_KDE_sf)\n# \n# tmap_mode(\"plot\")\n# tm_shape(chin_KDE_sf) +\n#   tm_raster(palette = \"-RdYlBu\", title = \"KDE\") +\n#   tm_basemap(\"OpenStreetMap\") +\n#   tm_layout(main.title = \"KDE on OpenStreetMap\")",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#convert-images-into-spatialgriddataframe-layers",
    "href": "take-home-exercise/ex01/take-home-ex01.html#convert-images-into-spatialgriddataframe-layers",
    "title": "01 Take Home Exercise 1",
    "section": "8.1 Convert Images into SpatialGridDataFrame Layers",
    "text": "8.1 Convert Images into SpatialGridDataFrame Layers\n\ngridded_kde_abduction_studyarea &lt;-  as(abduction_ppp.ppl,\n                                     \"SpatialGridDataFrame\")\ngridded_kde_abduction_chin &lt;-  as(abduction_ppp_chin.ppl,\n                                     \"SpatialGridDataFrame\")\ngridded_kde_abduction_magway &lt;-  as(abduction_ppp_magway.ppl,\n                                     \"SpatialGridDataFrame\")\ngridded_kde_abduction_sagaing &lt;-  as(abduction_ppp_sagaing.ppl,\n                                     \"SpatialGridDataFrame\")\nspplot(gridded_kde_abduction_studyarea)\nspplot(gridded_kde_abduction_chin)\nspplot(gridded_kde_abduction_magway)\nspplot(gridded_kde_abduction_sagaing)",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#converting-images-into-raster-layers",
    "href": "take-home-exercise/ex01/take-home-ex01.html#converting-images-into-raster-layers",
    "title": "01 Take Home Exercise 1",
    "section": "8.2 Converting Images into Raster Layers",
    "text": "8.2 Converting Images into Raster Layers\n\nraster_KDE_abduction &lt;- raster(abduction_ppp.ppl)\nraster_KDE_abduction_chin &lt;- raster(abduction_ppp_chin.ppl)\nraster_KDE_abduction_magway &lt;- raster(abduction_ppp_magway.ppl)\nraster_KDE_abduction_sagaing &lt;- raster(abduction_ppp_sagaing.ppl)\n\nprojection(raster_KDE_abduction) &lt;- CRS(\"+init=EPSG:32647\")\nprojection(raster_KDE_abduction_chin) &lt;- CRS(\"+init=EPSG:32647\")\nprojection(raster_KDE_abduction_magway) &lt;- CRS(\"+init=EPSG:32647\")\nprojection(raster_KDE_abduction_sagaing) &lt;- CRS(\"+init=EPSG:32647\")\n\n\nraster_KDE_abduction\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 2994.56, 5439.657  (x, y)\nextent     : -141557.3, 241746.4, 2108597, 2804873  (xmin, xmax, ymin, ymax)\ncrs        : +proj=utm +zone=47 +datum=WGS84 +units=m +no_defs \nsource     : memory\nnames      : layer \nvalues     : -1.940912e-24, 2.271995e-08  (min, max)",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "take-home-exercise/ex01/take-home-ex01.html#visualisation-of-kde-on-osm-using-tmap",
    "href": "take-home-exercise/ex01/take-home-ex01.html#visualisation-of-kde-on-osm-using-tmap",
    "title": "01 Take Home Exercise 1",
    "section": "8.3 Visualisation of KDE on OSM using tmap",
    "text": "8.3 Visualisation of KDE on OSM using tmap\n\ntmap_mode(\"view\")\n\n# tm_shape(studyArea_sf) +\n#   tm_polygons() +\n  tm_shape(raster_KDE_abduction_chin) + \n    tm_raster(palette = \"plasma\" , alpha = 0.8, legend.show = FALSE) +\n  tm_shape(raster_KDE_abduction_magway) + \n    tm_raster(palette = \"plasma\", alpha = 0.8, legend.show = FALSE) +\n  tm_shape(raster_KDE_abduction_sagaing) + \n    tm_raster(palette = \"plasma\", alpha = 0.8, legend.show = FALSE)",
    "crumbs": [
      "Home",
      "Take Home Exercises",
      "01 Take Home Exercise 1"
    ]
  },
  {
    "objectID": "in-class-exercise/ex06/in-class-ex06.html",
    "href": "in-class-exercise/ex06/in-class-ex06.html",
    "title": "06 In-class Exercise (Review)",
    "section": "",
    "text": "Local Statistics -&gt; Test for outliers between states and its neighbours?\nTobler’s First Law of Geography -&gt; i.e basically nearer things are more related than further things\nIn this lesson, we will not touch on Spatial Dependency, we will focus on Spatial Autocorrelation.\nStatisticians often use it in time series data – since time series data have a lot of cyclical or seasonal patterns.\nInstead of testing for serial stability, in geospatial, we will use autocorrelation for spatial stability. Whether locations are randomly distributed or is it autocorrelated to its neighbours.\nTypes of Spatial Autocorrelation Interpretation:\n– Reject null hypo, can infer that there is spatial autocorrelation when interpreting, know that spatial autocorrelation is continuous, just like your correlation coefficients. e.g. in Positive Spatial Autocorrelation, we likely see a range of values from 0 to 1 (clustering is strong when nearer to 1.)\n– When you have more negative spatial autocorrelation, we are likely to see more outliers, that is the checkboard patterns.\nBrief history: 1950s to 1960s –&gt; ‘quantitative geography revolution’ Using quantitative methods to measure geographical phenomena.\nConfidence Interval:\n\n\n\n\n\n\n\nClick on Image for Reference\n\n\nWhen we reject the null hypothesis, we can only infer that spatial points are not randomly distributed, we cannot say for sure, or determine that it is really not spatially randomly distributed.\nGetis-Ord Global G:\nd has to be distance matrix, cannot use proximity matrix.\nThe function only tells us whether there are signs of positive or negative clusters –&gt; high-high (high value with high value neighbours) clusters and low-low (low value with low value neighbours) clusters.\nMeanwhile, there are signs of outliers –&gt; high-low, and low-high, which means high value surrounded by low value neighbours and vice versa.\n\n\n\n\n\nThe function only accepts only positive variables, since we are multiplying variables together.\nLocal Indicator of Spatial Association (LISA)\n\nBreak Moran’s I into local to come up with LISA\nthe LISA for each observation gives an indication of the extent of significant spatial clustering of similar values around that observation",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "06 In-class Exercise 6 (Review)"
    ]
  },
  {
    "objectID": "in-class-exercise/ex06/in-class-ex06.html#computing-global-morans-i-optional-since-we",
    "href": "in-class-exercise/ex06/in-class-ex06.html#computing-global-morans-i-optional-since-we",
    "title": "06 In-class Exercise (Review)",
    "section": "2.1 Computing Global Moran’s I [Optional, Since we]",
    "text": "2.1 Computing Global Moran’s I [Optional, Since we]\nglobal_moran() function is used to compute the Moran’s I value.\nDifferent from spdep package, the output is a tibble data.frame.\n\nmoranI &lt;- global_moran(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\nglimpse(moranI)\n\nList of 2\n $ I: num 0.301\n $ K: num 7.64\n\n\nI refers to Moran’s I value.\nK refers to the average number of neighbours found.",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "06 In-class Exercise 6 (Review)"
    ]
  },
  {
    "objectID": "in-class-exercise/ex06/in-class-ex06.html#performing-global-morans-i-test",
    "href": "in-class-exercise/ex06/in-class-ex06.html#performing-global-morans-i-test",
    "title": "06 In-class Exercise (Review)",
    "section": "2.2 Performing Global Moran’s I Test",
    "text": "2.2 Performing Global Moran’s I Test\n\nglobal_moran_test(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\nCheck the p-value first!\nThe p-value is smaller than the alpha value if confidence level = 95%. We are therefore able to reject the null hypothesis and we can say that we have enough statistical evidence such that we are 95% confident that clustering is present.\nThen interpret the Moran I statistics\nI is positive, there are signs of clustering.",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "06 In-class Exercise 6 (Review)"
    ]
  },
  {
    "objectID": "in-class-exercise/ex06/in-class-ex06.html#performing-global-morans-i-permutation-test-repeatedly-test",
    "href": "in-class-exercise/ex06/in-class-ex06.html#performing-global-morans-i-permutation-test-repeatedly-test",
    "title": "06 In-class Exercise (Review)",
    "section": "2.3 Performing Global Moran’s I Permutation Test (Repeatedly Test)",
    "text": "2.3 Performing Global Moran’s I Permutation Test (Repeatedly Test)\nIn the real world, in fact you do not need to perform the previous tests. You can just start with the permutation test.\nWhat if the Global Moran’s I test, under randomisation,\nUsually, just a hundred iterations is enough. Note that iterations start from 0. You can also use 999 if need be.\n\nset.seed(1234)\n\nglobal_moran_perm(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\nlisa &lt;- wm_q %&gt;%\n  mutate(local_moran = local_moran(GDPPC, \n                                   nb, \n                                   wt, \n                                   nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)",
    "crumbs": [
      "Home",
      "In-class Exercises",
      "06 In-class Exercise 6 (Review)"
    ]
  }
]