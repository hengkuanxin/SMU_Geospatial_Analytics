---
title: "07 ‘Geographical Segmentation with Spatially Constrained Clustering Techniques’"
author: "Heng Kuan Xin"
date: 2024-10-12
date-modified: "last-modified"
execute:
  eval: true
  echo: true
  freeze: true
  output: true
  warning: false
  error: false

toc: true
toc-depth: 4
---

[Exercise Reference Source](https://r4gdsa.netlify.app/chap12.html)

# Overview

This exercise focuses on methods to delinate homogenous region by using geographically referenced multivariate data. There are 2 major analysis, namely:

-   hierarchical cluster analysis, and
-   spatially constrainted cluster analysis

## Learning Outcome

(Copied from Exercise) By the end of this hands-on exercise, you will able:

-   to convert GIS polygon data into R’s simple feature data.frame by using appropriate functions of **sf** package of R;

-   to convert simple feature data.frame into R’s SpatialPolygonDataFrame object by using appropriate **sf** of package of R;

-   to perform custer analysis by using *hclust()* of Base R;

-   to perform spatially constrained cluster analysis using *skater()* of Base R; and

-   to visualise the analysis output by using **ggplot2** and **tmap** package.

In geobusiness and spatial policy, it is a common practice to delineate the market or planning area into homogeneous regions by using multivariate data. In this hands-on exercise, we are interested to delineate [Shan State, Myanmar](https://en.wikipedia.org/wiki/Shan_State) into homogeneous regions by using multiple *Information and Communication technology (ICT)* *measures*, namely: **Radio**, **Television**, **Land line phone**, **Mobile phone**, **Computer**, and **Internet at home**.

## Data

Two data sets will be used in this study. They are:

-   Myanmar Township Boundary Data (i.e. *myanmar_township_boundaries*) : This is a GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features.

-   *Shan-ICT.csv*: This is an extract of [**The 2014 Myanmar Population and Housing Census Myanmar**](https://myanmar.unfpa.org/en/publications/2014-population-and-housing-census-myanmar-data-sheet) at the township level.

Both data sets are download from [Myanmar Information Management Unit (MIMU)](http://themimu.info/)

# Data Preparation

## Import Packages

The following packages will be used for the analysis:

--

```{r}
pacman::p_load(spdep, tmap, sf, ClustGeo, 
               ggpubr, cluster, factoextra, NbClust,
               heatmaply, corrplot, psych, tidyverse, GGally)
```

## Import Data

```{r}
##| eval: false
shan_sf <- st_read(dsn = "data/geospatial", 
                   layer = "myanmar_township_boundaries") %>%
  filter(ST %in% c("Shan (East)", "Shan (North)", "Shan (South)")) %>% # select Shan State only, no Shan (West)
  select(c(2:7)) # see the output screenshot below 
```

![](images/clipboard-3647189408.png){width="531"}

```{r}
##| eval: false
ict <- read_csv ("data/aspatial/Shan-ICT.csv", show_col_types = FALSE)
summary(ict)
```

## Deriving New Attributes

Let us now derive the relative attributes values by considering the number of households.

(Copied from Exercise) The unit of measurement of the values are number of household. Using these values directly will be bias by the underlying total number of households. In general, the townships with relatively higher total number of households will also have higher number of households owning radio, TV, etc. In order to overcome this problem, we will derive the penetration rate of each ICT variable by using the code chunk below.

```{r}
##| eval: false
ict_derived <- ict %>%
  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %>%
  mutate(`TV_PR` = `Television`/`Total households`*1000) %>%
  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %>%
  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %>%
  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %>%
  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %>%
  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,
         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,
         `TT_HOUSEHOLDS`=`Total households`,
         `RADIO`=`Radio`, `TV`=`Television`, 
         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,
         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) 
```

# Exploratory Data Analysis

## Using Raw Values

Let us plot the histogram and observe the distributions of the dataset. Starting with Radio.

```{r}
##| eval: false
ggplot(data=ict_derived, 
       aes(x=`RADIO`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

Using a box plot instead.

```{r}
##| eval: false
ggplot(data=ict_derived, 
       aes(x=`RADIO`)) +
  geom_boxplot(color="black", 
               fill="light blue")
```

From here, it is very obvious that 3 outliers exist.

## Using Derived Values

Now let us use the derived attributes instead (penetration rate), again starting with Radio.

```{r}
##| eval: false
ggplot(data=ict_derived, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

And the box plot.

```{r}
##| eval: false
ggplot(data=ict_derived, 
       aes(x=`RADIO_PR`)) +
  geom_boxplot(color="black", 
               fill="light blue")
```

Let us visualise the other newly derived attributes.

```{r}
##| eval: false
radio <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

tv <- ggplot(data=ict_derived, 
             aes(x= `TV_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

llphone <- ggplot(data=ict_derived, 
             aes(x= `LLPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

mphone <- ggplot(data=ict_derived, 
             aes(x= `MPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

computer <- ggplot(data=ict_derived, 
             aes(x= `COMPUTER_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

internet <- ggplot(data=ict_derived, 
             aes(x= `INTERNET_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

# ggarrange() function is useful for grouping multiple plots together. This is rather similar to the tmap arrange function for tmaps.
ggarrange(radio, tv, llphone, mphone, computer, internet, 
          ncol = 3, 
          nrow = 2)
```

## EDA using Choropleth Map

Using the unique identifier TS_PCODE, which uniquely defines each geographical region, let us join the two datasets shan_sf and ict.

We will then export it as a RDS file to save our cleaned and derived data set.

```{r}
##| eval: false
shan_sf <- left_join(shan_sf, 
                     ict_derived, by="TS_PCODE")
  
write_rds(shan_sf, "data/rds/shan_sf.rds")
```

Let us import the RDS file.

```{r}
shan_sf <- read_rds("data/rds/shan_sf.rds")
```

### Visualise Choropleth Map

```{r}
qtm(shan_sf, fill= "RADIO_PR")
```

```{r}
TT_HOUSEHOLDS.map <- tm_shape(shan_sf) + 
  tm_fill(col = "TT_HOUSEHOLDS",
          n = 5,
          style = "jenks", 
          title = "Total households") + 
  tm_borders(alpha = 0.5) 

RADIO.map <- tm_shape(shan_sf) + 
  tm_fill(col = "RADIO",
          n = 5,
          style = "jenks",
          title = "Number Radio ") + 
  tm_borders(alpha = 0.5) 

tmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,
             asp=NA, ncol=2)
```

```{r}
tm_shape(shan_sf) +
    tm_polygons(c("TT_HOUSEHOLDS", "RADIO_PR"),
                style="jenks") +
    tm_facets(sync = TRUE, ncol = 2) +
  tm_legend(legend.position = c("right", "bottom"))+
  tm_layout(outer.margins=0, asp=0)
```

TO BE COMPLETED (Observations)

At the very least, we can see that overall a higher number of total households coincides with a higher number of radio 'usage' (which is expected.) However, with the choropleth map using RADIO_PR, we can see that the higher number of total households does not necessarily mean an overall higher radio 'usage' rate, or more specifically, the penetration rate.

## Simple Correlation Analysis

```{r}
cluster_vars.cor = cor(ict_derived[,12:17])
corrplot.mixed(cluster_vars.cor,
         lower = "ellipse", 
               upper = "number",
               tl.pos = "lt",
               diag = "l",
               tl.col = "black")
```

The correlation plot above shows that COMPUTER_PR and INTERNET_PR are highly correlated. This suggest that only one of them should be used in the cluster analysis instead of both.

# Hierarchical Cluster Analysis

## Extracting Cluster Variables

```{r}
cluster_vars <- shan_sf %>%
  st_set_geometry(NULL) %>% # Remove geometry
  select("TS.x", "RADIO_PR", "TV_PR", "LLPHONE_PR", "MPHONE_PR", "COMPUTER_PR") # select all the derived variables

head(cluster_vars,10)
```

Notice that the final clustering variables list does not include variable INTERNET_PR because it is highly correlated with variable COMPUTER_PR.

```{r}
shan_ict <- select(cluster_vars, c(2:6))
head(shan_ict, 10)
```

Export the cluster variables into RDS file format

```{r}
write_rds(shan_ict, "data/rds/shan_ict.rds")
```

```{r}
shan_ict <- read_rds("data/rds/shan_ict.rds")
```

## Data Standardisation Methods

### Min-Max Standardisation

Notice that the values range of the Min-max standardised clustering variables are 0-1 now.

```{r}
shan_ict.std <- normalize(shan_ict)
summary(shan_ict.std)
```

### Z-score Standardisation

Notice the mean and standard deviation of the Z-score standardised clustering variables are 0 and 1 respectively.

Caution: Z-score standardisation method should only be used if we would assume all variables come from some normal distribution.

```{r}
shan_ict.z <- scale(shan_ict)
describe(shan_ict.z)
```

### Visualisation (Histogram)

(Copied from Exercise) :

Beside reviewing the summary statistics of the standardised clustering variables, it is also a good practice to visualise their distribution graphically. Let us start with Radio_PR.

```{r}
r <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Raw values without standardisation")

shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot(data=shan_ict_s_df, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Min-Max Standardisation")

shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot(data=shan_ict_z_df, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Z-score Standardisation")

ggarrange(r, s, z,
          ncol = 3,
          nrow = 1)
```

Note that our previous observations are reflected graphically here. The min-max standardisation rescaled the data to a range of values between 0 and 1 (inclusive), while the z-score standardisation rescaled the data to a range of values centred around 0 (mean) and each standard deviation is a value of 1. Visually, the distribution might look different, but that is simply due to how values are allocated to different bins of the histogram.

Overall, the distribution is not normal. While the z-score can be used for such distributions, note that the distribution is still non-normal after transformation and cannot be interpreted as if normal.

### Visualisation (density plot)

Note that the area under the density will add up to a 100%.

```{r}
r <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Raw values without standardisation")

shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot(data=shan_ict_s_df, 
       aes(x=`RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Min-Max Standardisation")

shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot(data=shan_ict_z_df, 
       aes(x=`RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Z-score Standardisation")

ggarrange(r, s, z,
          ncol = 3,
          nrow = 1)
```

## Computing proximity matrix

*dist()* supports six distance proximity calculations, they are: **euclidean, maximum, manhattan, canberra, binary and minkowski**. The default is *euclidean* proximity matrix.

```{r}
proxmat <- dist(shan_ict, method = 'euclidean')
proxmat
```

## Computing hierarchical clustering

(Copied from Exercise) :

In R, there are several packages provide hierarchical clustering function. In this hands-on exercise, [*hclust()*](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/hclust.html) of R stats will be used.

*hclust()* employed agglomeration method to compute the cluster. Eight clustering algorithms are supported, they are: ward.D, ward.D2, single, complete, average(UPGMA), mcquitty(WPGMA), median(WPGMC) and centroid(UPGMC).

The code chunk below performs hierarchical cluster analysis using ward.D method. The hierarchical clustering output is stored in an object of class **hclust** which describes the tree produced by the clustering process.

```{r}
hclust_ward <- hclust(proxmat, method = 'ward.D')
```

```{r}
plot(hclust_ward, cex = 0.6)
```

## Selecting the optimal clustering algorithm

(Copied from Exercise) One of the challenge in performing hierarchical clustering is to identify stronger clustering structures. The issue can be solved by using use [*agnes()*](https://www.rdocumentation.org/packages/cluster/versions/2.1.0/topics/agnes) function of [**cluster**](https://cran.r-project.org/web/packages/cluster/) package. It functions like *hclus()*, however, with the *agnes()* function you can also get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).

The code chunk below will be used to compute the agglomerative coefficients of all hierarchical clustering algorithms.

```{r}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

ac <- function(x) {
  agnes(shan_ict, method = x)$ac
}

map_dbl(m, ac)
```

Based on the results above, we can see that using the ward method returns the strongest clustering structure (agglomerative coefficient closest to 1(?)). We will therefore use ward method from here onwards.

## Determining Optimal Clusters

(Copied from Exercise) :

Another technical challenge face by data analyst in performing clustering analysis is to determine the optimal clusters to retain.

There are [three](https://statweb.stanford.edu/~gwalther/gap) commonly used methods to determine the optimal clusters, they are:

-   [Elbow Method](https://en.wikipedia.org/wiki/Elbow_method_(clustering))

-   [Average Silhouette Method](https://www.sciencedirect.com/science/article/pii/0377042787901257?via%3Dihub)

-   [Gap Statistic Method](http://www.web.stanford.edu/~hastie/Papers/gap.pdf)

### Gap Statistic Method

```{r}
set.seed(12345)
gap_stat <- clusGap(shan_ict, 
                    FUN = hcut, 
                    nstart = 25, 
                    K.max = 10, 
                    B = 50)
# Print the result
print(gap_stat, method = "firstmax")
fviz_gap_stat(gap_stat)
```

(Copied from Exercise) :

With reference to the gap statistic graph above, the recommended number of cluster to retain is 1. However, it is not logical to retain only one cluster. By examine the gap statistic graph, the 6-cluster gives the largest gap statistic and should be the next best cluster to pick.

**Note:** In addition to these commonly used approaches, the [NbClust](https://cran.r-project.org/web/packages/NbClust/) package, published by Charrad et al., 2014, provides 30 indices for determining the relevant number of clusters and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.

Elbow Method and Average Silhouette Method will not be attempted this time.

## Interpreting the dendrograms

(Copied from Exercise) :

In the dendrogram displayed above, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height.

The height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are. Note that, conclusions about the proximity of two observations can be drawn only based on the height where branches containing those two observations first are fused. We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity.

It’s also possible to draw the dendrogram with a border around the selected clusters by using [*rect.hclust()*](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/rect.hclust.html) of R stats. The argument *border* is used to specify the border colors for the rectangles.

```{r}
plot(hclust_ward, cex = 0.6)
rect.hclust(hclust_ward, 
            k = 6, 
            border = 2:5)
```

## Visually-driven hierarchical clustering analysis

We will first transform data frame into data matrix (to be used in heatmaply package)

```{r}
shan_ict_mat <- data.matrix(shan_ict)
```

```{r}
heatmaply(normalize(shan_ict_mat),
          Colv=NA,
          dist_method = "euclidean",
          hclust_method = "ward.D",
          seriate = "OLO",
          colors = Blues,
          k_row = 6,
          margins = c(NA,200,60,NA),
          fontsize_row = 4,
          fontsize_col = 5,
          main="Geographic Segmentation of Shan State by ICT indicators",
          xlab = "ICT Indicators",
          ylab = "Townships of Shan State"
          )
```

## Mapping the clusters formed

```{r}
groups <- as.factor(cutree(hclust_ward, k=6))
```

```{r}
shan_sf_cluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER`=`as.matrix.groups.`)
```

```{r}
qtm(shan_sf_cluster, "CLUSTER")
```

The choropleth map above reveals the clusters are very fragmented. The is one of the major limitation when non-spatial clustering algorithm such as hierarchical cluster analysis method is used (i.e. it does not take into account the spatial properties, it only cluster based on non-spatial attributes.)

# Spatially Constrained Clustering: SKATER approach

This is an alternative method to clustering, which takes into account the spatial properties of the regions.

TO BE COMPLETED

# Spatially Constrained Clustering: ClustGeo Method

TO BE COMPLETED
